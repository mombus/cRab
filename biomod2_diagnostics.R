# ========================================================================================================================
# BIOMOD2 DIAGNOSTICS SCRIPT
# Purpose: Add statistical reliability diagnostics to biomod2 SDM runs and projections
# Author: Generated by assistant (GPT-5)
# Date: Sys.Date()
# Usage:
#   - Source this script AFTER running your main biomod2 modeling script so that
#     objects like `myBiomodModelOut`, `myBiomodProj`, `myBiomodEM`, `myBiomodEMProj`,
#     `myBiomodProj1`, `myBiomodEMProj1`, `myExpl`, `myExplP1`, `myResp`, `DATA` are in memory.
#   - Optional: place an independent validation CSV at path 'independent_validation.csv'
#       with columns: x, y, occ, and the same environmental variable names as in `myExpl`.
#   - Outputs are written to a timestamped folder under getwd()/diagnostics.
# ========================================================================================================================

suppressPackageStartupMessages({
  # Core
  library(biomod2)
  library(dplyr)
  library(tidyr)
  library(purrr)
  library(ggplot2)
  library(readr)
  # Metrics
  library(pROC)
  library(precrec)
  library(ecospat)
  library(yardstick)
  # Extrapolation
  library(dismo)
  # Mapping helpers (optional)
  library(ggpubr)
})

# ---------------------------- Helpers & Setup ---------------------------------- #
ensure_dir <- function(path) {
  if (!dir.exists(path)) dir.create(path, recursive = TRUE, showWarnings = FALSE)
}

scale_predictions_01 <- function(predictions_numeric) {
  # biomod2 EM predictions are often in [0, 1000]; scale to [0, 1] if needed
  max_value <- suppressWarnings(max(predictions_numeric, na.rm = TRUE))
  if (is.finite(max_value) && max_value > 1.5) {
    return(pmin(pmax(predictions_numeric / 1000, 0), 1))
  }
  pmin(pmax(predictions_numeric, 0), 1)
}

write_safe_csv <- function(df, path) {
  tryCatch(readr::write_csv(df, path), error = function(e) utils::write.csv(df, path, row.names = FALSE))
}

save_plot <- function(plot_obj, filename, width = 8, height = 6, dpi = 300) {
  ggplot2::ggsave(filename = filename, plot = plot_obj, width = width, height = height, dpi = dpi)
}

compute_calibration <- function(labels_binary, probs_01, num_bins = 10) {
  stopifnot(length(labels_binary) == length(probs_01))
  calibration_df <- tibble(prob = probs_01, label = labels_binary) %>%
    mutate(bin = cut(prob, breaks = seq(0, 1, length.out = num_bins + 1), include.lowest = TRUE)) %>%
    group_by(bin) %>%
    summarise(
      bin_mid = mean(as.numeric(sub(".*,", "", sub("\\[(.*)\\]", "\\1", as.character(bin)))) +
                       as.numeric(sub(",.*", "", sub("\\[(.*)\\]", "\\1", as.character(bin)))))/2,
      prob_mean = mean(prob, na.rm = TRUE),
      obs_rate = mean(label, na.rm = TRUE),
      n = n(),
      .groups = "drop"
    ) %>%
    filter(is.finite(obs_rate))

  brier <- mean((probs_01 - labels_binary)^2, na.rm = TRUE)
  # Expected Calibration Error (ECE)
  ece <- calibration_df %>% mutate(abs_gap = abs(obs_rate - prob_mean)) %>% summarise(
    ece = sum((n / sum(n)) * abs_gap)
  ) %>% pull(ece)

  list(table = calibration_df, brier = brier, ece = ece)
}

plot_calibration <- function(calibration_df) {
  ggplot(calibration_df, aes(x = prob_mean, y = obs_rate, size = n)) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") +
    geom_point(color = "#2C7FB8", alpha = 0.8) +
    scale_size_continuous(name = "N") +
    coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
    labs(x = "Predicted probability (bin mean)", y = "Observed prevalence", title = "Calibration (Reliability) Curve") +
    theme_minimal(base_size = 12)
}

compute_roc_pr <- function(labels_binary, probs_01) {
  roc_obj <- pROC::roc(response = labels_binary, predictor = probs_01, quiet = TRUE)
  auc_roc <- tryCatch(pROC::auc(roc_obj)[1], error = function(e) NA_real_)

  pr_obj <- precrec::evalmod(scores = probs_01, labels = labels_binary)
  aucs <- precrec::auc(pr_obj)
  auc_pr <- tryCatch(aucs %>% filter(curvetypes == "PRC") %>% pull(aucs) %>% as.numeric(), error = function(e) NA_real_)

  list(roc = roc_obj, auc_roc = as.numeric(auc_roc), pr = pr_obj, auc_pr = as.numeric(auc_pr))
}

plot_roc_curve <- function(roc_obj) {
  ggroc(roc_obj, colour = "#1B9E77", size = 1) +
    geom_abline(slope = 1, intercept = 1, linetype = "dashed", color = "gray50") +
    coord_equal(xlim = c(1, 0), ylim = c(0, 1)) +
    labs(x = "1 - Specificity", y = "Sensitivity", title = "ROC Curve") +
    theme_minimal(base_size = 12)
}

plot_pr_curve <- function(pr_obj) {
  autoplot(pr_obj) +
    labs(title = "Precision-Recall Curve") +
    theme_minimal(base_size = 12)
}

find_optimal_threshold_tss <- function(labels_binary, probs_01, step = 0.01) {
  thresholds <- seq(0, 1, by = step)
  metrics <- map_dfr(thresholds, function(th) {
    pred_class <- as.integer(probs_01 >= th)
    tp <- sum(pred_class == 1 & labels_binary == 1, na.rm = TRUE)
    tn <- sum(pred_class == 0 & labels_binary == 0, na.rm = TRUE)
    fp <- sum(pred_class == 1 & labels_binary == 0, na.rm = TRUE)
    fn <- sum(pred_class == 0 & labels_binary == 1, na.rm = TRUE)
    tpr <- ifelse((tp + fn) > 0, tp / (tp + fn), NA_real_)
    tnr <- ifelse((tn + fp) > 0, tn / (tn + fp), NA_real_)
    tibble(threshold = th, TSS = (tpr + tnr - 1), Sensitivity = tpr, Specificity = tnr)
  })
  metrics %>% slice_max(order_by = TSS, n = 1, with_ties = FALSE)
}

compute_boyce_index <- function(labels_binary, probs_01, num_class = 0, window_w = NULL) {
  # Continuous Boyce index requires presence scores and background scores
  presence_scores <- probs_01[labels_binary == 1]
  background_scores <- probs_01
  tryCatch({
    res <- ecospat::ecospat.boyce(fit = background_scores, obs = presence_scores, nclass = num_class, window.w = window_w)
    list(CBI = as.numeric(res$Spearman.cor), curve = res$F.ratio)
  }, error = function(e) list(CBI = NA_real_, curve = NULL))
}

compute_uncertainty_sd_cv <- function(predictions_long_df) {
  # Expect columns: algo, pred; rows aligned across algos
  mat <- predictions_long_df %>%
    select(algo, pred) %>%
    group_split(algo, keep = FALSE) %>%
    purrr::map(~ .x$pred) %>%
    purrr::reduce(cbind)
  sd_vec <- apply(mat, 1, sd, na.rm = TRUE)
  mean_vec <- apply(mat, 1, mean, na.rm = TRUE)
  cv_vec <- sd_vec / ifelse(mean_vec == 0, NA, mean_vec)
  list(sd = sd_vec, cv = cv_vec)
}

compute_mess_scores <- function(reference_env_df, target_env_df) {
  # dismo::mess expects a data.frame/matrix target and reference; returns vector of MESS
  tryCatch({
    mess_vals <- dismo::mess(x = as.data.frame(target_env_df), v = as.data.frame(reference_env_df))
    as.numeric(mess_vals)
  }, error = function(e) {
    message("MESS computation failed: ", conditionMessage(e))
    rep(NA_real_, nrow(target_env_df))
  })
}

# ---------------------------- Output Directory ---------------------------------- #
base_out_dir <- file.path(getwd(), "diagnostics")
ensure_dir(base_out_dir)
run_tag <- format(Sys.time(), "%Y%m%d_%H%M%S")
out_dir <- file.path(base_out_dir, paste0("run_", run_tag))
ensure_dir(out_dir)
message("Diagnostics outputs will be written to: ", out_dir)

# ---------------------------- Sanity Checks ------------------------------------- #
required_objs <- c("myBiomodModelOut", "myBiomodEM", "myExpl", "myResp")
missing_objs <- required_objs[!vapply(required_objs, exists, logical(1))]
if (length(missing_objs) > 0) {
  stop("Missing objects in environment: ", paste(missing_objs, collapse = ", "),
       "\nPlease run your main biomod2 script first.")
}

# Try to fetch current ensemble predictions (EMmean); compute if absent
if (!exists("myBiomodEMProj") || !inherits(myBiomodEMProj, "biomod2_model_forecasting")) {
  message("'myBiomodEMProj' not found. Attempting to build from available objects (current env)...")
  stopifnot(exists("myBiomodProj"))
  myBiomodEMProj <- BIOMOD_EnsembleForecasting(
    bm.em = myBiomodEM,
    bm.proj = myBiomodProj,
    models.chosen = "all",
    metric.binary = "all",
    metric.filter = "all"
  )
}

# Current EMmean predictions
pred_current_em_long <- get_predictions(myBiomodEMProj)
pred_current_emmean <- pred_current_em_long %>% filter(.data$algo == "EMmean")

# Build MAPDATA for current if not available
if (!exists("MAPDATA")) {
  stopifnot(exists("DATA"))
  MAPDATA <- tibble(
    X = DATA$x,
    Y = DATA$y,
    PRED = pred_current_emmean$pred
  )
}

# Single-model predictions for uncertainty on current
if (!exists("myBiomodProj")) {
  stop("'myBiomodProj' (single-model projection) is required to compute uncertainty for current.")
}
pred_current_single <- get_predictions(myBiomodProj)

# ---------------------------- INTERNAL EVALUATION SUMMARIES --------------------- #
message("Saving internal evaluation scores and variable importance...")
EVAL_single <- get_evaluations(myBiomodModelOut)
IMP_single  <- get_variables_importance(myBiomodModelOut)
EVAL_ens    <- tryCatch(get_evaluations(myBiomodEM), error = function(e) NULL)
IMP_ens     <- tryCatch(get_variables_importance(myBiomodEM), error = function(e) NULL)

# Write raw objects (RDS) and flattened CSVs when possible
saveRDS(EVAL_single, file.path(out_dir, "eval_single_models_raw.rds"))
saveRDS(IMP_single,  file.path(out_dir, "varimp_single_models_raw.rds"))
if (!is.null(EVAL_ens)) saveRDS(EVAL_ens, file.path(out_dir, "eval_ensemble_raw.rds"))
if (!is.null(IMP_ens))  saveRDS(IMP_ens,  file.path(out_dir, "varimp_ensemble_raw.rds"))

try({
  eval_single_df <- as.data.frame(EVAL_single)
  write_safe_csv(eval_single_df, file.path(out_dir, "eval_single_models.csv"))
}, silent = TRUE)
try({
  varimp_single_df <- as.data.frame(IMP_single)
  write_safe_csv(varimp_single_df, file.path(out_dir, "varimp_single_models.csv"))
}, silent = TRUE)
if (!is.null(EVAL_ens)) try({
  eval_ens_df <- as.data.frame(EVAL_ens)
  write_safe_csv(eval_ens_df, file.path(out_dir, "eval_ensemble.csv"))
}, silent = TRUE)
if (!is.null(IMP_ens)) try({
  varimp_ens_df <- as.data.frame(IMP_ens)
  write_safe_csv(varimp_ens_df, file.path(out_dir, "varimp_ensemble.csv"))
}, silent = TRUE)

# ---------------------------- CALIBRATION, ROC, PR, THRESHOLDS ------------------ #
message("Computing calibration, ROC, PR, thresholds, Boyce index for CURRENT...")
labels <- as.integer(myResp)
probs_current_01 <- scale_predictions_01(pred_current_emmean$pred)

calib_res <- compute_calibration(labels, probs_current_01, num_bins = 10)
write_safe_csv(calib_res$table, file.path(out_dir, "calibration_current_table.csv"))
calib_plot <- plot_calibration(calib_res$table) +
  labs(subtitle = sprintf("Brier=%.3f, ECE=%.3f", calib_res$brier, calib_res$ece))
save_plot(calib_plot, file.path(out_dir, "calibration_current.png"))

roc_pr <- compute_roc_pr(labels, probs_current_01)
# ROC
roc_plot <- plot_roc_curve(roc_pr$roc) + labs(subtitle = sprintf("AUC=%.3f", roc_pr$auc_roc))
save_plot(roc_plot, file.path(out_dir, "roc_current.png"))
# PR
pr_plot <- plot_pr_curve(roc_pr$pr) + labs(subtitle = sprintf("AUPRC=%.3f", roc_pr$auc_pr))
save_plot(pr_plot, file.path(out_dir, "pr_current.png"))

opt_thr <- find_optimal_threshold_tss(labels, probs_current_01, step = 0.005)
write_safe_csv(opt_thr, file.path(out_dir, "optimal_thresholds_current.csv"))

boyce <- compute_boyce_index(labels, probs_current_01)
write_safe_csv(tibble(metric = "Continuous Boyce Index", value = boyce$CBI), file.path(out_dir, "boyce_current.csv"))
if (!is.null(boyce$curve)) {
  try({
    boyce_curve_df <- tibble(F_ratio = as.numeric(boyce$curve)) %>% mutate(x = row_number())
    p_boyce <- ggplot(boyce_curve_df, aes(x = x, y = F_ratio)) +
      geom_line(color = "#984EA3") +
      labs(title = "Boyce Index Curve (Current)", x = "Class", y = "F-ratio") +
      theme_minimal(base_size = 12)
    save_plot(p_boyce, file.path(out_dir, "boyce_current_curve.png"))
  }, silent = TRUE)
}

# ---------------------------- UNCERTAINTY MAPS (CURRENT) ------------------------ #
message("Computing uncertainty across algorithms for CURRENT...")
unc <- compute_uncertainty_sd_cv(pred_current_single)
unc_df <- tibble(UNC_SD = as.numeric(unc$sd), UNC_CV = as.numeric(unc$cv))
MAPDATA_unc <- bind_cols(MAPDATA, unc_df)
write_safe_csv(MAPDATA_unc, file.path(out_dir, "mapdata_current_with_uncertainty.csv"))

p_unc_sd <- ggplot(MAPDATA_unc, aes(x = X, y = Y, fill = UNC_SD)) +
  geom_raster(interpolate = FALSE) +
  scale_fill_viridis_c(option = "C", name = "SD") +
  coord_equal() +
  labs(title = "Uncertainty (SD) across algorithms - Current") +
  theme_minimal(base_size = 12)
save_plot(p_unc_sd, file.path(out_dir, "uncertainty_sd_current.png"), width = 9, height = 6)

# ---------------------------- FUTURE PROJECTIONS DIAGNOSTICS -------------------- #
future_available <- exists("myBiomodEMProj1") && exists("myExplP1") && exists("DATA")
if (future_available) {
  message("Processing FUTURE diagnostics (delta, uncertainty, MESS)...")
  pred_future_em_long <- get_predictions(myBiomodEMProj1)
  pred_future_emmean <- pred_future_em_long %>% filter(.data$algo == "EMmean")

  if (!exists("MAPDATA2")) {
    MAPDATA2 <- tibble(
      X = DATA$x,
      Y = DATA$y,
      PRED = pred_future_emmean$pred
    )
  }

  # Delta EMmean (scaled 0-1)
  probs_future_01 <- scale_predictions_01(pred_future_emmean$pred)
  delta_df <- tibble(delta = probs_future_01 - probs_current_01)
  MAPDATA_delta <- bind_cols(MAPDATA %>% select(X, Y), delta_df)
  write_safe_csv(MAPDATA_delta, file.path(out_dir, "delta_future_minus_current.csv"))

  p_delta <- ggplot(MAPDATA_delta, aes(x = X, y = Y, fill = delta)) +
    geom_raster(interpolate = FALSE) +
    scale_fill_gradient2(low = "#D7301F", mid = "#FFFFBF", high = "#1A9850", midpoint = 0, name = "Î” Prob") +
    coord_equal() +
    labs(title = "Delta (Future - Current) EMmean (scaled)") +
    theme_minimal(base_size = 12)
  save_plot(p_delta, file.path(out_dir, "delta_future_current.png"), width = 9, height = 6)

  # Future single-model uncertainty
  if (exists("myBiomodProj1")) {
    pred_future_single <- get_predictions(myBiomodProj1)
    unc_fut <- compute_uncertainty_sd_cv(pred_future_single)
    MAPDATA2_unc <- MAPDATA2 %>% bind_cols(tibble(UNC_SD = as.numeric(unc_fut$sd), UNC_CV = as.numeric(unc_fut$cv)))
    write_safe_csv(MAPDATA2_unc, file.path(out_dir, "mapdata_future_with_uncertainty.csv"))

    p_unc_sd_f <- ggplot(MAPDATA2_unc, aes(x = X, y = Y, fill = UNC_SD)) +
      geom_raster(interpolate = FALSE) +
      scale_fill_viridis_c(option = "C", name = "SD") +
      coord_equal() +
      labs(title = "Uncertainty (SD) across algorithms - Future") +
      theme_minimal(base_size = 12)
    save_plot(p_unc_sd_f, file.path(out_dir, "uncertainty_sd_future.png"), width = 9, height = 6)
  }

  # MESS (Future vs Current environmental space)
  try({
    mess_vals <- compute_mess_scores(reference_env_df = myExpl, target_env_df = myExplP1)
    MAPDATA2_MESS <- MAPDATA2 %>% mutate(MESS = mess_vals)
    write_safe_csv(MAPDATA2_MESS, file.path(out_dir, "mapdata_future_with_MESS.csv"))

    p_mess <- ggplot(MAPDATA2_MESS, aes(x = X, y = Y, fill = MESS)) +
      geom_raster(interpolate = FALSE) +
      scale_fill_gradient2(low = "#762A83", mid = "#F7F7F7", high = "#1B7837", midpoint = 0, name = "MESS") +
      coord_equal() +
      labs(title = "MESS (Extrapolation risk) - Future vs Current") +
      theme_minimal(base_size = 12)
    save_plot(p_mess, file.path(out_dir, "mess_future_vs_current.png"), width = 9, height = 6)
  }, silent = TRUE)
} else {
  message("Future diagnostics skipped: missing 'myBiomodEMProj1' or 'myExplP1'.")
}

# ---------------------------- OPTIONAL: Independent validation ------------------ #
# If an external dataset 'independent_validation.csv' exists in the working dir,
# perform projection and compute metrics.
indep_path <- file.path(getwd(), "independent_validation.csv")
if (file.exists(indep_path)) {
  message("Independent validation dataset found. Running external validation...")
  indep <- readr::read_csv(indep_path, show_col_types = FALSE)
  stopifnot(all(c("x", "y", "occ") %in% names(indep)))
  env_cols <- setdiff(names(indep), c("x", "y", "occ"))
  new_env <- indep %>% select(all_of(env_cols))

  indep_proj <- BIOMOD_Projection(bm.mod = myBiomodModelOut, proj.name = "IndependentEval", new.env = new_env, models.chosen = "all")
  indep_emf  <- BIOMOD_EnsembleForecasting(bm.em = myBiomodEM, bm.proj = indep_proj, models.chosen = "all", metric.binary = "all", metric.filter = "all")
  indep_pred <- get_predictions(indep_emf) %>% filter(.data$algo == "EMmean")
  indep_probs <- scale_predictions_01(indep_pred$pred)
  indep_labels <- as.integer(indep$occ)

  indep_cal <- compute_calibration(indep_labels, indep_probs, num_bins = 10)
  write_safe_csv(indep_cal$table, file.path(out_dir, "calibration_independent_table.csv"))
  save_plot(plot_calibration(indep_cal$table) + labs(subtitle = sprintf("Brier=%.3f, ECE=%.3f", indep_cal$brier, indep_cal$ece)),
            file.path(out_dir, "calibration_independent.png"))

  indep_rocpr <- compute_roc_pr(indep_labels, indep_probs)
  save_plot(plot_roc_curve(indep_rocpr$roc) + labs(subtitle = sprintf("AUC=%.3f", indep_rocpr$auc_roc)),
            file.path(out_dir, "roc_independent.png"))
  save_plot(plot_pr_curve(indep_rocpr$pr) + labs(subtitle = sprintf("AUPRC=%.3f", indep_rocpr$auc_pr)),
            file.path(out_dir, "pr_independent.png"))

  indep_opt <- find_optimal_threshold_tss(indep_labels, indep_probs, step = 0.005)
  write_safe_csv(indep_opt, file.path(out_dir, "optimal_thresholds_independent.csv"))

  indep_boyce <- compute_boyce_index(indep_labels, indep_probs)
  write_safe_csv(tibble(metric = "Continuous Boyce Index", value = indep_boyce$CBI), file.path(out_dir, "boyce_independent.csv"))
}

message("Diagnostics complete. Outputs at: ", out_dir)