[
  {
    "objectID": "chapter 8.html",
    "href": "chapter 8.html",
    "title": "9  Модель Catch-Survey Analysis (CSA)",
    "section": "",
    "text": "9.1 Введение\nМодель “анализа уловов и съемок” - Catch-Survey Analysis (CSA) представляет собой инструмент для оценки состояния запасов, особенно тех видов, данные по индивидуальному возрасту которых труднодоступны или отсутствуют, что типично для многих беспозвоночных, таких как крабы, креветки, а также для некоторых рыб. В отличие от классических продукционных моделей, которые оперируют агрегированными показателями всей популяции и требуют строгих допущений о ее равновесном состоянии и постоянной емкости среды, когортные модели, подобные CSA, позволяют отслеживать судьбу отдельных функциональных категорий (например, пререкруты, рекруты, пострекруты). Они явным образом учитывают такие процессы, как рост, пополнение и естественная смертность, разделяя запас на дискретные размерные или возрастные группы. Это дает несомненное преимущество при анализе динамики популяций с выраженной цикличностью или тех, которые подвергаются интенсивному промысловому прессу, избирательно воздействующему на определенные размерные или возрастные категории (например, пререкруты не подвержены прямой прмысловой смертности в отличие от рекрутов и посрекрутов). Подробнее о модели и ее реализации можно почитать в статье “Результаты применения стохастической когортной модели CSA для оценки запаса камчатского краба Paralithodes camtschaticus в Баренцевом море”. В статье описывается реализация модели в программе OpenBUGS, которая в упрощенном виде (без прогноза, риск-анализа и диагностики) и в учебных целях была переведена в среду R и представлена ниже, а полный срипт здесь.Также доступна иммитационная CSA модель для 4 размерных групп, реализованная в MS Excel по ссылке.\nДанная реализация модели представляет собой байесовский подход к оценке запасов, который позволяет учитывать неопределенности как в процессе динамики популяции, так и в процессе наблюдений, что особенно важно при работе с данными, характеризующимися высокой вариабельностью и неполнотой. В основе модели лежит разделение популяции на три размерно-возрастные группы: пререкруты (P1), рекруты (R) и пострекруты (P), что соответствует биологическим особенностям многих видов крабов, включая камчатского краба. Модель включает два основных компонента: динамику процесса, описывающую естественные изменения численности популяции, и модель наблюдений, связывающую ненаблюдаемые “истинную” численность запаса с доступными данными съемок (индексами численности пререкрутов, рекрутов и пострекрутов). Уравнения процессной динамики для пострекрутов имеют вид:\nP[i] = [(P1[i-1]×Gp×Mp) + R[i-1] + P[i-1] - catch[i-1]] × exp(-M) + εP, где\nGp обозначает вероятность перехода пререкрутов в пострекруты,\nMp - вероятность линьки пререкрутов,\nM - коэффициент естественной смертности, а εP представляет собой процессную ошибку.\nДля рекрутов уравнение динамики выглядит как\nR[i] = (P1[i-1]×Gr×Mp) × exp(-M) + εR, где\nGr - вероятность перехода пререкрутов в рекруты. Динамика пререкрутов моделируется как лог-случайное блуждание P1[i] = P1[i-1] + εP1. Модель наблюдений предполагает, что данные траловых съемок соответствуют логнормальному распределению относительно истинной численности, умноженной на коэффициент улавливаемости:\nbioindexP1[i] ~ lognormal(log(q1×P1[i]), precbioindexP1),\nаналогично для рекрутов и пострекрутов, где q1, q2, q3 - коэффициенты улавливаемости для каждой группы, а precbioindex - параметры точности. В байесовском подходе ключевую роль играют априорные распределения параметров, которые в данной реализации задаются как равномерные для коэффициентов улавливаемости (q1, q2, q3 ~ dunif(0.1,1)), нормальные для вероятностей перехода (Gr ~ dnorm(0.9,500), Gp ~ dnorm(0.075,500), Mp ~ dnorm(0.95,500)) и для коэффициента естественной смертности (M ~ dnorm(0.2,100)). Использование байесовского подхода позволяет не только получить точечные оценки параметров, но и оценить полные апостериорные распределения, что дает возможность проводить риск-анализ различных сценариев управления запасом. В данном занятии мы реализуем модель CSA в среде R с использованием пакетов rjags и coda, что позволяет эффективно работать с байесовскими иерархическими моделями через интерфейс с программой JAGS, которую также необходимо установить.\nМы рассмотрим полный цикл работы с моделью: от подготовки данных и задания априорных распределений до обучения модели и анализа результатов, включая визуализацию априорных и апостериорных распределений параметров, анализ остатков и сравнение моделируемой и фактической динамики запаса. Особое внимание будет уделено интерпретации результатов в контексте управления водными биоресурсами, что является ключевой целью применения подобных моделей в практической деятельности гидробиологов и ихтиологов. ## Загрузка данных и первичный осмотр",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Модель Catch-Survey Analysis (CSA)</span>"
    ]
  },
  {
    "objectID": "chapter 8.html#реализация-модели",
    "href": "chapter 8.html#реализация-модели",
    "title": "9  Модель Catch-Survey Analysis (CSA)",
    "section": "9.2 Реализация модели",
    "text": "9.2 Реализация модели\n\n# ========================================================================================================================\n# ПРАКТИЧЕСКОЕ ЗАНЯТИЕ: МОДЕЛЬ Catch-Survey Analysis (CSA) - три #категории (пререкруты (P1), рекруты (R), пострекруты (P)\n# Курс: \"Оценка водных биоресурсов в среде R (для начинающих)\"\n# Автор: Баканев С. В. Дата: 20.08.2025\n# Структура:\n# 1) Входные данные\n# 2) Модель\n# 3) Прайеры\n# 4) Обучение модели\n# 5) Подготовка выходных данных \n# 6) Анализ результатов (визуализация априорных и апостериорных параметров;бабл-плоты остатков;  динамика индексов) \n# ========================================================================================================================\n# Установка рабочей директории\nsetwd(\"C:/CSA\")\n\n# Подключение необходимых библиотек\n# install.packages(c(\"rjags\", \"coda\"))  # Раскомментировать для установки\nlibrary(rjags)  # Для работы с JAGS\n\nWarning: пакет 'rjags' был собран под R версии 4.5.1\n\n\nЗагрузка требуемого пакета: coda\n\n\nWarning: пакет 'coda' был собран под R версии 4.5.1\n\n\nLinked to JAGS 4.3.1\n\n\nLoaded modules: basemod,bugs\n\nlibrary(coda)   # Для анализа MCMC-выхода\nlibrary(ggplot2) # Рисунки\n\n# ========================================================================================================================\n# --- Входные данные ---\n# ========================================================================================================================\ndata_list &lt;- list(\n  N = 16,# Количество временных точек\n # Наблюдаемые данные (индексы запаса)\n  bioindexP1 = c(1500,1028,554,887,1345,1817,2291,1958,1500,1028,554,887,1345,1817,2291,1958),\n  bioindexR  = c(2531,1927,1305,764,1216,   1820,2442,2983,2531,1927,1305,764,1216,1820,2442,2983),\n  bioindexP  = c(13741,13770,13060,11653,9782,8634,8321,8793,9809,10177,9776,9566,8789,8640,9240,10547),\n  catch      = c(6,2,6,15,21,37,37,315,945,890,991,1060,1000,1000,1600,1673,1250)\n)\n\n# Создание вектора лет для подписей\nYEAR &lt;- 2000 + 0:(data_list$N - 1)\n\n# ========================================================================================================================\n# --- Генерация модели CSA --\n# ========================================================================================================================\nmodel_string &lt;- \"\nmodel {\n  # Observation model\n  for (i in 1:N) {\n    bioindexP1med[i] &lt;- log(1.0E-6 + q1 * P1[i])\n    bioindexP1[i] ~ dlnorm(bioindexP1med[i], precbioindexP1)\n\n    bioindexRmed[i]  &lt;- log(1.0E-6 + q2 * R[i])\n    bioindexR[i] ~ dlnorm(bioindexRmed[i],  precbioindexR)\n\n    bioindexPmed[i]  &lt;- log(1.0E-6 + q3 * P[i])\n    bioindexP[i] ~ dlnorm(bioindexPmed[i],  precbioindexP)\n  }\n\n  # State dynamics\n  inv_surv &lt;- exp(-M)\n  for (i in 2:N) {\n    tmpPraw[i] &lt;- (P1[i-1]*Gp*Mp + R[i-1] + P[i-1] - catch[i-1]) * inv_surv\n    tmpPpos[i] &lt;- tmpPraw[i] * step(tmpPraw[i])\n    Pmed[i] &lt;- log(1.0E-6 + tmpPpos[i])\n    P[i] ~ dlnorm(Pmed[i], precP)\n\n    tmpRraw[i] &lt;- (P1[i-1]*Gr*Mp) * inv_surv\n    tmpRpos[i] &lt;- tmpRraw[i] * step(tmpRraw[i])\n    Rmed[i] &lt;- log(1.0E-6 + tmpRpos[i])\n    R[i] ~ dlnorm(Rmed[i], precR)\n\n    P1med[i] &lt;- log(1.0E-6 + P1[i-1])\n    P1[i] ~ dlnorm(P1med[i], precP1)\n  }\n\n  # Risk\n  for (i in 1:N) {\n    PR[i] &lt;- P[i] + R[i]\n    p.PRlim[i] &lt;- step(PRlim - PR[i])\n  }\n  PRlim &lt;- 4000\n\n  # Priors\n  P1[1] ~ dunif(200,4000)\n  P[1]  ~ dunif(200,6000)\n  R[1]  ~ dunif(200,25000)\n\n  Gr ~ dnorm(0.9,  500)\n  Gp ~ dnorm(0.075,500)\n  Mp ~ dnorm(0.95, 500)\n\n  precbioindexP1 ~ dgamma(12.22, 1.1)\n  precbioindexR  ~ dgamma(12.22, 1.1)\n  precbioindexP  ~ dgamma(12.22, 1.1)\n\n  q1 ~ dunif(0.1,1)\n  q2 ~ dunif(0.1,1)\n  q3 ~ dunif(0.1,1)\n\n  precP1 ~ dgamma(12.22, 1.1)\n  precR  ~ dgamma(12.22, 1.1)\n  precP  ~ dgamma(12.22, 1.1)\n\n  M ~ dnorm(0.2, 100)\n}\n\"\n\n\n# ========================================================================================================================\n# --- Обучение модели ---\n# ========================================================================================================================\nset.seed(1)  # Для воспроизводимости\n# Инициализация модели JAGS\njm &lt;- jags.model(\n  textConnection(model_string),  # Модель из строки\n  data = data_list,             # Данные\n  n.chains = 3,                 # Количество цепей\n  n.adapt = 1500                # Длина адаптационной фазы\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 48\n   Unobserved stochastic nodes: 61\n   Total graph size: 576\n\nInitializing model\n\n# Обновление модели (burn-in)\nupdate(jm, 4000)\n\n# Переменные для мониторинга\nvars_to_monitor &lt;- c(\n  \"M\",\"Gp\",\"Gr\",\"Mp\",\"q1\",\"q2\",\"q3\",                    # Параметры\n  \"precP\",\"precP1\",\"precR\",\"precbioindexP\",\"precbioindexP1\",\"precbioindexR\",  # Точности\n  \"P\",\"P1\",\"R\",\"PR\",\"p.PRlim\"                           # Состояния и производные\n)\n\n\n# Генерация MCMC-выборок\nsamps &lt;- coda.samples(\n  jm, \n  variable.names = vars_to_monitor,  # Мониторируемые переменные\n  n.iter = 6000,                     # Длина выборки\n  thin = 3                           # Прореживание\n)\n# ========================================================================================================================\n# --- Анализ результатов ---\n# ========================================================================================================================\n# Стандартная статистика по выборкам\nsm &lt;- summary(samps)\nstats &lt;- sm$statistics   # Средние, SD, стандартные ошибки\nquants &lt;- sm$quantiles   # Квантили (2.5%, 25%, 50%, 75%, 97.5%)\n\n# Матрица всех сэмплов для ручных вычислений\ndraws_mat &lt;- as.matrix(samps)\n\n# Функция для расчета MC ошибки через эффективный размер выборки\nmcse_from_ess &lt;- function(vec) {\n  ess &lt;- effectiveSize(as.mcmc(vec))  # Эффективный размер выборки\n  sd(vec) / sqrt(as.numeric(ess))     # MC ошибка\n}\n\n# Функция для создания строки результата\nmake_row &lt;- function(year, mapping, node, mean, sd, mcse, q2.5, q25, q50, q75, q97.5) {\n  data.frame(\n    YEAR = year,\n    `#Vectors to monitor` = mapping,\n    node = node,\n    mean = mean,\n    sd = sd,\n    `MC error` = mcse,\n    `2.50%` = q2.5,\n    `25.00%` = q25,\n    median = q50,\n    `75.00%` = q75,\n    `97.50%` = q97.5,\n    check.names = FALSE\n  )\n}\n\n# Список для накопления результатов\nrows &lt;- list()\n\n# Функция добавления скалярных параметров\nadd_scalar &lt;- function(x_idx, vname) {\n  if (vname %in% rownames(stats)) {\n    # Если параметр есть в готовой статистике\n    m &lt;- stats[vname, \"Mean\"]\n    s &lt;- stats[vname, \"SD\"]\n    mcse &lt;- mcse_from_ess(draws_mat[, vname])\n    q &lt;- quants[vname, c(\"2.5%\", \"25%\", \"50%\", \"75%\", \"97.5%\")]\n    rows[[length(rows) + 1]] &lt;&lt;- make_row(NA, paste0(\"x[\", x_idx, \"]&lt;-\", vname), paste0(\"x[\", x_idx, \"]\"),\n                                          m, s, mcse, q[1], q[2], q[3], q[4], q[5])\n  } else if (vname %in% c(\"sigmaP1\",\"sigmaR\",\"sigmaP\")) {\n    # Для стандартных отклонений (преобразуем из точности)\n    src &lt;- switch(vname,\n                  sigmaP1 = \"precP1\",\n                  sigmaR  = \"precR\",\n                  sigmaP  = \"precP\")\n    if (src %in% colnames(draws_mat)) {\n      vec &lt;- sqrt(1 / draws_mat[, src])  # Преобразование precision -&gt; sigma\n      m &lt;- mean(vec); s &lt;- sd(vec); mcse &lt;- mcse_from_ess(vec)\n      q &lt;- quantile(vec, c(0.025,0.25,0.5,0.75,0.975))\n      rows[[length(rows) + 1]] &lt;&lt;- make_row(NA, paste0(\"x[\", x_idx, \"]&lt;-\", vname), paste0(\"x[\", x_idx, \"]\"),\n                                            m, s, mcse, q[1], q[2], q[3], q[4], q[5])\n    }\n  }\n}\n\n# Добавление основных параметров\nadd_scalar(1,  \"M\")\nadd_scalar(2,  \"q1\")\nadd_scalar(3,  \"q2\")\nadd_scalar(4,  \"q3\")\nadd_scalar(5,  \"sigmaP1\")\nadd_scalar(6,  \"sigmaR\")\nadd_scalar(7,  \"sigmaP\")\nadd_scalar(8,  \"precbioindexP1\")\nadd_scalar(9,  \"precbioindexR\")\nadd_scalar(10, \"precbioindexP\")\nadd_scalar(11, \"Gr\")\nadd_scalar(12, \"Gp\")\nadd_scalar(13, \"Mp\")\n\n# Функция добавления временных рядов\nadd_series &lt;- function(base_idx, varname, years) {\n  for (i in seq_along(years)) {\n    rn &lt;- paste0(varname, \"[\", i, \"]\")  # Имя переменной с индексом\n    if (!rn %in% rownames(stats)) next  # Пропуск если нет данных\n    m &lt;- stats[rn, \"Mean\"]\n    s &lt;- stats[rn, \"SD\"]\n    mcse &lt;- mcse_from_ess(draws_mat[, rn])\n    q &lt;- quants[rn, c(\"2.5%\", \"25%\", \"50%\", \"75%\", \"97.5%\")]\n    xi &lt;- base_idx + (i - 1)  # Вычисление индекса в выходной таблице\n    rows[[length(rows) + 1]] &lt;&lt;- make_row(years[i], paste0(\"x[\", xi, \"]&lt;-\", rn), paste0(\"x[\", xi, \"]\"),\n                                          m, s, mcse, q[1], q[2], q[3], q[4], q[5])\n  }\n}\n\n# Добавление временных рядов\nadd_series(100, \"P1\", YEAR)\nadd_series(200, \"R\",  YEAR)\nadd_series(300, \"P\",  YEAR)\n\n# Создание итоговой таблицы\nout_df &lt;- do.call(rbind, rows)\n\n# Создание групп для сортировки\nout_df$group &lt;- ifelse(is.na(out_df$YEAR), \"param\",\n                ifelse(grepl(\"&lt;-P1\\\\[\", out_df$`#Vectors to monitor`), \"P1\",\n                ifelse(grepl(\"&lt;-R\\\\[\",  out_df$`#Vectors to monitor`), \"R\", \"P\")))\n\n# Сортировка параметров по индексу\nparam_rows &lt;- out_df[out_df$group == \"param\", ]\nparam_idx  &lt;- as.numeric(sub(\".*\\\\[(\\\\d+)\\\\].*\", \"\\\\1\", param_rows$node))\nparam_rows &lt;- param_rows[order(param_idx), ]\n\n# Сортировка временных рядов по году\np1_rows &lt;- out_df[out_df$group == \"P1\", ]\np1_rows &lt;- p1_rows[order(p1_rows$YEAR), ]\n\nr_rows  &lt;- out_df[out_df$group == \"R\", ]\nr_rows  &lt;- r_rows[order(r_rows$YEAR), ]\n\np_rows  &lt;- out_df[out_df$group == \"P\", ]\np_rows  &lt;- p_rows[order(p_rows$YEAR), ]\n\n# Компоновка финальной таблицы\nout_df &lt;- rbind(param_rows, p1_rows, r_rows, p_rows)\nout_df$group &lt;- NULL  # Удаление вспомогательной колонки\n\n# Сохранение результатов\nwrite.csv(out_df, \"monitor_summary.csv\", row.names = FALSE)\ncat(\"Saved: monitor_summary.csv\\n\")\n\nSaved: monitor_summary.csv\n\n# Вывод структуры результатов\nstr(out_df)\n\n'data.frame':   61 obs. of  11 variables:\n $ YEAR               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ #Vectors to monitor: chr  \"x[1]&lt;-M\" \"x[2]&lt;-q1\" \"x[3]&lt;-q2\" \"x[4]&lt;-q3\" ...\n $ node               : chr  \"x[1]\" \"x[2]\" \"x[3]\" \"x[4]\" ...\n $ mean               : num  0.175 0.414 0.735 0.935 0.315 ...\n $ sd                 : num  0.0642 0.0912 0.1343 0.0602 0.0417 ...\n $ MC error           : num  0.002072 0.006449 0.008274 0.001534 0.000584 ...\n $ 2.50%              : num  0.0483 0.2653 0.4895 0.7802 0.2451 ...\n $ 25.00%             : num  0.131 0.347 0.633 0.909 0.285 ...\n $ median             : num  0.174 0.403 0.731 0.953 0.311 ...\n $ 75.00%             : num  0.218 0.468 0.839 0.98 0.34 ...\n $ 97.50%             : num  0.299 0.621 0.978 0.998 0.409 ...\n\n# ========================================================================================================================\n# Визуализация априорных и апостериорных параметров\n# Параметры: M, Gp, Gr, Mp, q1, q2, q3, precP1, precR, precP, precbioindexP1, precbioindexR, precbioindexP\n# И производные: sigmaP1, sigmaR, sigmaP\n# ========================================================================================================================\n\n# Сэмплируем приоры прямо из той же JAGS-модели (без данных)\nsample_priors_from_model &lt;- function(model_string, n_iter = 20000, n_adapt = 500) {\n  jm_prior &lt;- jags.model(textConnection(model_string), data = list(N = 0), n.chains = 1, n.adapt = n_adapt)\n  vars &lt;- c(\"M\",\"Gp\",\"Gr\",\"Mp\",\"q1\",\"q2\",\"q3\",\n            \"precP1\",\"precR\",\"precP\",\"precbioindexP1\",\"precbioindexR\",\"precbioindexP\")\n  priors &lt;- coda.samples(jm_prior, variable.names = vars, n.iter = n_iter)\n  as.matrix(priors)\n}\n\n# Получаем матрицы приоров и постериоров\nprior_mat &lt;- sample_priors_from_model(model_string, n_iter = 20000, n_adapt = 500)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 0\n   Unobserved stochastic nodes: 16\n   Total graph size: 33\n\nInitializing model\n\npost_mat  &lt;- as.matrix(samps)\n\n# Добавляем производные сигмы из прецизионов\nadd_sigmas &lt;- function(mat) {\n  add &lt;- function(dst, src) {\n    if (all(src %in% colnames(mat))) dst &lt;- cbind(dst, setNames(as.data.frame(sqrt(1/mat[, src, drop=FALSE])), gsub(\"^prec\",\"sigma\", src)))\n    dst\n  }\n  out &lt;- mat\n  out &lt;- add(out, c(\"precP1\"))\n  out &lt;- add(out, c(\"precR\"))\n  out &lt;- add(out, c(\"precP\"))\n  out\n}\nprior_mat &lt;- add_sigmas(prior_mat)\npost_mat  &lt;- add_sigmas(post_mat)\n\n# Список параметров для визуализации\nparams &lt;- intersect(\n  c(\"M\",\"Gp\",\"Gr\",\"Mp\",\"q1\",\"q2\",\"q3\",\n    \"sigmaP1\",\"sigmaR\",\"sigmaP\",\n    \"precbioindexP1\",\"precbioindexR\",\"precbioindexP\"),\n  union(colnames(prior_mat), colnames(post_mat))\n)\n\n# В long-формат\nmk_df &lt;- function(mat, label) {\n  if (is.null(mat) || nrow(mat) == 0) return(data.frame())\n  mat &lt;- mat[, intersect(colnames(mat), params), drop = FALSE]\n  reshape(\n    data.frame(iter = seq_len(nrow(mat)), mat, check.names = FALSE),\n    direction = \"long\", varying = params, v.names = \"value\", timevar = \"param\", times = params\n  )[, c(\"param\",\"value\")]\n}\nprior_df &lt;- mk_df(prior_mat, \"Prior\"); prior_df$dist &lt;- \"Prior\"\npost_df  &lt;- mk_df(post_mat,  \"Posterior\"); post_df$dist &lt;- \"Posterior\"\nplot_df  &lt;- rbind(prior_df, post_df)\n\n# Подписи\nparam_labels &lt;- c(\n  M=\"M (mortality)\", Gp=\"Gp\", Gr=\"Gr\", Mp=\"Mp\",\n  q1=\"q1\", q2=\"q2\", q3=\"q3\",\n  sigmaP1=\"sigmaP1\", sigmaR=\"sigmaR\", sigmaP=\"sigmaP\",\n  precbioindexP1=\"precbioindexP1\", precbioindexR=\"precbioindexR\", precbioindexP=\"precbioindexP\"\n)\nplot_df$param_f &lt;- factor(plot_df$param, levels = params, labels = unname(param_labels[params]))\n\n# График prior vs posterior (берёт priors из модели!)\nlibrary(ggplot2)\nggplot(plot_df, aes(x = value, color = dist, fill = dist)) +\n  geom_density(alpha = 0.25, linewidth = 0.7) +\n  facet_wrap(~ param_f, scales = \"free\", ncol = 4) +\n  scale_color_manual(values = c(\"Prior\" = \"#999999\", \"Posterior\" = \"#1b9e77\")) +\n  scale_fill_manual(values  = c(\"Prior\" = \"#bbbbbb\", \"Posterior\" = \"#1b9e77\")) +\n  labs(title = \"Априорные (из модели) vs апостериорные распределения\",\n       x = \"Значение\", y = \"Плотность\", color = \"\", fill = \"\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n# ========================================================================================================================\n# Бабл-плоты остатков P1, R, P по годам (2000–2015)\n# Требуется: объекты samps, data_list. Если YEAR не создан, создадим.\n# ========================================================================================================================\n\nif (!exists(\"YEAR\")) YEAR &lt;- 2000 + 0:(data_list$N - 1)\ndraws_mat &lt;- as.matrix(samps)\neps &lt;- 1.0e-6\n\nresid_bubble_summary &lt;- function(series, obs_vec, q_name, state_name_prefix) {\n  rows &lt;- list()\n  for (i in seq_along(obs_vec)) {\n    if (is.na(obs_vec[i])) next\n    q_draws     &lt;- draws_mat[, q_name]\n    state_draws &lt;- draws_mat[, paste0(state_name_prefix, \"[\", i, \"]\")]\n    # residual per draw: log(observed) - log(expected)\n    res_draws &lt;- log(obs_vec[i]) - log(eps + q_draws * state_draws)\n    r_mean &lt;- mean(res_draws, na.rm = TRUE)\n    rows[[length(rows) + 1]] &lt;- data.frame(\n      YEAR = YEAR[i],\n      series = series,\n      resid = r_mean,\n      abs_resid = abs(r_mean),\n      sign = ifelse(r_mean &gt;= 0, \"pos\", \"neg\")\n    )\n  }\n  do.call(rbind, rows)\n}\n\nb1 &lt;- resid_bubble_summary(\"P1\", data_list$bioindexP1, \"q1\", \"P1\")\nb2 &lt;- resid_bubble_summary(\"R\",  data_list$bioindexR,  \"q2\", \"R\")\nb3 &lt;- resid_bubble_summary(\"P\",  data_list$bioindexP,  \"q3\", \"P\")\nbubbles &lt;- rbind(b1, b2, b3)\n\n# Порядок рядов сверху вниз: P1, R, P\nbubbles$series &lt;- factor(bubbles$series, levels = c(\"P1\", \"R\", \"P\"))\n\n# Убираем пустое расстояние - используем минимальные интервалы\nlvl &lt;- c(\"P1\",\"R\",\"P\")\ny_map &lt;- setNames(c(1, 2, 3), lvl)  # Числовые позиции без больших промежутков\n\nbubbles$y_pos &lt;- unname(y_map[as.character(bubbles$series)])\n\n# Создаем вытянутый прямоугольный график\nggplot(bubbles, aes(x = YEAR, y = y_pos)) +\n  geom_point(aes(size = abs_resid, fill = sign), shape = 21, color = \"black\", alpha = 0.9) +\n  scale_fill_manual(values = c(neg = \"black\", pos = \"white\"),\n                    breaks = c(\"pos\",\"neg\"),\n                    labels = c(\"положительные\",\"отрицательные\"),\n                    name = \"\") +\n  scale_size_area(max_size = 12, name = \"Остатки\") +\n  scale_x_continuous(breaks = seq(2000, 2015, by = 2), limits = c(2000, 2015)) +\n  scale_y_continuous(breaks = unname(y_map), \n                     labels = names(y_map),\n                     limits = c(0.5, 3.5),  # Убираем пустое пространство сверху и снизу\n                     expand = c(0, 0)) +     # Убираем расширение осей\n  labs(title = \"Пузырьковая диаграмма остатков (лог-шкала): P1, R, P\", \n       x = \"Год\", \n       y = \"\") +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = \"top\",\n    panel.grid.major.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    aspect.ratio = 0.3,  # Делаем график вытянутым прямоугольником (ширина &gt; высоты)\n    plot.margin = margin(5, 10, 5, 5, \"pt\")  # Убираем лишние отступы вокруг графика\n  )\n\n\n\n\n\n\n\n# ========================================================================================================================\n# ДИНАМИКА ИНДЕКСОВ (ПРЕРЕКРУТЫ, РЕКРУТЫ, ПОСТРЕКРУТЫ) МОДЕЛЬНЫХ И ФАКТИЧЕСКИХ (ТОЧКИ)\n# ========================================================================================================================\n# Три графика динамики P1, R, P: медиана (линия), 95% ДИ (лента), точки — наблюдения,\n# приведённые к единому масштабу  делением на медиану q (Posterior median).\n# ========================================================================================================================\nif (!exists(\"YEAR\")) YEAR &lt;- 2000 + 0:(data_list$N - 1)\ndraws_mat &lt;- as.matrix(samps)\n\nseries_summary &lt;- function(varname, obs_vec, qname, series_label) {\n  med_q &lt;- median(draws_mat[, qname], na.rm = TRUE)\n  rows &lt;- vector(\"list\", length(obs_vec))\n  for (i in seq_along(obs_vec)) {\n    rn &lt;- paste0(varname, \"[\", i, \"]\")\n    if (!rn %in% colnames(draws_mat)) next\n    v &lt;- draws_mat[, rn]\n    qs &lt;- quantile(v, c(0.025, 0.5, 0.975), na.rm = TRUE)\n    obs_state &lt;- if (!is.na(obs_vec[i])) obs_vec[i] / med_q else NA_real_\n    rows[[i]] &lt;- data.frame(\n      YEAR = YEAR[i],\n      series = series_label,\n      median = qs[2],\n      lo = qs[1],\n      hi = qs[3],\n      obs = obs_state\n    )\n  }\n  do.call(rbind, rows)\n}\n\ndf_p1 &lt;- series_summary(\"P1\", data_list$bioindexP1, \"q1\", \"P1\")\ndf_r  &lt;- series_summary(\"R\",  data_list$bioindexR,  \"q2\", \"R\")\ndf_p  &lt;- series_summary(\"P\",  data_list$bioindexP,  \"q3\", \"P\")\n\n\np_P1 &lt;- ggplot(df_p1, aes(x = YEAR)) +\n  geom_ribbon(aes(ymin = lo, ymax = hi), fill = \"#1f77b4\", alpha = 0.2) +\n  geom_line(aes(y = median), color = \"#1f77b4\", linewidth = 1) +\n  geom_point(aes(y = obs), shape = 21, size = 2, color = \"black\", fill = \"white\", na.rm = TRUE) +\n  scale_x_continuous(breaks = seq(2000, 2015, by = 2), limits = c(2000, 2015)) +\n  labs(title = \"Моделируемая и фактическая (точки) динамика пререкрутов\", x = \"Годы\", y = \"Пререкруты (экз.)\") +\n  theme_minimal(base_size = 12)\n\nprint(p_P1)\n\n\n\n\n\n\n\np_R &lt;- ggplot(df_r, aes(x = YEAR)) +\n  geom_ribbon(aes(ymin = lo, ymax = hi), fill = \"#2ca02c\", alpha = 0.2) +\n  geom_line(aes(y = median), color = \"#2ca02c\", linewidth = 1) +\n  geom_point(aes(y = obs), shape = 21, size = 2, color = \"black\", fill = \"white\", na.rm = TRUE) +\n  scale_x_continuous(breaks = seq(2000, 2015, by = 2), limits = c(2000, 2015)) +\n  labs(title = \"Моделируемая и фактическая (точки) динамика рекрутов\", x = \"Годы\", y = \"Рекруты (экз.)\") +\n  theme_minimal(base_size = 12)\n\nprint(p_R)\n\n\n\n\n\n\n\np_P &lt;- ggplot(df_p, aes(x = YEAR)) +\n  geom_ribbon(aes(ymin = lo, ymax = hi), fill = \"#ff7f0e\", alpha = 0.2) +\n  geom_line(aes(y = median), color = \"#ff7f0e\", linewidth = 1) +\n  geom_point(aes(y = obs), shape = 21, size = 2, color = \"black\", fill = \"white\", na.rm = TRUE) +\n  scale_x_continuous(breaks = seq(2000, 2015, by = 2), limits = c(2000, 2015)) +\n  labs(title = \"Моделируемая и фактическая (точки) динамика пострекрутов\", x = \"Годы\", y = \"Пострекруты (экз.)\") +\n  theme_minimal(base_size = 12)\n\nprint(p_P)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Модель Catch-Survey Analysis (CSA)</span>"
    ]
  },
  {
    "objectID": "chapter 7.html",
    "href": "chapter 7.html",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "",
    "text": "8.1 Введение\nВ этой практической работе представлен цикл прикладного анализа зависимости пополнения запаса гидробионта от факторов среды (в том числе нерестового запаса): от подготовки данных и отбора предикторов до сравнения нескольких семейств моделей, выбора устойчивой к хронологии прогностической схемы и построения прогноза с доверительными интервалами. Подход ориентирован на начинающих, но использует современные приёмы: автоматический отбор признаков (Boruta, LASSO), сопоставление линейных/нелинейных моделей, time-slice валидацию и ансамблевый прогноз. • Целевая переменная: R3haddock — пополнение запаса. • Кандидатные предикторы: гидрометеорология (температуры T…), океанография (O…), биотические показатели (например, codTSB) и нерестовый запас (haddock68). • Цель анализа: понять, какие факторы и в каких формах оказываются значимыми, отобрать рабочий набор моделей и получить прогноз на 2022–2024 с оценкой неопределенности.\nНастоящий анализ разделен на несколько этапов:\nВходные данные для работы скрипта: RECRUITMENT.xlsx, а также промежуточный файл с готовым набором предикторов: selected_predictors_dataset.csv.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 7.html#введение",
    "href": "chapter 7.html#введение",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "",
    "text": "Выбор предикторов. Скрипт можно скачать по ссылке\nПостроение биологически мотивированных (механистических) нелинейных классических моделей «запас-пополнение» Рикера и Бивертона-Холта. Анализ их значимости и сравнение с моделями LM/GLM/GAM. Скрипт можно скачать по ссылке\nПостроение классических статистических моделей LM/GLM/GAM. Анализ их прогностических способностей и выполнение прогноза. Скрипт можно скачать по ссылке\nПолный цикл ( СКРИПТ) прикладного анализа зависимости пополнения запаса гидробионта от факторов среды, включающий:\n\n\nа) выбор предикторов;\nб) базовое сравнение различных моделей;\nв) выбор лучшей прогностической модели;\nг) ансамблевый прогноз.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 7.html#выбор-предикторов",
    "href": "chapter 7.html#выбор-предикторов",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "8.2 Выбор предикторов",
    "text": "8.2 Выбор предикторов\nВ процессе анализа факторов, влияющих на пополнение рыбных запасов, ключевым этапом является тщательная подготовка данных и отбор наиболее информативных предикторов, поскольку качество последующих моделей напрямую зависит от качества входных данных. Начиная с первичной обработки, мы приводим все потенциальные предикторы к числовому формату, так как большинство статистических и машинно-обучаемых моделей требуют именно такой представления данных, при этом заменяем строковые обозначения пропущенных значений «NA» на стандартные NA, что позволяет системе R корректно обрабатывать отсутствующие наблюдения. Для заполнения пропусков мы применяем медианную импутацию, которая представляет собой простой и устойчивый к выбросам метод, поскольку медиана менее чувствительна к экстремальным значениям по сравнению со средним. Хотя существуют и более сложные альтернативы, такие как множественная импутация с использованием пакета mice, KNN-импутация через recipes::step_impute_knn или даже методы, специально разработанные для временных рядов, например, фильтр Калмана или ARIMA-модели, медианная импутация остается практичным выбором для начального этапа анализа, особенно когда объем данных ограничен или временные зависимости не являются доминирующими. Следующим важным этапом является анализ корреляционной структуры данных, поскольку высокая мультиколлинеарность между предикторами может серьезно ухудшить интерпретацию моделей и завысить дисперсию оценок параметров, особенно в линейных моделях. Для автоматического выявления и устранения сильно коррелированных переменных мы используем функцию findCorrelation с пороговым значением коэффициента корреляции 0.8, что позволяет сохранить лишь один представитель из каждой группы высококоррелированных переменных. Хотя альтернативными подходами могут служить диагностика по значениям VIF или применение методов снижения размерности, таких как PLS или PCA, удаление явно коррелированных предикторов оказывается наиболее прямолинейным решением для обеспечения стабильности последующих моделей. Для автоматического отбора наиболее значимых предикторов мы применяем два дополнительных метода, которые по-разному подходят к этой задаче и тем самым обеспечивают взаимную проверку результатов. Boruta представляет собой обертку над алгоритмом Random Forest, которая генерирует «теневые» переменные, полученные путем случайного перемешивания исходных признаков, и сравнивает важность реальных предикторов с этими теневыми копиями, сохраняя только те переменные, чья важность статистически превосходит уровень шума. Этот метод особенно эффективен при наличии нелинейных зависимостей и взаимодействий между переменными, демонстрируя высокую устойчивость к шуму, хотя и требует больше вычислительных ресурсов и может излишне благоволить к группам коррелированных признаков. Параллельно мы применяем LASSO-регрессию из пакета glmnet, которая использует L1-регуляризацию для зануления коэффициентов слабо влияющих предикторов, тем самым выполняет отбор признаков в процессе оценки модели. При выборе оптимального значения параметра регуляризации lambda мы сознательно предпочитаем значение lambda.1se, которое соответствует более простой модели, но при этом находится в пределах одной стандартной ошибки от минимального значения ошибки, так как этот консервативный подход часто обеспечивает лучшую обобщающую способность на небольших выборках, характерных для экологических данных. Однако LASSO имеет свои ограничения: он чувствителен к масштабу переменных, что делает центрирование и стандартизацию обязательными предварительными шагами, и предполагает линейную форму зависимости между предикторами и откликом, что может не соответствовать реальной биологической природе процессов. Финальный набор предикторов формируется как объединение результатов Boruta и LASSO с учетом биологической логики, что повышает устойчивость отбора к случайным флуктуациям, присущим каждому отдельному методу, и гарантирует включение ключевых переменных, таких как нерестовый запас (haddock68), который биологически должен влиять на пополнение запаса. Для предварительной проверки значимости отобранных предикторов мы строим простую линейную модель, которая не предназначена для окончательного прогноза, но служит в качестве sanity-check, позволяя оценить порядок величины эффектов и выявить явно незначимые или противоречащие биологической логике переменные. Важно отметить несколько нюансов и потенциальных подводных камней, с которыми можно столкнуться на этом этапе: если распределение целевой переменной R3haddock сильно скошено, может потребоваться лог-трансформация или использование моделей, специально разработанных для положительных откликов, таких как Gamma GLM; корреляция между переменными не обязательно отражает причинно-следственные связи, и при удалении высококоррелированных предикторов мы можем потерять полезную информацию, поэтому в некоторых случаях лучше применять методы, сохраняющие информацию из всех переменных, например, PLS или GAM; наконец, медианная импутация, хотя и проста в применении, может быть недостаточно точной для временных рядов, где хронологически осмысленная импутация, такая как скользящая медиана или интерполяция, часто дает более реалистичные результаты, учитывающие естественную динамику экологических процессов. Таким образом, этап подготовки данных и отбора предикторов представляет собой критически важный фундамент для последующего построения качественных моделей прогнозирования пополнения рыбных запасов, где баланс между статистической строгостью и биологической интерпретируемостью определяет успех всего анализа.\nСкрипт целиком можно скачать по ссылке\n\n# ==============================================================================\n# 1) ВЫБОР ПРЕДИКТОРОВ\n# ------------------------------------------------------------------------------\n# Цель блока: привести данные к числовому виду, обработать пропуски, сократить\n# мультиколлинеарность (сильные корреляции), а затем автоматически выделить\n# кандидатов-предикторов двумя методами (Boruta, LASSO). В конце сформируем\n# финальный пул признаков и проверим их значимость в простой LM.\n# ==============================================================================\n\n# Установка и подключение необходимых библиотек\n# Для автоматического отбора предикторов нам понадобятся дополнительные пакеты\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\nЗагрузка требуемого пакета: pacman\n\n\nWarning: пакет 'pacman' был собран под R версии 4.5.1\n\npacman::p_load(\n  readxl, tidyverse, caret, corrplot, mgcv, randomForest, xgboost,\n  Boruta,GGally, FactoMineR, glmnet, recipes, rsample  # Новые библиотеки для автоматического отбора\n)\n\n# Очистка среды и установка рабочей директории\n# Совет: rm(list=ls()) очищает все объекты в памяти R; setwd задаёт папку,\n# где искать/сохранять файлы. Убедитесь, что путь корректен на вашей машине.\nrm(list = ls())\nsetwd(\"C:/RECRUITMENT/\")\n\n# Пакеты для расширенного отбора предикторов\n# Boruta — обёртка над Random Forest для отбора признаков;\n# glmnet — регуляризация (LASSO/ElasticNet) для отбора/усиления обобщающей способности;\n# FactoMineR — PCA и другие многомерные методы (используем как утилиту).\nlibrary(Boruta)   # Алгоритм обертки для отбора признаков\nlibrary(glmnet)   # LASSO-регрессия\nlibrary(FactoMineR) # PCA анализ\n\n\n# Загрузка и первичная обработка данных\n# Шаги: фильтруем годы, приводим типы к числовому, заменяем строковые \"NA\" на NA.\nDATA &lt;- readxl::read_excel(\"RECRUITMENT.xlsx\", sheet = \"RECRUITMENT\") %&gt;%\n  filter(YEAR &gt; 1989 & YEAR &lt; 2022) %&gt;%\n  # Преобразуем необходимые столбцы в числовой формат\n  mutate(\n    across(starts_with(\"T\"), as.numeric),\n    across(starts_with(\"I\"), as.numeric),\n    across(starts_with(\"O\"), as.numeric),\n  ) %&gt;%\n  # Обработка пропущенных значений (заменяем строку \"NA\" на NA)\n  mutate(across(where(is.character), ~na_if(., \"NA\")))\n\n# 1. Подготовка данных -------------------------------------------------------\n# Выделим все возможные предикторы, включая географию и индексы трески\n# Примечание: оставляем только числовые переменные, т.к. большинство моделей\n# требует числовой вход без категориальных уровней.\npredictors &lt;- DATA %&gt;% \n  select(-YEAR, -R3haddock) %&gt;% \n  select_if(is.numeric) # Только числовые переменные\n\n# Целевая переменная\nresponse &lt;- DATA$R3haddock\n\n# В статистическом анализе мы различаем:\n# - Отклик (response/target variable) - то, что мы пытаемся предсказать (в нашем случае R3haddock)\n# - Предикторы (predictors/features) - переменные, которые могут объяснять изменения отклика\n# Для корректного анализа важно, чтобы предикторы были числовыми или преобразованы в числовой формат.\n\n# 2. Обработка пропусков -----------------------------------------------------\n# Заполнение медианными значениями — простой и устойчивый способ справиться с NA.\n# Альтернативы: множественная иммутация (mice), KNN-impute и др.\npredictors_filled &lt;- predictors %&gt;%\n  mutate(across(everything(), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))\n\n# Заполнение медианой - простой и устойчивый метод обработки пропусков для числовых переменных.\n# Медиана предпочтительнее среднего, так как менее чувствительна к выбросам.\n\n# 3. Предварительный анализ корреляций ---------------------------------------\n# Зачем: высокие корреляции затрудняют интерпретацию и могут вредить ряду моделей.\ncor_matrix &lt;- cor(predictors_filled, use = \"complete.obs\")\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", tl.cex = 0.7)\n\n\n\n\n\n\n\n# Удаляем высокоскоррелированные предикторы (r &gt; 0.8)\n# Это механическое сокращение мультиколлинеарности до этапа отбора.\nhigh_cor &lt;- findCorrelation(cor_matrix, cutoff = 0.8)\npredictors_filtered &lt;- predictors_filled[, -high_cor]\n\n# Высокая корреляция между предикторами (мультиколлинеарность) может привести к нестабильности моделей.\n# Например, если два предиктора почти идентичны, модель может неустойчиво распределять их влияние на отклик.\n# Удаление сильно коррелированных переменных (r &gt; 0.8) помогает улучшить интерпретируемость и стабильность моделей.\n\n\n# 4. Автоматизированный отбор Boruta (обертка Random Forest) -----------------\n# Идея: определить признаки, которые важнее, чем случайный шум (shadow features).\n\n\n# Визуализация результатов\nplot(boruta_output, cex.axis = 0.7, las = 2)\n\n\n\n\n\n\n\nboruta_stats &lt;- attStats(boruta_output)\nselected_vars &lt;- getSelectedAttributes(boruta_output, withTentative = TRUE)\n\n# Boruta - это алгоритм отбора признаков, основанный на методе случайного леса.\n# Он сравнивает важность реальных переменных с \"теневыми\" переменными (случайными копиями),\n# чтобы определить, действительно ли переменная информативна.\n# Результаты Boruta показывают: \n#   - Confirmed (зеленые) - значимые предикторы\n#   - Tentative (желтые) - предикторы, близкие к порогу значимости\n#   - Rejected (красные) - незначимые предикторы\n\n\n# 5. LASSO с более строгим критерием ------------------------------------------\n# Идея: L1-регуляризация зануляет коэффициенты «слабых» предикторов.\n# Выбор lambda.1se вместо lambda.min — более консервативный (простая модель).\nx &lt;- as.matrix(predictors_filtered)\ny &lt;- response\n\n# LASSO (Least Absolute Shrinkage and Selection Operator) - метод регрессии с L1-регуляризацией,\n# который одновременно выполняет отбор признаков и оценку коэффициентов. [[8]]\n# Параметр lambda контролирует силу регуляризации:\n#   - lambda.min дает наименьшую ошибку, но может включать шумовые переменные\n#   - lambda.1se (на 1 стандартную ошибку больше) дает более простую модель с меньшим риском переобучения\n# Для прогнозирования мы предпочитаем более строгий критерий (lambda.1se), чтобы модель была устойчивее. [[1]]\n\n# Кросс-валидация\ncv_fit &lt;- cv.glmnet(x, y, alpha = 1, nfolds = 10)\nplot(cv_fit)\n\n\n\n\n\n\n\n# ИСПОЛЬЗУЕМ lambda.1se вместо lambda.min — СТРОЖЕ!\nlasso_coef &lt;- coef(cv_fit, s = \"lambda.1se\")  # &lt;-- Ключевое изменение!\nlasso_vars &lt;- rownames(lasso_coef)[lasso_coef[,1] != 0][-1]  # исключаем (Intercept)\n\n\n# 6. Сравнение отобранных предикторов ----------------------------------------\n# Полезно видеть, какие признаки отмечают оба метода (устойчивые кандидаты).\ncat(\"Boruta selected:\", length(selected_vars), \"variables\\n\")\n\nBoruta selected: 3 variables\n\nprint(selected_vars)\n\n[1] \"codTSB\" \"T12\"    \"I5\"    \n\ncat(\"\\nLASSO selected:\", length(lasso_vars), \"variables\\n\")\n\n\nLASSO selected: 5 variables\n\nprint(lasso_vars)\n\n[1] \"codTSB\" \"T12\"    \"NAO3\"   \"NAO4\"   \"NAO5\"  \n\n# 7. Финальный набор предикторов (объединение результатов) -------------------\n# Логика: объединяем списки, добавляем биологически важные переменные вручную.\nfinal_vars &lt;- union(selected_vars, lasso_vars) \n\n# Добавляем обязательные переменные по биологической логике\nmandatory &lt;- c(\"haddock68\")\nfinal_vars &lt;- union(final_vars, mandatory) %&gt;% unique()\n\n# Мы объединяем результаты двух методов отбора признаков для большей надежности.\n# Также добавляем переменную haddock68 (нерестовый запас), так как биологически \n# логично, что пополнение запаса напрямую зависит от численности производителей. \n# Это пример интеграции экспертных знаний в статистический анализ - важный принцип \n# при работе с данными в биологических науках.\n\n# 8. Проверка значимости -----------------------------------------------------\n# Быстрая оценка значимости с LM: не как окончательный вывод, а как sanity-check.\nfinal_model &lt;- lm(response ~ as.matrix(predictors_filled[, final_vars]))\nsummary(final_model)\n\n\nCall:\nlm(formula = response ~ as.matrix(predictors_filled[, final_vars]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-270986  -82376   -1037   98086  276129 \n\nCoefficients:\n                                                      Estimate Std. Error\n(Intercept)                                         -1.082e+06  3.943e+05\nas.matrix(predictors_filled[, final_vars])codTSB    -2.346e-01  5.536e-02\nas.matrix(predictors_filled[, final_vars])T12        3.864e+05  7.198e+04\nas.matrix(predictors_filled[, final_vars])I5        -1.825e+02  2.572e+03\nas.matrix(predictors_filled[, final_vars])NAO3      -5.801e+04  3.129e+04\nas.matrix(predictors_filled[, final_vars])NAO4       8.345e+04  3.035e+04\nas.matrix(predictors_filled[, final_vars])NAO5      -7.278e+04  2.488e+04\nas.matrix(predictors_filled[, final_vars])haddock68  1.232e-01  4.515e-01\n                                                    t value Pr(&gt;|t|)    \n(Intercept)                                          -2.744 0.011305 *  \nas.matrix(predictors_filled[, final_vars])codTSB     -4.238 0.000288 ***\nas.matrix(predictors_filled[, final_vars])T12         5.368 1.64e-05 ***\nas.matrix(predictors_filled[, final_vars])I5         -0.071 0.944028    \nas.matrix(predictors_filled[, final_vars])NAO3       -1.854 0.076118 .  \nas.matrix(predictors_filled[, final_vars])NAO4        2.750 0.011146 *  \nas.matrix(predictors_filled[, final_vars])NAO5       -2.925 0.007412 ** \nas.matrix(predictors_filled[, final_vars])haddock68   0.273 0.787227    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 158600 on 24 degrees of freedom\nMultiple R-squared:  0.7173,    Adjusted R-squared:  0.6348 \nF-statistic: 8.698 on 7 and 24 DF,  p-value: 2.58e-05\n\n# 9. Формирование финального датасета ----------------------------------------\n# Собираем набор с откликом и выбранными предикторами; удалим строки с NA.\nmodel_data &lt;- DATA %&gt;%\n  select(R3haddock, all_of(final_vars)) %&gt;%\n  drop_na()\n\n# Просмотр структуры финальных данных\nglimpse(model_data)\n\nRows: 32\nColumns: 8\n$ R3haddock &lt;dbl&gt; 812363, 389416, 99474, 98946, 118812, 63028, 147657, 83270, ~\n$ codTSB    &lt;dbl&gt; 913000, 1347064, 1687381, 2197863, 2112773, 1849957, 1697388~\n$ T12       &lt;dbl&gt; 4.72, 4.66, 4.24, 3.90, 3.96, 4.27, 4.16, 4.07, 4.23, 5.08, ~\n$ I5        &lt;dbl&gt; 43, 55, 26, 49, 56, 28, 52, 51, 69, 68, 41, 48, 50, 63, 40, ~\n$ NAO3      &lt;dbl&gt; 1.46, -0.20, 0.87, 0.67, 1.26, 1.25, -0.24, 1.46, 0.87, 0.23~\n$ NAO4      &lt;dbl&gt; 2.00, 0.29, 1.86, 0.97, 1.14, -0.85, -0.17, -1.02, -0.68, -0~\n$ NAO5      &lt;dbl&gt; -1.53, 0.08, 2.63, -0.78, -0.57, -1.49, -1.06, -0.28, -1.32,~\n$ haddock68 &lt;dbl&gt; 74586, 79205, 53195, 36337, 49122, 81514, 172177, 160886, 96~\n\n# Визуализация важности переменных\n# Внимание: важности от RF — относительные; сопоставляйте с предметной логикой.\nvar_importance &lt;- randomForest(R3haddock ~ ., data = model_data, importance = TRUE)\nvarImpPlot(var_importance, main = \"Важность предикторов\")\n\n\n\n\n\n\n\n# Перед окончательным выбором модели мы проверяем значимость предикторов с помощью линейной регрессии.\n# Функция summary() показывает p-значения коэффициентов - если p &lt; 0.05, переменная считается статистически значимой. \n# Визуализация важности переменных с помощью случайного леса дает дополнительную перспективу,\n# показывая, какие переменные наиболее информативны для предсказания без предположений о линейности.\n\n# ==============================================================================\n#  ПОДГОТОВКА ДАННЫХ\n# Создаём NAOspring, фиксируем финальный набор признаков, сохраняем CSV.\n# ------------------------------------------------------------------------------\n# Цель блока: стандартизировать набор признаков для дальнейшего сравнения\n# моделей и обеспечить воспроизводимость (фиксированный CSV с нужными полями).\n# ==============================================================================\n\n# 1.1 Пакеты и окружение\n# Примечание: блок повторяет базовую инициализацию для автономного запуска.\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(readxl, tidyverse, caret, corrplot)\n\nrm(list = ls())\nset.seed(123)\nsetwd(\"C:/RECRUITMENT/\")\n\n# 1.2 Загрузка исходных данных и приведение типов\nDATA &lt;- readxl::read_excel(\"RECRUITMENT.xlsx\", sheet = \"RECRUITMENT\") %&gt;%\n  filter(YEAR &gt; 1989 & YEAR &lt; 2022) %&gt;%\n  mutate(\n    across(starts_with(\"T\"), as.numeric),\n    across(starts_with(\"I\"), as.numeric),\n    across(starts_with(\"O\"), as.numeric),\n    across(where(is.character), ~na_if(., \"NA\"))\n  )\n\n# 1.3 Создаём NAOspring (если есть NAO3, NAO4, NAO5)\n# Идея: агрегируем весенний индекс NAO как среднее за месяцы 3–5.\nif (all(c(\"NAO3\",\"NAO4\",\"NAO5\") %in% names(DATA))) {\n  DATA &lt;- DATA %&gt;%\n    mutate(NAOspring = rowMeans(pick(NAO3, NAO4, NAO5), na.rm = TRUE)) %&gt;%\n    select(-NAO3, -NAO4, -NAO5)\n}\n\n# NAO (North Atlantic Oscillation) - важный климатический индекс, влияющий описывающий изменения атмосферного давления\n# над Северной Атлантикой. В частности, он отражает разницу в атмосферном давлении между Исландской депрессией и\n# Азорским максимумом. NAO влияет на силу и направление западных ветров, а также на траектории штормов в Северной Атлантике. \n# Мы создаем NAOspring как среднее значение за весенние месяцы (марта, апреля, мая),\n# так как именно в этот период происходят ключевые процессы, влияющие на нерест трески. \n# Создание составных переменных на основе экспертных знаний часто улучшает качество моделей.\n\n# 1.4 Финальный учебный набор предикторов (фиксируем)\n# Важно: проверяем присутствие нужных колонок и формируем компактный датасет.\nneeded &lt;- c(\"codTSB\", \"T12\", \"I5\", \"NAOspring\", \"haddock68\")\nstopifnot(all(needed %in% names(DATA)))\n\n# Сохраняем YEAR в CSV (ниже он будет отброшен при обучении, но нужен для графика)\nmodel_data &lt;- DATA %&gt;%\n  select(YEAR, all_of(needed), R3haddock) %&gt;%\n  drop_na()\n\nwrite.csv(model_data, \"selected_predictors_dataset.csv\", row.names = FALSE)\nglimpse(model_data)\n\nRows: 32\nColumns: 7\n$ YEAR      &lt;dbl&gt; 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, ~\n$ codTSB    &lt;dbl&gt; 913000, 1347064, 1687381, 2197863, 2112773, 1849957, 1697388~\n$ T12       &lt;dbl&gt; 4.72, 4.66, 4.24, 3.90, 3.96, 4.27, 4.16, 4.07, 4.23, 5.08, ~\n$ I5        &lt;dbl&gt; 43, 55, 26, 49, 56, 28, 52, 51, 69, 68, 41, 48, 50, 63, 40, ~\n$ NAOspring &lt;dbl&gt; 0.64333333, 0.05666667, 1.78666667, 0.28666667, 0.61000000, ~\n$ haddock68 &lt;dbl&gt; 74586, 79205, 53195, 36337, 49122, 81514, 172177, 160886, 96~\n$ R3haddock &lt;dbl&gt; 812363, 389416, 99474, 98946, 118812, 63028, 147657, 83270, ~",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 7.html#модели-запас-пополнение-рикера-и-бивертона-холта",
    "href": "chapter 7.html#модели-запас-пополнение-рикера-и-бивертона-холта",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "8.3 Модели «запас-пополнение» Рикера и Бивертона-Холта",
    "text": "8.3 Модели «запас-пополнение» Рикера и Бивертона-Холта\nМодели запас-пополнение представляют собой фундаментальный инструмент в оценке водных биоресурсов, которые гораздо больше, чем просто математические кривые, — это формализованные выражения фундаментальных биологических представлений о том, как численность родительского стада определяет успех следующего поколения. Среди классических моделей этого типа наиболее широко используются модель Рикера и модель Бивертона-Холта, каждая из которых отражает различные гипотезы о биологических процессах, происходящих в популяции. Модель Рикера, предложенная Уильямом Рикером в 1954 году и имеющая характерный горб на графике, выражается уравнением R = a*S*exp(-b*S), где R обозначает пополнение, S — нерестовый запас, а параметры a и b имеют четкую биологическую интерпретацию: a соответствует максимальной продуктивности на единицу запаса при очень низких плотностях, фактически отражая максимальное пополнение (количество рекрутов) на одного производителя, а b характеризует степень плотностной зависимости, определяющей точку, после которой начинается снижение из-за внутривидовой конкуренции. Эта модель предсказывает, что с ростом нерестового запаса пополнение сначала увеличивается, достигает максимума, а затем снижается, что отражает явление перенаселенности, когда чрезмерная плотность производителей приводит к конкуренции за ресурсы, нехватке корма для личинок, усилению каннибализма или даже эпидемиям, что в итоге снижает выход молоди — мы буквально видим, как чрезмерный успех закладывает семена будущего коллапса пополнения. В отличие от нее, модель Бивертона-Холта, разработанная в 1957 году, имеет вид R = aS/(1+b*S) и предполагает, что пополнение асимптотически приближается к предельному значению a/b, называемому Rmax, с увеличением нерестового запаса, без последующего снижения, что соответствует ситуации, когда основной лимитирующий фактор — это не внутривидовая конкуренция, а внешние условия: ограниченное количество нерестовых площадок, хищничество, которое не зависит от плотности, или просто конечная пропускная способность экосистемы для молоди. Эта модель идеально описывает сценарий, когда кривая плавно выходит на плато, символизируя насыщение, и представляет собой альтернативную логику, где главным ограничивающим фактором являются внешние, а не внутривидовые процессы.\nПри оценке параметров этих нелинейных моделей мы сталкиваемся с необходимостью применения специализированных методов, поскольку обычный метод наименьших квадратов не справляется с их сложной структурой; в нашем анализе мы используем улучшенный алгоритм nlsLM из пакета minpack.lm, который сочетает метод Левенберга-Марквардта с возможностью наложения ограничений на параметры, что важно для обеспечения биологической правдоподобности результатов, так как параметры a и b должны оставаться положительными. Для получения надежных начальных оценок параметров в модели Рикера мы применяем функцию srStarts из пакета FSA, которая автоматически определяет разумные стартовые значения на основе анализа данных, тогда как для модели Бивертона-Холта мы используем комбинацию автоматических и ручных подходов, оценивая a как среднее отношение R/S при низких значениях запаса и устанавливая разумные начальные значения для b с последующей защитой от некорректных значений. Однако подбор модели — это только полдела, и критически важно провести тщательную диагностику, поскольку самая большая ошибка — слепо применять эти модели, не задумываясь об их предпосылках. Мы строим график остатков, потому что любая закономерность в их распределении — это сигнал о том, что модель не уловила какой-то важный процесс в данных. Мы смотрим на доверительные интервалы параметров; если они невероятно широки, значит, наша модель перепараметризована для имеющихся данных, и её прогностическая сила будет сомнительной. Модель Рикера не будет работать, если в вашей системе нет механизма перенаселения, а модель Бивертона-Холта окажется бесполезной, если пополнение продолжает расти или, наоборот, обрушивается после достижения пика. Именно поэтому мы всегда начинаем с простого графика «запас-пополнение» — его форма сама подскажет, какая из концепций более адекватна для конкретной популяции.\nНо реальный мир часто бывает сложнее этих двух идеализированных сценариев. Что если система ведёт себя по-рикеровски при высокой численности, но при низкой — работает иначе? Здесь на помощь приходит модификация — модель Рикера с порогом, или hockey-stick модель, которая сочетает в себе линейный рост при малых запасах и плато или спад при высоких, что может быть биологически более оправдано для многих запасов, находящихся под прессом промысла. И здесь мы подходим к самому главному — интеграции классики и современности. Эти модели не являются застывшими реликтами, а служат мощным инструментом для создания гипотез. Если модель Рикера плохо описывает данные, особенно в области низких значений запаса, это прямой сигнал о том, что возможно, существует какой-то дополнительный лимитирующий фактор, не учтенный в модели. Возможно, это температура воды на ключевой стадии развития икры, наличие хищников или доступность корма. Таким образом, классические модели становятся трамплином для более сложного анализа, включающего средовые предикторы. Мы можем включить параметры модели Рикера в качестве фиксированных эффектов в GAM или использовать предсказания классической модели в качестве одного из входных признаков для Random Forest. Этот синтез позволяет нам сохранить биологическую интерпретируемость классических моделей и добавить к ним гибкость и прогностическую силу машинного обучения для учета сложных, нелинейных влияний окружающей среды. В сущности, мы строим мост между глубоким, но узким знанием, заключенным в одной кривой, и широким, но зачастую “черно-ящичным” прогнозом сложного алгоритма, пытаясь получить лучшее из двух миров. Среди распространенных подводных камней при работе с моделями запас-пополнение следует отметить высокую чувствительность к начальным значениям параметров, что может приводить к сходимости к локальным минимумам, необходимость учета неоднородности дисперсии ошибок, особенно при работе с данными, охватывающими широкий диапазон значений запаса, и влияние временных лагов, поскольку пополнение в текущем году может зависеть не только от нерестового запаса в том же году, но и от условий прошлых лет. Кроме того, чистые модели запас-пополнение часто оказываются недостаточными для точного прогнозирования, так как пополнение зависит не только от размера нерестового запаса, но и от множества экологических факторов, что делает целесообразным развитие этих моделей в направлении включения дополнительных предикторов, как это продемонстрировано в последующих разделах нашего анализа. Тем не менее, классические модели Рикера и Бивертона-Холта остаются важной отправной точкой в анализе динамики рыбных популяций, предоставляя интерпретируемую основу для понимания механизмов регулирования численности и служа эталоном для оценки добавленной ценности более сложных моделей, что особенно важно в условиях ограниченных данных, характерных для многих водных экосистем.\n\n# ==============================================================================\n# ПРАКТИЧЕСКОЕ ЗАНЯТИЕ: АНАЛИЗ ФАКТОРОВ, ВЛИЯЮЩИХ НА ПОПОЛНЕНИЕ \n# (КЛАССИЧЕСКИЕ МОДЕЛИ ЗАПАС-ПОПОЛНЕНИЕ)\n# Курс: \"Оценка водных биоресурсов в среде R (для начинающих)\"\n# ==============================================================================\n\n# Установка и подключение ТОЛЬКО необходимых библиотек\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,    # Манипуляции с данными и визуализация\n  FSA,          # Начальные оценки для моделей запас-пополнение\n  minpack.lm,   # Улучшенный алгоритм нелинейной регрессии (nlsLM)\n  car,          # Проверка допущений моделей\n  mgcv,         # Построение GAM-моделей\n  investr,      # Доверительные интервалы для нелинейных моделей\n   caret)       # Расчет RMSE\n\n# Очистка среды и установка рабочей директории\nrm(list = ls())\nsetwd(\"C:/RECRUITMENT/\")\n\n# 1. ЗАГРУЗКА ДАННЫХ -----------------------------------------------------------\nmodel_data &lt;- read.csv(\"selected_predictors_dataset.csv\", \n                      header = TRUE, \n                      stringsAsFactors = FALSE)\n\n# Проверка структуры данных\nstr(model_data)\n\n'data.frame':   32 obs. of  7 variables:\n $ YEAR     : int  1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 ...\n $ codTSB   : int  913000 1347064 1687381 2197863 2112773 1849957 1697388 1537459 1350918 1199169 ...\n $ T12      : num  4.72 4.66 4.24 3.9 3.96 4.27 4.16 4.07 4.23 5.08 ...\n $ I5       : int  43 55 26 49 56 28 52 51 69 68 ...\n $ NAOspring: num  0.6433 0.0567 1.7867 0.2867 0.61 ...\n $ haddock68: int  74586 79205 53195 36337 49122 81514 172177 160886 96380 37977 ...\n $ R3haddock: int  812363 389416 99474 98946 118812 63028 147657 83270 359701 386866 ...\n\n# Проверка на пропущенные значения (должно быть 0 после предобработки)\nsum(is.na(model_data))\n\n[1] 0\n\n# 2. ПОДГОТОВКА ДАННЫХ ДЛЯ МОДЕЛЕЙ ЗАПАС-ПОПОЛНЕНИЕ ----------------------------\nrec_data &lt;- data.frame(\n  S = model_data$haddock68,  # Нерестовый запас\n  R = model_data$R3haddock   # Пополнение\n)\n\n# 3. ПОДГОНКА МОДЕЛИ РИКЕРА ----------------------------------------------------\nricker_starts &lt;- FSA::srStarts(R ~ S, data = rec_data, type = \"Ricker\")\nricker_model &lt;- minpack.lm::nlsLM(\n  R ~ a * S * exp(-b * S),\n  data = rec_data,\n  start = ricker_starts,\n  lower = c(a = 0, b = 0)\n)\n\n# 4. ПОДГОНКА МОДЕЛИ БИВЕРТОНА-ХОЛТА -------------------------------------------\na_start &lt;- mean(rec_data$R[rec_data$S &lt; quantile(rec_data$S, 0.25)] / \n                rec_data$S[rec_data$S &lt; quantile(rec_data$S, 0.25)], na.rm = TRUE)\n\nif (is.na(a_start) || a_start &lt;= 0) a_start &lt;- 0.001\n\nbh_model &lt;- minpack.lm::nlsLM(\n  R ~ (a * S) / (1 + b * S),\n  data = rec_data,\n  start = list(a = a_start, b = 0.0001),\n  lower = c(a = 0.0001, b = 0.00001),\n  control = nls.lm.control(maxiter = 200)\n)\n\n# 5. ОЦЕНКА КАЧЕСТВА МОДЕЛЕЙ ---------------------------------------------------\n\ncalculate_R2 &lt;- function(model, data) {\n  predicted &lt;- predict(model, newdata = data)\n  residuals &lt;- data$R - predicted\n  SSE &lt;- sum(residuals^2)\n  SST &lt;- sum((data$R - mean(data$R))^2)\n  R2 &lt;- 1 - (SSE / SST)\n  n &lt;- nrow(data)\n  p &lt;- length(coef(model))\n  adj_R2 &lt;- 1 - ((n - 1) / (n - p - 1)) * (1 - R2)\n  return(list(R2 = R2, adj_R2 = adj_R2))\n}\n\ncalculate_pvalue &lt;- function(model, data) {\n  predicted &lt;- predict(model, newdata = data)\n  residuals &lt;- data$R - predicted\n  SSE &lt;- sum(residuals^2)\n  SST &lt;- sum((data$R - mean(data$R))^2)\n  SSR &lt;- SST - SSE\n  n &lt;- nrow(data)\n  p &lt;- length(coef(model))\n  F_stat &lt;- (SSR / (p - 1)) / (SSE / (n - p))\n  p_value &lt;- pf(F_stat, df1 = p - 1, df2 = n - p, lower.tail = FALSE)\n  return(p_value)\n}\n\nricker_r2 &lt;- calculate_R2(ricker_model, rec_data)\nbh_r2 &lt;- calculate_R2(bh_model, rec_data)\n\nricker_p &lt;- calculate_pvalue(ricker_model, rec_data)\nbh_p &lt;- calculate_pvalue(bh_model, rec_data)\n\ncat(\"AIC Рикера:\", AIC(ricker_model), \"\\n\")\n\nAIC Рикера: 891.9919 \n\ncat(\"AIC Бивертона-Холта:\", AIC(bh_model), \"\\n\")\n\nAIC Бивертона-Холта: 894.2029 \n\n# 6. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ --------------------------------------------------\nnew_data &lt;- data.frame(S = seq(min(rec_data$S), max(rec_data$S), length.out = 100))\nricker_ci &lt;- investr::predFit(ricker_model, newdata = new_data, interval = \"confidence\")\nbh_ci &lt;- investr::predFit(bh_model, newdata = new_data, interval = \"confidence\")\n\nplot_data &lt;- new_data %&gt;%\n  mutate(\n    ricker_pred = predict(ricker_model, newdata = .),\n    ricker_lwr = ricker_ci[, \"lwr\"],\n    ricker_upr = ricker_ci[, \"upr\"],\n    bh_pred = predict(bh_model, newdata = .),\n    bh_lwr = bh_ci[, \"lwr\"],\n    bh_upr = bh_ci[, \"upr\"]\n  )\n\nggplot() +\n  geom_point(data = rec_data, aes(x = S, y = R), \n             color = \"darkgray\", size = 3, alpha = 0.7) +\n  geom_ribbon(data = plot_data, aes(x = S, ymin = ricker_lwr, ymax = ricker_upr), \n              fill = \"red\", alpha = 0.2) +\n  geom_ribbon(data = plot_data, aes(x = S, ymin = bh_lwr, ymax = bh_upr), \n              fill = \"blue\", alpha = 0.2) +\n  geom_line(data = plot_data, aes(x = S, y = ricker_pred), \n            color = \"red\", linewidth = 1.2) +\n  geom_line(data = plot_data, aes(x = S, y = bh_pred), \n            color = \"blue\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(\n    title = \"Сравнение моделей запас-пополнение\",\n    subtitle = paste0(\n      \"Рикер: R² = \", round(ricker_r2$R2, 2), \", p = \", format.pval(ricker_p, digits = 3),\n      \" | Бивертон-Холт: R² = \", round(bh_r2$R2, 2), \", p = \", format.pval(bh_p, digits = 3)\n    ),\n    x = \"Нерестовый запас\",\n    y = \"Пополнение\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n# 7. СРАВНЕНИЕ С ДРУГИМИ ТИПАМИ МОДЕЛЕЙ ----------------------------------------\n\nlm_model &lt;- lm(R3haddock ~ ., data = model_data)\nglm_model &lt;- glm(R3haddock ~ ., family = Gamma(link = \"log\"), data = model_data)\ngam_model &lt;- mgcv::gam(R3haddock ~ s(haddock68) + s(codTSB) + s(T12) + s(I5) + s(NAOspring),\n               data = model_data, method = \"REML\")\n\nmodel_comparison &lt;- data.frame(\n  Model = c(\"Рикер\", \"Бивертон-Холт\", \"LM\", \"GLM\", \"GAM\"),\n  AIC = c(AIC(ricker_model), AIC(bh_model), AIC(lm_model), AIC(glm_model), AIC(gam_model)),\n  R2 = c(\n    ricker_r2$R2, \n    bh_r2$R2, \n    summary(lm_model)$r.squared,\n    cor(model_data$R3haddock, predict(glm_model, type = \"response\"))^2,\n    summary(gam_model)$r.sq\n  )\n)\n\nprint(model_comparison)\n\n          Model      AIC          R2\n1         Рикер 891.9919 0.072305265\n2 Бивертон-Холт 894.2029 0.005938625\n3            LM 877.0895 0.573972535\n4           GLM 857.0346 0.566389771\n5           GAM 862.9064 0.738660678\n\n# 8. ИНТЕРПРЕТАЦИЯ РЕЗУЛЬТАТОВ -------------------------------------------------\ncat(\"\\nПараметры модели Рикера:\")\n\n\nПараметры модели Рикера:\n\ncat(\"\\na =\", coef(ricker_model)[1], \"- максимальная продукция потомства\")\n\n\na = 7.931302 - максимальная продукция потомства\n\ncat(\"\\nb =\", coef(ricker_model)[2], \"- коэффициент плотностной зависимости\")\n\n\nb = 7.4007e-06 - коэффициент плотностной зависимости\n\ncat(\"\\n\\nПараметры модели Бивертона-Холта:\")\n\n\n\nПараметры модели Бивертона-Холта:\n\ncat(\"\\na =\", coef(bh_model)[1], \"- максимальное пополнение на особь\")\n\n\na = 43.77667 - максимальное пополнение на особь\n\ncat(\"\\nb =\", coef(bh_model)[2], \"- коэффициент внутривидовой конкуренции\")\n\n\nb = 0.0001247403 - коэффициент внутривидовой конкуренции\n\n# ==============================================================================\n# 7. СРАВНЕНИЕ С ДРУГИМИ ТИПАМИ МОДЕЛЕЙ ----------------------------------------\n\n# Построение линейной модели LM\nlm_model &lt;- lm(R3haddock ~ ., data = model_data)\n\n# Диагностика\npar(mfrow = c(2, 2))\nplot(lm_model)\n\n\n\n\n\n\n\nvif(lm_model)  # Проверка мультиколлинеарности\n\n     YEAR    codTSB       T12        I5 NAOspring haddock68 \n 2.925549  3.439340  1.970023  1.332529  1.181897  2.522569 \n\n# Интерпретация\nsummary(lm_model)\n\n\nCall:\nlm(formula = R3haddock ~ ., data = model_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-279162 -111056  -35757  141083  324173 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.448e+07  1.224e+07   1.183 0.248123    \nYEAR        -7.863e+03  6.248e+03  -1.258 0.219869    \ncodTSB      -1.888e-01  7.817e-02  -2.416 0.023338 *  \nT12          4.339e+05  9.738e+04   4.456 0.000153 ***\nI5          -2.568e+03  3.058e+03  -0.840 0.409036    \nNAOspring   -7.666e+04  5.735e+04  -1.337 0.193325    \nhaddock68    2.782e-01  5.485e-01   0.507 0.616444    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 190800 on 25 degrees of freedom\nMultiple R-squared:  0.574, Adjusted R-squared:  0.4717 \nF-statistic: 5.614 on 6 and 25 DF,  p-value: 0.0008393\n\n# Построение обобщенной линейной модели GLM\nglm_model &lt;- glm(R3haddock ~ ., \n                family = Gamma(link = \"log\"), \n                data = model_data)\nsummary(glm_model)\n\n\nCall:\nglm(formula = R3haddock ~ ., family = Gamma(link = \"log\"), data = model_data)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.375e+01  3.667e+01   0.648   0.5231    \nYEAR        -8.601e-03  1.872e-02  -0.459   0.6499    \ncodTSB      -5.945e-07  2.342e-07  -2.539   0.0177 *  \nT12          1.411e+00  2.917e-01   4.837 5.68e-05 ***\nI5           7.430e-03  9.161e-03   0.811   0.4250    \nNAOspring    1.508e-02  1.718e-01   0.088   0.9307    \nhaddock68    9.422e-07  1.643e-06   0.573   0.5715    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.326724)\n\n    Null deviance: 19.8880  on 31  degrees of freedom\nResidual deviance:  8.4535  on 25  degrees of freedom\nAIC: 857.03\n\nNumber of Fisher Scoring iterations: 9\n\n# Построение обобщенной аддитивной модели GАM\nlibrary(mgcv)\ngam_model &lt;- gam(R3haddock ~ \n                 s(codTSB) + \n                 s(T12) + \n                 s(I5) + \n                 s(NAOspring) + \n                 s(haddock68),\n               data = model_data,\n               method = \"REML\")\nsummary(gam_model)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nR3haddock ~ s(codTSB) + s(T12) + s(I5) + s(NAOspring) + s(haddock68)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   320163      23724   13.49 4.37e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n               edf Ref.df     F p-value   \ns(codTSB)    2.354  2.899 1.684 0.17581   \ns(T12)       2.190  2.676 5.908 0.00357 **\ns(I5)        4.642  5.539 1.518 0.15004   \ns(NAOspring) 1.293  1.503 1.154 0.22854   \ns(haddock68) 1.824  2.200 0.629 0.65124   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.739   Deviance explained = 84.2%\n-REML = 361.62  Scale est. = 1.8011e+10  n = 32\n\nplot(gam_model, pages = 1, residuals = TRUE)\n\n\n\n\n\n\n\n# Таблица сравнения моделей\n# Сравнение моделей\nmodel_comparison &lt;- data.frame(\n  Model = c(\"Рикер\", \"Бивертон-Холт\", \"LM\", \"GLM\", \"GAM\"),\n  AIC = c(AIC(ricker_model), AIC(bh_model), AIC(lm_model), AIC(glm_model), AIC(gam_model)),\n  R2 = c(ricker_r2$R2, bh_r2$R2, summary(lm_model)$r.squared, \n         cor(model_data$R3haddock, predict(glm_model))^2, \n         summary(gam_model)$r.sq),  # Используем summary(gam_model)$r.sq для R^2\n  Adj_R2 = c(ricker_r2$adj_R2, bh_r2$adj_R2, summary(lm_model)$adj.r.squared, NA, \n             summary(gam_model)$r.sq),  # Используем summary(gam_model)$r.sq для Adjusted R^2\n  RMSE = c(RMSE(predict(ricker_model), rec_data$R), \n           RMSE(predict(bh_model), rec_data$R),\n           RMSE(predict(lm_model), model_data$R3haddock),\n           RMSE(predict(glm_model, type = \"response\"), model_data$R3haddock),\n           RMSE(predict(gam_model, type = \"response\"), model_data$R3haddock))\n)\n\n# Вывод таблицы\nprint(model_comparison)\n\n          Model      AIC          R2       Adj_R2     RMSE\n1         Рикер 891.9919 0.072305265  0.008326318 248869.6\n2 Бивертон-Холт 894.2029 0.005938625 -0.062617332 257617.8\n3            LM 877.0895 0.573972535  0.471725944 168650.7\n4           GLM 857.0346 0.502625978           NA 170386.6\n5           GAM 862.9064 0.738660678  0.738660678 102584.0\n\n# ==============================================================================\n# ВИЗУАЛИЗАЦИЯ ВСЕХ МОДЕЛЕЙ НА ОДНОМ ГРАФИКЕ \n# ==============================================================================\n\n# Фиксируем другие предикторы на их средних значениях (исключая haddock68)\nmean_values &lt;- model_data %&gt;%\n  select(-R3haddock, -haddock68) %&gt;%\n  summarise(across(everything(), ~ mean(.x, na.rm = TRUE)))\n\n# Расширяем new_data, добавляя средние значения других предикторов\nnew_data_full &lt;- new_data %&gt;%\n  bind_cols(mean_values[rep(1, nrow(new_data)), ]) %&gt;%\n  rename(haddock68 = S)  # Переименовываем S в haddock68 для совместимости\n\n# Получаем предсказания для всех моделей\nnew_data_full &lt;- new_data_full %&gt;%\n  mutate(\n    # Предсказания для моделей запаса-пополнения\n    ricker_pred = predict(ricker_model, newdata = data.frame(S = haddock68)),\n    bh_pred = predict(bh_model, newdata = data.frame(S = haddock68)),\n    \n    # Предсказания для линейной модели (LM)\n    lm_pred = predict(lm_model, newdata = .),\n    \n    # Предсказания для обобщенной линейной модели (GLM)\n    glm_pred = predict(glm_model, newdata = ., type = \"response\"),\n    \n    # Предсказания для обобщенной аддитивной модели (GAM)\n    gam_pred = predict(gam_model, newdata = ., type = \"response\")\n  )\n\n# Создаем длинный формат данных для ggplot\nplot_data &lt;- new_data_full %&gt;%\n  select(haddock68, ricker_pred, bh_pred, lm_pred, glm_pred, gam_pred) %&gt;%\n  pivot_longer(\n    cols = -haddock68,\n    names_to = \"model\",\n    values_to = \"prediction\"\n  ) %&gt;%\n  mutate(\n    model = case_when(\n      model == \"ricker_pred\" ~ \"Рикер\",\n      model == \"bh_pred\" ~ \"Бивертон-Холт\",\n      model == \"lm_pred\" ~ \"Линейная (LM)\",\n      model == \"glm_pred\" ~ \"Обобщенная линейная (GLM)\",\n      model == \"gam_pred\" ~ \"Обобщенная аддитивная (GAM)\",\n      TRUE ~ model\n    )\n  )\n\n# Создаем палитру цветов для моделей\nmodel_colors &lt;- c(\n  \"Рикер\" = \"#E41A1C\",          # Красный\n  \"Бивертон-Холт\" = \"#377EB8\",  # Синий\n  \"Линейная (LM)\" = \"#4DAF4A\",  # Зеленый\n  \"Обобщенная линейная (GLM)\" = \"#984EA3\", # Фиолетовый\n  \"Обобщенная аддитивная (GAM)\" = \"#FF7F00\" # Оранжевый\n)\n\n# Создаем график\nggplot() +\n  # Точки исходных данных\n  geom_point(data = rec_data, aes(x = S, y = R), \n             color = \"darkgray\", size = 2.5, alpha = 0.7) +\n  \n  # Линии предсказаний моделей\n  geom_line(data = plot_data, \n            aes(x = haddock68, y = prediction, color = model, linetype = model),\n            linewidth = 1.2) +\n  \n  # Настройка цветов и типов линий\n  scale_color_manual(values = model_colors) +\n  scale_linetype_manual(values = c(\n    \"Рикер\" = \"solid\",\n    \"Бивертон-Холт\" = \"dashed\",\n    \"Линейная (LM)\" = \"dotdash\",\n    \"Обобщенная линейная (GLM)\" = \"longdash\",\n    \"Обобщенная аддитивная (GAM)\" = \"twodash\"\n  )) +\n  \n  # Подписи и темы\n  labs(\n    title = \"Сравнение моделей зависимости пополнения от нерестового запаса\",\n    subtitle = \"Фиксация других предикторов на средних значениях\",\n    x = \"Нерестовый запас (тыс. тонн)\",\n    y = \"Пополнение (млн особей)\",\n    color = \"Модель\",\n    linetype = \"Модель\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray30\"),\n    axis.title = element_text(size = 12),\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\",\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA, linewidth = 0.5)\n  ) +\n  guides(\n    color = guide_legend(nrow = 2, byrow = TRUE),\n    linetype = guide_legend(nrow = 2, byrow = TRUE)\n  )",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 7.html#статистические-модели-lmglmgam",
    "href": "chapter 7.html#статистические-модели-lmglmgam",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "8.4 Статистические модели LM/GLM/GAM",
    "text": "8.4 Статистические модели LM/GLM/GAM\nСтатистические модели линейной регрессии (LM), обобщенной линейной регрессии (GLM) и обобщенной аддитивной регрессии (GAM) представляют собой мощный и взаимодополняющий набор инструментов для анализа водных биоресурсов, позволяющий исследователям от простых линейных зависимостей переходить к сложным нелинейным взаимодействиям, сохраняя при этом интерпретируемость результатов. Линейная модель (LM) служит фундаментом для всего статистического анализа в гидробиологии, основываясь на предположении, что зависимость между предикторами и откликом является линейной, а остатки распределены нормально с постоянной дисперсией. Эта модель предоставляет простую интерпретацию коэффициентов как величины изменения отклика при единичном изменении предиктора, что особенно ценно при работе с такими биологическими показателями, как пополнение запаса или нерестовая биомасса. Однако при анализе водных биоресурсов мы часто сталкиваемся с данными, которые нарушают ключевые предположения LM: пополнение рыбы или беспозвоночного не может быть отрицательным, его распределение обычно сильно скошено вправо, а дисперсия часто увеличивается с ростом среднего значения. Именно здесь на помощь приходит обобщенная линейная модель (GLM), расширяющая возможности LM за счет введения двух ключевых компонентов — экспоненциального семейства распределений и связующей функции (link-function). Для данных о рыбных запасах особенно полезно Gamma-распределение с логарифмической связкой, которое учитывает положительность отклика и мультипликативную природу ошибок, характерную для биологических данных. В отличие от LM, где мы интерпретируем коэффициенты как абсолютные изменения, в GLM с лог-связкой коэффициенты отражают относительные изменения: увеличение предиктора на единицу приводит к умножению ожидаемого отклика на exp(коэффициент), что соответствует биологической реальности, где эффекты часто действуют мультипликативно, а не аддитивно. Но даже GLM сохраняет ограничение на линейность в преобразованном пространстве, что может быть недостаточным для описания сложных экологических зависимостей, таких как оптимальный диапазон температуры для нереста или пороговые эффекты средовых факторов. Здесь в игру вступают обобщенные аддитивные модели (GAM), которые заменяют линейные комбинации предикторов на гладкие функции, оцениваемые с помощью сплайнов, что позволяет моделировать практически любые нелинейные зависимости без предварительного задания их формы. GAM сохраняет интерпретируемость линейных моделей, так как каждая гладкая функция может быть визуализирована и проанализирована отдельно, показывая, как именно каждый фактор влияет на пополнение запаса, будь то монотонный рост, оптимум с максимумом или сложная колебательная зависимость. При работе с GAM особое внимание уделяется выбору степени гладкости, так как чрезмерно гибкие функции могут переобучиться на шум в данных, тогда как недостаточно гибкие не уловят реальные биологические закономерности; в пакете mgcv это решается автоматически через метод максимального правдоподобия с штрафом (REML), который балансирует качество подгонки и гладкость функций. Сравнивая эти модели с классическими моделями запас-пополнение, мы видим, что GAM может рассматриваться как их естественное обобщение: вместо фиксированной формы кривой Рикера или Бивертона-Холта GAM позволяет данным “говорить за себя”, выявляя оптимальную форму зависимости без предварительных гипотез, при этом сохраняя возможность включить нерестовый запас как один из гладких членов в модель, дополненный другими экологическими факторами. Однако при всей своей гибкости, GAM, как и LM с GLM, требует тщательной проверки предположений: мы анализируем графики остатков против предсказанных значений, чтобы убедиться в отсутствии систематических отклонений, проверяем нормальность остатков (для LM) или соответствие выбранному распределению (для GLM/GAM), и исследуем влияние влиятельных точек, которые могут исказить результаты, особенно в условиях ограниченных данных, характерных для гидробиологических исследований. Выбор между LM, GLM и GAM должен основываться не только на статистических критериях, таких как AIC или кросс-валидация, но и на биологической интерпретируемости результатов: иногда более простая модель с меньшей точностью предпочтительнее сложного “черного ящика”, особенно когда результаты должны быть понятны начальникам и менеджерам рыболовства. Практический подход, который обычно рекомендуется начинающим ихтиологам/гидробиологам, состоит в последовательном усложнении модели: начните с классической модели запас-пополнение, затем добавьте средовые факторы через LM/GLM, и только если зависимости явно нелинейны, перейдите к GAM, всегда проверяя, действительно ли усложнение модели приводит к биологически значимому улучшению понимания процесса. Важно помнить, что статистическая модель — это не самоцель, а инструмент для понимания биологических процессов, и даже самая изощренная модель бесполезна, если её результаты нельзя перевести на язык биологии и применить для устойчивого управления водными ресурсами. В конечном счете, сочетание классических представлений об экосистемах с современными статистическими методами позволяет нам строить мост между фундаментальной биологией и прикладной оценкой запасов, где каждая модель, от простой линейной регрессии до сложного GAM, вносит свой вклад в формирование целостного понимания динамики водных биоресурсов.\n\n# ==============================================================================\n# Версия: только LM / GLM(Gamma) / GAM\n# Без caret/train: стандартная оценка параметров lm/glm/gam, собственная time-slice CV,\n# выбор лучшей модели, прогноз 2022–2024, эмпирические интервалы и график.\n# ==============================================================================\n\n# 0) Пакеты и окружение --------------------------------------------------------\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  readxl, tidyverse, mgcv, lmtest, car, ggplot2, corrplot\n)\n\nrm(list = ls())\nset.seed(123)\nsetwd(\"C:/RECRUITMENT/\")  # при необходимости измените путь\n\n\n# 1) Подготовка данных ---------------------------------------------------------\n# Загрузка, приведение типов, создание NAOspring, фиксируем набор признаков\nDATA &lt;- readxl::read_excel(\"RECRUITMENT.xlsx\", sheet = \"RECRUITMENT\") %&gt;%\n  filter(YEAR &gt; 1989 & YEAR &lt; 2022) %&gt;%\n  mutate(\n    across(starts_with(\"T\"), as.numeric),\n    across(starts_with(\"I\"), as.numeric),\n    across(starts_with(\"O\"), as.numeric),\n    across(where(is.character), ~na_if(., \"NA\"))\n  )\n\nif (all(c(\"NAO3\",\"NAO4\",\"NAO5\") %in% names(DATA))) {\n  DATA &lt;- DATA %&gt;%\n    mutate(NAOspring = rowMeans(pick(NAO3, NAO4, NAO5), na.rm = TRUE)) %&gt;%\n    select(-NAO3, -NAO4, -NAO5)\n}\n\nneeded &lt;- c(\"codTSB\", \"T12\", \"I5\", \"NAOspring\", \"haddock68\")\nstopifnot(all(needed %in% names(DATA)))\n\nmodel_data &lt;- DATA %&gt;%\n  select(YEAR, all_of(needed), R3haddock) %&gt;%\n  drop_na() %&gt;%\n  arrange(YEAR)\n\nwrite.csv(model_data, \"selected_predictors_dataset.csv\", row.names = FALSE)\nglimpse(model_data)\n\nRows: 32\nColumns: 7\n$ YEAR      &lt;dbl&gt; 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, ~\n$ codTSB    &lt;dbl&gt; 913000, 1347064, 1687381, 2197863, 2112773, 1849957, 1697388~\n$ T12       &lt;dbl&gt; 4.72, 4.66, 4.24, 3.90, 3.96, 4.27, 4.16, 4.07, 4.23, 5.08, ~\n$ I5        &lt;dbl&gt; 43, 55, 26, 49, 56, 28, 52, 51, 69, 68, 41, 48, 50, 63, 40, ~\n$ NAOspring &lt;dbl&gt; 0.64333333, 0.05666667, 1.78666667, 0.28666667, 0.61000000, ~\n$ haddock68 &lt;dbl&gt; 74586, 79205, 53195, 36337, 49122, 81514, 172177, 160886, 96~\n$ R3haddock &lt;dbl&gt; 812363, 389416, 99474, 98946, 118812, 63028, 147657, 83270, ~\n\n# 2) Формулы моделей и вспомогательные функции --------------------------------\nf_lm  &lt;- as.formula(\"R3haddock ~ codTSB + T12 + I5 + NAOspring + haddock68\")\nf_gam &lt;- as.formula(\"R3haddock ~ s(codTSB,bs='tp',k=5) + s(T12,bs='tp',k=5) + s(I5,bs='tp',k=5) + s(NAOspring,bs='tp',k=5) + s(haddock68,bs='tp',k=5)\")\n\nrmse &lt;- function(a, p) sqrt(mean((a - p)^2, na.rm = TRUE))\nmae  &lt;- function(a, p) mean(abs(a - p), na.rm = TRUE)\nr2   &lt;- function(a, p) 1 - sum((a - p)^2, na.rm = TRUE) / sum((a - mean(a))^2, na.rm = TRUE)\n\nsafe_fit &lt;- function(expr) {\n  out &lt;- try(eval(expr), silent = TRUE)\n  if (inherits(out, \"try-error\")) NULL else out\n}\n\n\n# 3) Time-slice CV (expanding window, h=3) + хронологический тест -------------\nmd &lt;- model_data\nmd_for_fit &lt;- md %&gt;% select(codTSB, T12, I5, NAOspring, haddock68, R3haddock)\n\nn &lt;- nrow(md_for_fit)\nholdout_frac &lt;- 0.2\nn_test &lt;- max(4, ceiling(n * holdout_frac))\ntrain_ts &lt;- head(md_for_fit, n - n_test)\ntest_ts  &lt;- tail(md_for_fit, n_test)\n\nn_train &lt;- nrow(train_ts)\ninitial_frac &lt;- 0.6\nhorizon      &lt;- 3\ninitialWindow &lt;- max(10, floor(initial_frac * n_train))\nif (initialWindow + horizon &gt; n_train) initialWindow &lt;- n_train - horizon\n\n# Аккумулируем метрики и остатки по срезам\ncv_summ &lt;- tibble(Model = character(), RMSE = double(), MAE = double())\nresids_cv &lt;- list(LM = c(), GLM = c(), GAM = c())\n\nslice_id &lt;- 0\nfor (i in seq(initialWindow, n_train - horizon)) {\n  slice_id &lt;- slice_id + 1\n  idx_tr &lt;- 1:i\n  idx_te &lt;- (i+1):(i+horizon)\n  dtr &lt;- train_ts[idx_tr, ]\n  dte &lt;- train_ts[idx_te, ]\n\n  # LM\n  lm_fit &lt;- safe_fit(quote(lm(f_lm, data = dtr)))\n  if (!is.null(lm_fit)) {\n    pr &lt;- try(predict(lm_fit, newdata = dte), silent = TRUE)\n    if (!inherits(pr, \"try-error\")) {\n      cv_summ &lt;- add_row(cv_summ, Model = \"LM\", RMSE = rmse(dte$R3haddock, pr), MAE = mae(dte$R3haddock, pr))\n      resids_cv$LM &lt;- c(resids_cv$LM, dte$R3haddock - pr)\n    }\n  }\n\n  # GLM (Gamma)\n  glm_fit &lt;- safe_fit(quote(glm(f_lm, data = dtr, family = Gamma(link = \"log\"))))\n  if (!is.null(glm_fit)) {\n    pr &lt;- try(predict(glm_fit, newdata = dte, type = \"response\"), silent = TRUE)\n    if (!inherits(pr, \"try-error\")) {\n      cv_summ &lt;- add_row(cv_summ, Model = \"GLM\", RMSE = rmse(dte$R3haddock, pr), MAE = mae(dte$R3haddock, pr))\n      resids_cv$GLM &lt;- c(resids_cv$GLM, dte$R3haddock - pr)\n    }\n  }\n\n  # GAM (Gamma log), ограничиваем сложность k для стабильности на малом n\n  gam_fit &lt;- safe_fit(quote(mgcv::gam(f_gam, data = dtr, family = Gamma(link = \"log\"), method = \"REML\", select = TRUE)))\n  if (!is.null(gam_fit)) {\n    pr &lt;- try(predict(gam_fit, newdata = dte, type = \"response\"), silent = TRUE)\n    if (!inherits(pr, \"try-error\")) {\n      cv_summ &lt;- add_row(cv_summ, Model = \"GAM\", RMSE = rmse(dte$R3haddock, pr), MAE = mae(dte$R3haddock, pr))\n      resids_cv$GAM &lt;- c(resids_cv$GAM, dte$R3haddock - pr)\n    }\n  }\n}\n\n# Средние метрики по моделям\ncv_rank &lt;- cv_summ %&gt;% group_by(Model) %&gt;% summarise(RMSE = mean(RMSE, na.rm = TRUE), MAE = mean(MAE, na.rm = TRUE), .groups = \"drop\") %&gt;% arrange(RMSE, MAE)\nprint(cv_rank)\n\n# A tibble: 3 x 3\n  Model    RMSE     MAE\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 GLM   280259. 237187.\n2 LM    370298. 340884.\n3 GAM   504613. 432062.\n\nbest_model_name &lt;- cv_rank$Model[1]\ncat(sprintf(\"\\nЛучшая модель по time-slice CV: %s\\n\", best_model_name))\n\n\nЛучшая модель по time-slice CV: GLM\n\n# Хронологический тест: обучаем на всём train_ts, прогнозируем на test_ts\nfit_on &lt;- function(model_name, data) {\n  if (model_name == \"LM\") return(lm(f_lm, data = data))\n  if (model_name == \"GLM\") return(glm(f_lm, data = data, family = Gamma(link = \"log\")))\n  mgcv::gam(f_gam, data = data, family = Gamma(link = \"log\"), method = \"REML\", select = TRUE)\n}\n\npredict_on &lt;- function(fit, newdata, model_name) {\n  if (model_name == \"GLM\") return(predict(fit, newdata = newdata, type = \"response\"))\n  if (inherits(fit, \"gam\")) return(predict(fit, newdata = newdata, type = \"response\"))\n  predict(fit, newdata = newdata)\n}\n\nfit_train &lt;- fit_on(best_model_name, train_ts)\npred_te   &lt;- predict_on(fit_train, test_ts, best_model_name)\ntest_metrics &lt;- tibble(\n  Model = best_model_name,\n  RMSE  = rmse(test_ts$R3haddock, pred_te),\n  MAE   = mae (test_ts$R3haddock, pred_te),\n  R2    = r2  (test_ts$R3haddock, pred_te)\n)\nprint(test_metrics)\n\n# A tibble: 1 x 4\n  Model    RMSE     MAE    R2\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 GLM   182048. 141692. 0.363\n\n# 4) Диагностика моделей (подгонка на всех данных до 2021) -------------------\nfull_fit_df &lt;- md_for_fit\n\nlm_full  &lt;- lm(f_lm,  data = full_fit_df)\nglm_full &lt;- glm(f_lm, data = full_fit_df, family = Gamma(link = \"log\"))\ngam_full &lt;- mgcv::gam(f_gam, data = full_fit_df, family = Gamma(link = \"log\"), method = \"REML\", select = TRUE)\n\ncat(\"\\n[LM] Сводка:\\n\"); print(summary(lm_full))\n\n\n[LM] Сводка:\n\n\n\nCall:\nlm(formula = f_lm, data = full_fit_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-257877 -155326  -18935  101135  326940 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.189e+05  4.459e+05  -2.061 0.049455 *  \ncodTSB      -2.406e-01  6.722e-02  -3.579 0.001386 ** \nT12          3.679e+05  8.296e+04   4.435 0.000149 ***\nI5          -1.770e+03  3.025e+03  -0.585 0.563536    \nNAOspring   -5.125e+04  5.427e+04  -0.944 0.353710    \nhaddock68    4.385e-01  5.395e-01   0.813 0.423698    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 192900 on 26 degrees of freedom\nMultiple R-squared:  0.547, Adjusted R-squared:  0.4599 \nF-statistic: 6.279 on 5 and 26 DF,  p-value: 0.0006042\n\ncat(\"\\n[LM] VIF:\\n\"); print(car::vif(lm_full))\n\n\n[LM] VIF:\n\n\n   codTSB       T12        I5 NAOspring haddock68 \n 2.487391  1.398254  1.275233  1.035349  2.386554 \n\ncat(\"\\n[LM] Breusch–Pagan:\\n\"); print(lmtest::bptest(lm_full))\n\n\n[LM] Breusch–Pagan:\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  lm_full\nBP = 5.1481, df = 5, p-value = 0.3981\n\ncat(\"\\n[LM] Durbin–Watson:\\n\"); print(lmtest::dwtest(lm_full))\n\n\n[LM] Durbin–Watson:\n\n\n\n    Durbin-Watson test\n\ndata:  lm_full\nDW = 1.7745, p-value = 0.1264\nalternative hypothesis: true autocorrelation is greater than 0\n\nglm_resid &lt;- residuals(glm_full, type = \"pearson\")\ncat(\"\\n[GLM-Gamma] Сводка:\\n\"); print(summary(glm_full))\n\n\n[GLM-Gamma] Сводка:\n\n\n\nCall:\nglm(formula = f_lm, family = Gamma(link = \"log\"), data = full_fit_df)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.907e+00  1.288e+00   5.361 1.30e-05 ***\ncodTSB      -6.525e-07  1.942e-07  -3.359  0.00242 ** \nT12          1.341e+00  2.397e-01   5.593 7.08e-06 ***\nI5           8.270e-03  8.740e-03   0.946  0.35278    \nNAOspring    4.898e-02  1.568e-01   0.312  0.75725    \nhaddock68    1.117e-06  1.559e-06   0.717  0.48000    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.3107759)\n\n    Null deviance: 19.8880  on 31  degrees of freedom\nResidual deviance:  8.5197  on 26  degrees of freedom\nAIC: 855.29\n\nNumber of Fisher Scoring iterations: 8\n\ncat(sprintf(\"[GLM-Gamma] Pearson dispersion: %.3f\\n\", sum(glm_resid^2, na.rm = TRUE) / glm_full$df.residual))\n\n[GLM-Gamma] Pearson dispersion: 0.311\n\ncat(\"\\n[GAM] Сводка:\\n\"); print(summary(gam_full))\n\n\n[GAM] Сводка:\n\n\n\nFamily: Gamma \nLink function: log \n\nFormula:\nR3haddock ~ s(codTSB, bs = \"tp\", k = 5) + s(T12, bs = \"tp\", k = 5) + \n    s(I5, bs = \"tp\", k = 5) + s(NAOspring, bs = \"tp\", k = 5) + \n    s(haddock68, bs = \"tp\", k = 5)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.48948    0.08886   140.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                   edf Ref.df     F  p-value    \ns(codTSB)    1.7083407      4 4.917 7.43e-05 ***\ns(T12)       0.9669993      4 7.752 3.96e-06 ***\ns(I5)        0.0001638      4 0.000    0.616    \ns(NAOspring) 0.0001092      4 0.000    0.980    \ns(haddock68) 0.4529453      4 0.145    0.252    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.592   Deviance explained = 60.2%\n-REML = 426.83  Scale est. = 0.2527    n = 32\n\ncat(\"\\n[GAM] Concurvity (коротко):\\n\")\n\n\n[GAM] Concurvity (коротко):\n\nccv &lt;- try(mgcv::concurvity(gam_full, full = FALSE), silent = TRUE)\nif (!inherits(ccv, \"try-error\") && is.list(ccv)) {\n  # Выведем усечённо и безопасно\n  print(lapply(ccv, function(m) if (is.null(m)) NULL else round(m, 3)))\n} else {\n  cat(\"не удалось оценить concurvity\\n\")\n}\n\n$worst\n             para s(codTSB) s(T12) s(I5) s(NAOspring) s(haddock68)\npara            1     0.000  0.000 0.000        0.000        0.000\ns(codTSB)       0     1.000  0.554 0.298        0.178        0.821\ns(T12)          0     0.554  1.000 0.361        0.362        0.347\ns(I5)           0     0.298  0.361 1.000        0.182        0.132\ns(NAOspring)    0     0.178  0.362 0.182        1.000        0.216\ns(haddock68)    0     0.821  0.347 0.132        0.216        1.000\n\n$observed\n             para s(codTSB) s(T12) s(I5) s(NAOspring) s(haddock68)\npara            1     0.000  0.000 0.000        0.000        0.000\ns(codTSB)       0     1.000  0.392 0.166        0.045        0.385\ns(T12)          0     0.273  1.000 0.217        0.064        0.218\ns(I5)           0     0.166  0.283 1.000        0.047        0.044\ns(NAOspring)    0     0.031  0.128 0.114        1.000        0.049\ns(haddock68)    0     0.441  0.204 0.113        0.116        1.000\n\n$estimate\n             para s(codTSB) s(T12) s(I5) s(NAOspring) s(haddock68)\npara            1     0.000  0.000 0.000        0.000        0.000\ns(codTSB)       0     1.000  0.320 0.140        0.041        0.747\ns(T12)          0     0.281  1.000 0.201        0.068        0.243\ns(I5)           0     0.121  0.278 1.000        0.053        0.096\ns(NAOspring)    0     0.055  0.128 0.113        1.000        0.089\ns(haddock68)    0     0.544  0.205 0.099        0.136        1.000\n\ninvisible(try(mgcv::gam.check(gam_full), silent = TRUE))\n\n\n\n\n\n\n\n\n\nMethod: REML   Optimizer: outer newton\nfull convergence after 13 iterations.\nGradient range [-4.544099e-05,0.000320414]\n(score 426.834 & scale 0.2526969).\nHessian positive definite, eigenvalue range [5.946259e-06,16.95877].\nModel rank =  21 / 21 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n                   k'      edf k-index p-value\ns(codTSB)    4.000000 1.708341    1.12    0.69\ns(T12)       4.000000 0.966999    1.29    0.95\ns(I5)        4.000000 0.000164    0.86    0.20\ns(NAOspring) 4.000000 0.000109    0.99    0.50\ns(haddock68) 4.000000 0.452945    1.10    0.73\n\n# 5) Прогноз 2022–2024 и эмпирические интервалы ------------------------------\nbest_full &lt;- switch(best_model_name,\n  LM  = lm_full,\n  GLM = glm_full,\n  GAM = gam_full\n)\n\n# Остатки для PI: из CV выбранной модели, иначе из полного фита\nresids &lt;- if (length(resids_cv[[best_model_name]]) &gt; 5) resids_cv[[best_model_name]] else residuals(best_full)\n\nq025 &lt;- as.numeric(quantile(resids, 0.025, na.rm = TRUE))\nq250 &lt;- as.numeric(quantile(resids, 0.250, na.rm = TRUE))\nq750 &lt;- as.numeric(quantile(resids, 0.750, na.rm = TRUE))\nq975 &lt;- as.numeric(quantile(resids, 0.975, na.rm = TRUE))\n\nfc_start &lt;- 2022\npred_cols &lt;- c(\"codTSB\",\"T12\",\"I5\",\"NAOspring\",\"haddock68\")\nmu &lt;- md %&gt;% filter(YEAR &gt; 1989 & YEAR &lt; fc_start) %&gt;% summarise(across(all_of(pred_cols), ~mean(.x, na.rm = TRUE))) %&gt;% as.list()\n\nif (!exists(\"user_future\")) user_future &lt;- NULL\n\nbuild_future &lt;- function(years, mu, user_df = NULL) {\n  df &lt;- tibble::tibble(YEAR = years)\n  for (v in pred_cols) df[[v]] &lt;- mu[[v]]\n  if (!is.null(user_df)) {\n    for (i in seq_len(nrow(user_df))) {\n      yr &lt;- user_df$YEAR[i]\n      if (yr %in% years) {\n        idx &lt;- which(df$YEAR == yr)\n        for (v in intersect(pred_cols, names(user_df))) {\n          val &lt;- user_df[[v]][i]\n          if (!is.na(val)) df[[v]][idx] &lt;- val\n        }\n      }\n    }\n  }\n  df\n}\n\nfuture_years &lt;- fc_start:2024\nscenario_future &lt;- build_future(future_years, mu, user_future)\n\npredict_best &lt;- function(fit, newdata, model_name) {\n  if (model_name == \"GLM\") return(predict(fit, newdata = newdata, type = \"response\"))\n  predict(fit, newdata = newdata)\n}\n\npred_future &lt;- predict_best(best_full, scenario_future, best_model_name)\n\nforecast_tbl &lt;- tibble::tibble(\n  YEAR      = scenario_future$YEAR,\n  Model     = best_model_name,\n  pred_mean = as.numeric(pred_future),\n  PI50_low  = pred_future + q250, PI50_high = pred_future + q750,\n  PI95_low  = pred_future + q025, PI95_high = pred_future + q975\n)\n\n\nknitr::kable(\n  forecast_tbl %&gt;% dplyr::mutate(dplyr::across(where(is.numeric), ~round(.x, 2))),\n  caption = \"Holdout-метрики (округлено до 2 знаков)\"\n)\n\n\nHoldout-метрики (округлено до 2 знаков)\n\n\nYEAR\nModel\npred_mean\nPI50_low\nPI50_high\nPI95_low\nPI95_high\n\n\n\n\n2022\nGLM\n268057.6\n172383\n509783.3\n-203196.1\n1051570\n\n\n2023\nGLM\n268057.6\n172383\n509783.3\n-203196.1\n1051570\n\n\n2024\nGLM\n268057.6\n172383\n509783.3\n-203196.1\n1051570\n\n\n\n\n# 6) Визуализация 1990–2024 ---------------------------------------------------\npred_df &lt;- bind_rows(\n  md %&gt;% select(YEAR, all_of(pred_cols)),\n  scenario_future\n) %&gt;% distinct(YEAR, .keep_all = TRUE) %&gt;% arrange(YEAR)\n\npred_df$Pred      &lt;- as.numeric(predict_best(best_full, pred_df, best_model_name))\npred_df$PI50_low  &lt;- pred_df$Pred + q250\npred_df$PI50_high &lt;- pred_df$Pred + q750\npred_df$PI95_low  &lt;- pred_df$Pred + q025\npred_df$PI95_high &lt;- pred_df$Pred + q975\n\nhist_df &lt;- md %&gt;% select(YEAR, R3haddock)\n\nggplot() +\n  geom_ribbon(data = pred_df, aes(x = YEAR, ymin = PI95_low, ymax = PI95_high), fill = \"grey80\", alpha = 0.25) +\n  geom_ribbon(data = pred_df, aes(x = YEAR, ymin = PI50_low, ymax = PI50_high), fill = \"grey60\", alpha = 0.35) +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = Pred), color = \"steelblue4\", linewidth = 1) +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = Pred), color = \"steelblue4\", linewidth = 1, linetype = \"dashed\") +\n  geom_point(data = hist_df, aes(x = YEAR, y = R3haddock), color = \"black\", size = 2, alpha = 0.9) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0))) +\n  labs(title = paste0(\"Пополнение R3haddock: факт (1990–2021) и прогноз (2022–2024) — \", best_model_name),\n       subtitle = \"Прогноз — пунктир, интервалы — эмпирические из остатков\",\n       x = \"Год\", y = \"R3haddock\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# AIC-таблица (LM/GLM сопоставимы напрямую; для GAM также показываем ML)\ncat(\"\\nAIC (LM): \",  AIC(lm_full),  \"\\n\", sep = \"\")\n\n\nAIC (LM): 877.0549\n\ncat(\"AIC (GLM): \", AIC(glm_full), \"\\n\", sep = \"\")\n\nAIC (GLM): 855.2949\n\ngam_full_ml &lt;- mgcv::gam(f_gam, data = full_fit_df, family = Gamma(link = \"log\"), method = \"ML\", select = TRUE)\ncat(\"AIC (GAM, REML): \", AIC(gam_full),    \"\\n\", sep = \"\")\n\nAIC (GAM, REML): 850.8686\n\ncat(\"AIC (GAM, ML):   \", AIC(gam_full_ml), \"\\n\", sep = \"\")\n\nAIC (GAM, ML):   850.7948\n\n# ============================================================================\n# Конец\n# ============================================================================",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 7.html#полный-цикл-от-факторов-до-ансамблевого-прогноза",
    "href": "chapter 7.html#полный-цикл-от-факторов-до-ансамблевого-прогноза",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "8.5 Полный цикл от факторов до ансамблевого прогноза",
    "text": "8.5 Полный цикл от факторов до ансамблевого прогноза\nПолный цикл анализа от идентификации ключевых факторов до создания надежного ансамблевого прогноза пополнения рыбных запасов представляет собой сложный, но систематизированный процесс, требующий как глубокого понимания биологических процессов, так и владения современными методами анализа данных. Начиная с формирования исходного набора предикторов, включающего как биологические переменные (нерестовый запас, биомасса хищников), так и комплексные океанографические показатели (температура, соленость, климатические индексы), мы проходим через строгую последовательность этапов, каждый из которых важен для конечного результата. На этапе подготовки данных мы не просто приводим информацию к числовому формату и заменяем строковые обозначения пропущенных значений «NA» на стандартные NA, но и проводим глубокий анализ корреляционной структуры, устраняя мультиколлинеарность через анализ корреляций и VIF-диагностику, что важно для корректной интерпретации последующих моделей. Для обработки пропусков мы применяем медианную импутацию, которая представляет собой простой и устойчивый к выбросам метод, хотя в некоторых случаях могут быть использованы и более сложные методы, такие как KNN-импутация или множественная импутация с использованием пакета MICE, особенно когда данные имеют сложную структуру или временные зависимости.\nЗатем следует этап отбора предикторов, где мы применяем два комплементарных метода: Boruta на основе Random Forest для выявления нелинейных зависимостей и LASSO-регрессию для линейного отбора с регуляризацией. Их объединение позволяет получить устойчивый набор предикторов, дополненный биологически значимыми переменными по экспертной оценке, что создает баланс между статистической значимостью и содержательной интерпретируемостью. Этот этап является мостом между классической ихтиологией и современными методами анализа, где экспертные знания биолога взаимодействуют с алгоритмической строгостью статистики, гарантируя включение ключевых факторов, таких как нерестовый запас, который должен присутствовать в модели по самой своей природе процесса пополнения.\nПосле подготовки данных мы переходим к сравнению различных семейств моделей через единую кросс-валидационную процедуру (5-fold CV) с последующим хронологическим тестированием на отложенной выборке. Помимо линейных и обобщенных линейных моделей (LM, GLM), обобщенных аддитивных моделей (GAM), мы тестируем современные алгоритмы машинного обучения: Random Forest для улавливания сложных нелинейных зависимостей и взаимодействий между факторами, будучи при этом устойчивым к шуму и выбросам; XGBoost, с его градиентным бустингом над деревьями решений, часто дающий высочайшую точность прогноза; SVM с радиальным ядром для сложных разделяющих поверхностей; и нейронные сети для автоматического извлечения признаков. Каждая модель оценивается по комплексу метрик: RMSE, MAE, R² и MAPE, что позволяет сравнивать их прогностическую силу на разных участках данных и выявлять модели, которые лучше всего справляются с конкретными аспектами прогнозирования.\nОсобое внимание уделяется временным характеристикам данных, поскольку при анализе водных биоресурсов мы имеем дело с временными рядами, где случайное перемешивание данных приведет к утечке информации из будущего в прошлое, искусственно завысив качество прогноза. Для решения этой проблемы мы применяем специализированную time-slice кросс-валидацию с расширяющимся окном и горизонтом прогноза 3 года, которая имитирует реальные условия прогнозирования, обучаясь только на данных из прошлого и проверяя на последующих периодах. Это позволяет оценить устойчивость моделей к временным сдвигам и их способность к экстраполяции, что критически важно для практических задач управления рыбными запасами.\nВыбор окончательной модели — это не просто вопрос максимальной точности на кросс-валидации, а сложный компромисс между точностью, интерпретируемостью и биологической правдоподобностью. Кульминацией цикла становится построение ансамблевой модели, комбинирующей сильные стороны отдельных алгоритмов. В нашем анализе оптимальный ансамбль (CUBIST + LM) строится через взвешенное усреднение предсказаний, где веса определяются на основе кросс-валидационной ошибки — например, 75% веса приходится на мощную нелинейную модель Cubist, а 25% — на простую и устойчивую линейную регрессию. Такой подход позволяет нивелировать индивидуальные недостатки моделей, сохранить интерпретируемость линейных моделей, где биолог может понять, как именно каждый фактор влияет на прогноз, и при этом использовать гибкость методов машинного обучения для захвата сложных нелинейных паттернов, которые могут ускользнуть от классических статистических методов.\nВажнейшим компонентом становится оценка неопределенности через эмпирические доверительные интервалы, построенные на основе распределения остатков ансамблевой модели. Мы используем квантили остатков из кросс-валидации для построения 50% и 95% доверительных интервалов, что позволяет получить не только точечный прогноз, но и меру его надежности, важную для принятия управленческих решений. Это дает возможность визуализировать не только ожидаемое значение пополнения, но и диапазон возможных сценариев, что особенно важно в условиях высокой экологической неопределенности.\nФинальная визуализация представляет собой совмещение исторических данных с прогнозом на 3 года вперед, где исторические данные отображаются сплошной линией, прогноз — пунктиром, а 50% и 95% доверительные интервалы — серыми лентами различной интенсивности. Такой график не только демонстрирует результат, но и позволяет визуально оценить точность модели на исторических данных и неопределенность будущих предсказаний, делая результаты доступными не только для статистиков, но и для управленцев и политиков, принимающих решения на основе этих прогнозов.\nПредставленный цикл является итеративным процессом: прогнозная точность ансамбля может быть улучшена через включение новых предикторов, изучение влияния предикторов с задержкой (лагами), тонкую настройку гиперпараметров моделей и обновление данных по мере их поступления. Этот подход представляет собой практический компромисс между статистической строгостью, вычислительной эффективностью и биологической интерпретируемостью, делая его мощным инструментом для решения прикладных задач оценки водных биоресурсов, где каждый этап, от первичной обработки данных до финального прогноза, подчинен одной цели — обеспечению устойчивого управления рыбными запасами на основе надежного научного анализа.\nСкрипт лучше скачать целиком).\n\n# ==============================================================================\n# ПРАКТИЧЕСКОЕ ЗАНЯТИЕ: АНАЛИЗ ФАКТОРОВ И ПРОГНОЗ ПОПОЛНЕНИЯ ЗАПАСА\n# Курс: \"Оценка водных биоресурсов в среде R (для начинающих)\"\n# Автор: Баканев С. В. Дата:20.08.2025\n# Структура:\n# 1) Подготовка данных и выбор предикторов\n# 2) Базовое сравнение моделей (5-fold CV + holdout)\n# 3) Выбор лучшей прогностической модели (time-slice CV на 3 года + хронологический тест)\n# 4) Прогноз 2022–2024 (ансамбль CUBIST+LM) и график 1990–2024 с ДИ\n# ------------------------------------------------------------------------------\n# Пояснения к занятию (для начинающих):\n# - Мы работаем с временным рядом пополнения запаса R3haddock и набором факторов\n#   среды/биомассы. Цель — построить понятные и проверяемые модели прогноза.\n# - Сначала отберём информативные предикторы (Boruta и LASSO), затем сравним\n#   разные модели машинного обучения на кросс-валидации (CV), после чего выберем\n#   лучшую схему по time-slice CV (учитывая хронологию), и сделаем прогноз.\n# ==============================================================================\n\n\n# ==============================================================================\n# 1) ВЫБОР ПРЕДИКТОРОВ\n# ------------------------------------------------------------------------------\n# Цель блока: привести данные к числовому виду, обработать пропуски, сократить\n# мультиколлинеарность (сильные корреляции), а затем автоматически выделить\n# кандидатов-предикторов двумя методами (Boruta, LASSO). В конце сформируем\n# финальный пул признаков и проверим их значимость в простой LM.\n# ==============================================================================\n\n# Установка и подключение необходимых библиотек\n# Для автоматического отбора предикторов нам понадобятся дополнительные пакеты\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  readxl, tidyverse, caret, corrplot, mgcv, randomForest, xgboost,\n  Boruta,GGally, FactoMineR, glmnet, recipes, rsample  # Новые библиотеки для автоматического отбора\n)\n\n# Очистка среды и установка рабочей директории\n# Совет: rm(list=ls()) очищает все объекты в памяти R; setwd задаёт папку,\n# где искать/сохранять файлы. Убедитесь, что путь корректен на вашей машине.\nrm(list = ls())\nsetwd(\"C:/RECRUITMENT/\")\n\n# Пакеты для расширенного отбора предикторов\n# Boruta — обёртка над Random Forest для отбора признаков;\n# glmnet — регуляризация (LASSO/ElasticNet) для отбора/усиления обобщающей способности;\n# FactoMineR — PCA и другие многомерные методы (используем как утилиту).\nlibrary(Boruta)   # Алгоритм обертки для отбора признаков\nlibrary(glmnet)   # LASSO-регрессия\nlibrary(FactoMineR) # PCA анализ\n\n\n# Загрузка и первичная обработка данных\n# Шаги: фильтруем годы, приводим типы к числовому, заменяем строковые \"NA\" на NA.\nDATA &lt;- readxl::read_excel(\"RECRUITMENT.xlsx\", sheet = \"RECRUITMENT\") %&gt;%\n  filter(YEAR &gt; 1989 & YEAR &lt; 2022) %&gt;%\n  # Преобразуем необходимые столбцы в числовой формат\n  mutate(\n    across(starts_with(\"T\"), as.numeric),\n    across(starts_with(\"I\"), as.numeric),\n    across(starts_with(\"O\"), as.numeric),\n  ) %&gt;%\n  # Обработка пропущенных значений (заменяем строку \"NA\" на NA)\n  mutate(across(where(is.character), ~na_if(., \"NA\")))\n\n# 1. Подготовка данных -------------------------------------------------------\n# Выделим все возможные предикторы, включая географию и индексы трески\n# Примечание: оставляем только числовые переменные, т.к. большинство моделей\n# требует числовой вход без категориальных уровней.\npredictors &lt;- DATA %&gt;% \n  select(-YEAR, -R3haddock) %&gt;% \n  select_if(is.numeric) # Только числовые переменные\n\n# Целевая переменная\nresponse &lt;- DATA$R3haddock\n\n# В статистическом анализе мы различаем:\n# - Отклик (response/target variable) - то, что мы пытаемся предсказать (в нашем случае R3haddock)\n# - Предикторы (predictors/features) - переменные, которые могут объяснять изменения отклика\n# Для корректного анализа важно, чтобы предикторы были числовыми или преобразованы в числовой формат.\n\n# 2. Обработка пропусков -----------------------------------------------------\n# Заполнение медианными значениями — простой и устойчивый способ справиться с NA.\n# Альтернативы: множественная иммутация (mice), KNN-impute и др.\npredictors_filled &lt;- predictors %&gt;%\n  mutate(across(everything(), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))\n\n# Заполнение медианой - простой и устойчивый метод обработки пропусков для числовых переменных.\n# Медиана предпочтительнее среднего, так как менее чувствительна к выбросам.\n\n# 3. Предварительный анализ корреляций ---------------------------------------\n# Зачем: высокие корреляции затрудняют интерпретацию и могут вредить ряду моделей.\ncor_matrix &lt;- cor(predictors_filled, use = \"complete.obs\")\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", tl.cex = 0.7)\n\n\n\n\n\n\n\n# Удаляем высокоскоррелированные предикторы (r &gt; 0.8)\n# Это механическое сокращение мультиколлинеарности до этапа отбора.\nhigh_cor &lt;- findCorrelation(cor_matrix, cutoff = 0.8)\npredictors_filtered &lt;- predictors_filled[, -high_cor]\n\n# Высокая корреляция между предикторами (мультиколлинеарность) может привести к нестабильности моделей.\n# Например, если два предиктора почти идентичны, модель может неустойчиво распределять их влияние на отклик.\n# Удаление сильно коррелированных переменных (r &gt; 0.8) помогает улучшить интерпретируемость и стабильность моделей.\n\n\n# 4. Автоматизированный отбор Boruta (обертка Random Forest) -----------------\n# Идея: определить признаки, которые важнее, чем случайный шум (shadow features).\n\n\n# Визуализация результатов\nplot(boruta_output, cex.axis = 0.7, las = 2)\n\n\n\n\n\n\n\nboruta_stats &lt;- attStats(boruta_output)\nselected_vars &lt;- getSelectedAttributes(boruta_output, withTentative = TRUE)\n\n# Boruta - это алгоритм отбора признаков, основанный на методе случайного леса.\n# Он сравнивает важность реальных переменных с \"теневыми\" переменными (случайными копиями),\n# чтобы определить, действительно ли переменная информативна. \n# Результаты Boruta показывают: \n#   - Confirmed (зеленые) - значимые предикторы\n#   - Tentative (желтые) - предикторы, близкие к порогу значимости\n#   - Rejected (красные) - незначимые предикторы\n\n\n# 5. LASSO с более строгим критерием ------------------------------------------\n# Идея: L1-регуляризация зануляет коэффициенты «слабых» предикторов.\n# Выбор lambda.1se вместо lambda.min — более консервативный (простая модель).\nx &lt;- as.matrix(predictors_filtered)\ny &lt;- response\n\n# LASSO (Least Absolute Shrinkage and Selection Operator) - метод регрессии с L1-регуляризацией,\n# который одновременно выполняет отбор признаков и оценку коэффициентов. \n# Параметр lambda контролирует силу регуляризации:\n#   - lambda.min дает наименьшую ошибку, но может включать шумовые переменные\n#   - lambda.1se (на 1 стандартную ошибку больше) дает более простую модель с меньшим риском переобучения\n# Для прогнозирования мы предпочитаем более строгий критерий (lambda.1se), чтобы модель была устойчивее. \n\n# Кросс-валидация\ncv_fit &lt;- cv.glmnet(x, y, alpha = 1, nfolds = 10)\nplot(cv_fit)\n\n\n\n\n\n\n\n# ИСПОЛЬЗУЕМ lambda.1se вместо lambda.min — СТРОЖЕ!\nlasso_coef &lt;- coef(cv_fit, s = \"lambda.1se\")  # &lt;-- Ключевое изменение!\nlasso_vars &lt;- rownames(lasso_coef)[lasso_coef[,1] != 0][-1]  # исключаем (Intercept)\n\n\n# 6. Сравнение отобранных предикторов ----------------------------------------\n# Полезно видеть, какие признаки отмечают оба метода (устойчивые кандидаты).\ncat(\"Boruta selected:\", length(selected_vars), \"variables\\n\")\n\nBoruta selected: 3 variables\n\nprint(selected_vars)\n\n[1] \"codTSB\" \"T12\"    \"I5\"    \n\ncat(\"\\nLASSO selected:\", length(lasso_vars), \"variables\\n\")\n\n\nLASSO selected: 5 variables\n\nprint(lasso_vars)\n\n[1] \"codTSB\" \"T12\"    \"NAO3\"   \"NAO4\"   \"NAO5\"  \n\n# 7. Финальный набор предикторов (объединение результатов) -------------------\n# Логика: объединяем списки, добавляем биологически важные переменные вручную.\nfinal_vars &lt;- union(selected_vars, lasso_vars) \n\n# Добавляем обязательные переменные по биологической логике\nmandatory &lt;- c(\"haddock68\")\nfinal_vars &lt;- union(final_vars, mandatory) %&gt;% unique()\n\n# Мы объединяем результаты двух методов отбора признаков для большей надежности.\n# Также добавляем переменную haddock68 (нерестовый запас), так как биологически \n# логично, что пополнение запаса напрямую зависит от численности производителей. \n# Это пример интеграции экспертных знаний в статистический анализ - важный принцип \n# при работе с данными в биологических науках.\n\n# 8. Проверка значимости -----------------------------------------------------\n# Быстрая оценка значимости с LM: не как окончательный вывод, а как sanity-check.\nfinal_model &lt;- lm(response ~ as.matrix(predictors_filled[, final_vars]))\nsummary(final_model)\n\n\nCall:\nlm(formula = response ~ as.matrix(predictors_filled[, final_vars]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-270986  -82376   -1037   98086  276129 \n\nCoefficients:\n                                                      Estimate Std. Error\n(Intercept)                                         -1.082e+06  3.943e+05\nas.matrix(predictors_filled[, final_vars])codTSB    -2.346e-01  5.536e-02\nas.matrix(predictors_filled[, final_vars])T12        3.864e+05  7.198e+04\nas.matrix(predictors_filled[, final_vars])I5        -1.825e+02  2.572e+03\nas.matrix(predictors_filled[, final_vars])NAO3      -5.801e+04  3.129e+04\nas.matrix(predictors_filled[, final_vars])NAO4       8.345e+04  3.035e+04\nas.matrix(predictors_filled[, final_vars])NAO5      -7.278e+04  2.488e+04\nas.matrix(predictors_filled[, final_vars])haddock68  1.232e-01  4.515e-01\n                                                    t value Pr(&gt;|t|)    \n(Intercept)                                          -2.744 0.011305 *  \nas.matrix(predictors_filled[, final_vars])codTSB     -4.238 0.000288 ***\nas.matrix(predictors_filled[, final_vars])T12         5.368 1.64e-05 ***\nas.matrix(predictors_filled[, final_vars])I5         -0.071 0.944028    \nas.matrix(predictors_filled[, final_vars])NAO3       -1.854 0.076118 .  \nas.matrix(predictors_filled[, final_vars])NAO4        2.750 0.011146 *  \nas.matrix(predictors_filled[, final_vars])NAO5       -2.925 0.007412 ** \nas.matrix(predictors_filled[, final_vars])haddock68   0.273 0.787227    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 158600 on 24 degrees of freedom\nMultiple R-squared:  0.7173,    Adjusted R-squared:  0.6348 \nF-statistic: 8.698 on 7 and 24 DF,  p-value: 2.58e-05\n\n# 9. Формирование финального датасета ----------------------------------------\n# Собираем набор с откликом и выбранными предикторами; удалим строки с NA.\nmodel_data &lt;- DATA %&gt;%\n  select(R3haddock, all_of(final_vars)) %&gt;%\n  drop_na()\n\n# Просмотр структуры финальных данных\nglimpse(model_data)\n\nRows: 32\nColumns: 8\n$ R3haddock &lt;dbl&gt; 812363, 389416, 99474, 98946, 118812, 63028, 147657, 83270, ~\n$ codTSB    &lt;dbl&gt; 913000, 1347064, 1687381, 2197863, 2112773, 1849957, 1697388~\n$ T12       &lt;dbl&gt; 4.72, 4.66, 4.24, 3.90, 3.96, 4.27, 4.16, 4.07, 4.23, 5.08, ~\n$ I5        &lt;dbl&gt; 43, 55, 26, 49, 56, 28, 52, 51, 69, 68, 41, 48, 50, 63, 40, ~\n$ NAO3      &lt;dbl&gt; 1.46, -0.20, 0.87, 0.67, 1.26, 1.25, -0.24, 1.46, 0.87, 0.23~\n$ NAO4      &lt;dbl&gt; 2.00, 0.29, 1.86, 0.97, 1.14, -0.85, -0.17, -1.02, -0.68, -0~\n$ NAO5      &lt;dbl&gt; -1.53, 0.08, 2.63, -0.78, -0.57, -1.49, -1.06, -0.28, -1.32,~\n$ haddock68 &lt;dbl&gt; 74586, 79205, 53195, 36337, 49122, 81514, 172177, 160886, 96~\n\n# Визуализация важности переменных\n# Внимание: важности от RF — относительные; сопоставляйте с предметной логикой.\nvar_importance &lt;- randomForest(R3haddock ~ ., data = model_data, importance = TRUE)\nvarImpPlot(var_importance, main = \"Важность предикторов\")\n\n\n\n\n\n\n\n# Перед окончательным выбором модели мы проверяем значимость предикторов с помощью линейной регрессии.\n# Функция summary() показывает p-значения коэффициентов - если p &lt; 0.05, переменная считается статистически значимой. \n# Визуализация важности переменных с помощью случайного леса дает дополнительную перспективу,\n# показывая, какие переменные наиболее информативны для предсказания без предположений о линейности.\n\n# ==============================================================================\n#  ПОДГОТОВКА ДАННЫХ\n# Создаём NAOspring, фиксируем финальный набор признаков, сохраняем CSV.\n# ------------------------------------------------------------------------------\n# Цель блока: стандартизировать набор признаков для дальнейшего сравнения\n# моделей и обеспечить воспроизводимость (фиксированный CSV с нужными полями).\n# ==============================================================================\n\n# 1.1 Пакеты и окружение\n# Примечание: блок повторяет базовую инициализацию для автономного запуска.\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(readxl, tidyverse, caret, corrplot)\n\nrm(list = ls())\nset.seed(123)\nsetwd(\"C:/RECRUITMENT/\")\n\n# 1.2 Загрузка исходных данных и приведение типов\nDATA &lt;- readxl::read_excel(\"RECRUITMENT.xlsx\", sheet = \"RECRUITMENT\") %&gt;%\n  filter(YEAR &gt; 1989 & YEAR &lt; 2022) %&gt;%\n  mutate(\n    across(starts_with(\"T\"), as.numeric),\n    across(starts_with(\"I\"), as.numeric),\n    across(starts_with(\"O\"), as.numeric),\n    across(where(is.character), ~na_if(., \"NA\"))\n  )\n\n# 1.3 Создаём NAOspring (если есть NAO3, NAO4, NAO5)\n# Идея: агрегируем весенний индекс NAO как среднее за месяцы 3–5.\nif (all(c(\"NAO3\",\"NAO4\",\"NAO5\") %in% names(DATA))) {\n  DATA &lt;- DATA %&gt;%\n    mutate(NAOspring = rowMeans(pick(NAO3, NAO4, NAO5), na.rm = TRUE)) %&gt;%\n    select(-NAO3, -NAO4, -NAO5)\n}\n\n# NAO (North Atlantic Oscillation) - важный климатический индекс, влияющий описывающий изменения атмосферного давления\n# над Северной Атлантикой. В частности, он отражает разницу в атмосферном давлении между Исландской депрессией и\n# Азорским максимумом. NAO влияет на силу и направление западных ветров, а также на траектории штормов в Северной Атлантике. \n# Мы создаем NAOspring как среднее значение за весенние месяцы (марта, апреля, мая),\n# так как именно в этот период происходят ключевые процессы, влияющие на нерест трески. \n# Создание составных переменных на основе экспертных знаний часто улучшает качество моделей.\n\n# 1.4 Финальный учебный набор предикторов (фиксируем)\n# Важно: проверяем присутствие нужных колонок и формируем компактный датасет.\nneeded &lt;- c(\"codTSB\", \"T12\", \"I5\", \"NAOspring\", \"haddock68\")\nstopifnot(all(needed %in% names(DATA)))\n\n# Сохраняем YEAR в CSV (ниже он будет отброшен при обучении, но нужен для графика)\nmodel_data &lt;- DATA %&gt;%\n  select(YEAR, all_of(needed), R3haddock) %&gt;%\n  drop_na()\n\nwrite.csv(model_data, \"selected_predictors_dataset.csv\", row.names = FALSE)\nglimpse(model_data)\n\nRows: 32\nColumns: 7\n$ YEAR      &lt;dbl&gt; 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, ~\n$ codTSB    &lt;dbl&gt; 913000, 1347064, 1687381, 2197863, 2112773, 1849957, 1697388~\n$ T12       &lt;dbl&gt; 4.72, 4.66, 4.24, 3.90, 3.96, 4.27, 4.16, 4.07, 4.23, 5.08, ~\n$ I5        &lt;dbl&gt; 43, 55, 26, 49, 56, 28, 52, 51, 69, 68, 41, 48, 50, 63, 40, ~\n$ NAOspring &lt;dbl&gt; 0.64333333, 0.05666667, 1.78666667, 0.28666667, 0.61000000, ~\n$ haddock68 &lt;dbl&gt; 74586, 79205, 53195, 36337, 49122, 81514, 172177, 160886, 96~\n$ R3haddock &lt;dbl&gt; 812363, 389416, 99474, 98946, 118812, 63028, 147657, 83270, ~\n\n# (необязательно) Глянуть попарные связи и корреляции\n# ggpairs может быть медленным, оставим по желанию\n ggpairs(model_data, columns = 2:7,\n         lower = list(continuous = wrap(\"smooth\", alpha = 0.3, size = 0.5)),\n         upper = list(cor = wrap(\"cor\", size = 3)))\n\n\n\n\n\n\n\n# ==============================================================================\n# 2) БАЗОВОЕ СРАВНЕНИЕ МОДЕЛЕЙ (5-FOLD CV + HOLDOUT)\n# Единые фолды CV, тренировочно-тестовое разбиение, сводка метрик.\n# ------------------------------------------------------------------------------\n# Идея блока: быстрая «панель» сравнения разных семейств моделей на одинаковых\n# условиях (одинаковые фолды CV) и внешний тест (holdout). Это помогает увидеть\n# уровни ошибок и выбрать несколько лидеров для более строгой проверки далее.\n# ==============================================================================\n\n# 2.1 Пакеты и данные\npacman::p_load(mgcv, randomForest, xgboost, nnet, earth, kernlab, pls, Cubist, ranger, gbm, lattice)\n\nmodel_data &lt;- read.csv(\"selected_predictors_dataset.csv\", header = TRUE, stringsAsFactors = FALSE)\n# Если YEAR отсутствует (на всякий случай), создадим\nif (!\"YEAR\" %in% names(model_data)) {\n  model_data$YEAR &lt;- seq(1990, by = 1, length.out = nrow(model_data))\n}\n\n# Используем только предикторы и отклик (YEAR исключаем)\nmodel_data &lt;- model_data %&gt;%\n  select(codTSB, T12, I5, NAOspring, haddock68, R3haddock) %&gt;%\n  na.omit()\n\n# 2.2 Holdout и CV-контроллер\n# Пропорция 80/20 обеспечивает внешний тест; внутри train — 5-fold CV для\n# корректной настройки моделей и оценки средней ошибки.\ntrain_idx &lt;- caret::createDataPartition(model_data$R3haddock, p = 0.8, list = FALSE)\ntrain &lt;- model_data[train_idx, ]\ntest  &lt;- model_data[-train_idx, ]\n\nctrl &lt;- caret::trainControl(method = \"cv\", number = 5, savePredictions = \"final\")\n\n# Holdout-метод: мы делим данные на обучающую (80%) и тестовую (20%) выборки.\n# Кросс-валидация (5-fold CV): данные разбиваются на 5 частей, модель обучается на 4 частях и тестируется на 5-й, \n# и этот процесс повторяется 5 раз. Это дает более надежную оценку качества модели, чем одно разбиение. \n\n\n# 2.3 Кастомный GAM (mgcv) для caret (bs=\"tp\", REML, select=TRUE)\n# GAM даёт гладкие нелинейности по каждому признаку; REML стабилизирует оценку.\ngam_spec &lt;- list(\n  type = \"Regression\", library = \"mgcv\", loop = NULL,\n  parameters = data.frame(parameter = \"none\", class = \"character\", label = \"none\"),\n  grid = function(x,y,len=NULL,search=\"grid\") data.frame(none = NA),\n  fit = function(x,y,...) {\n    df &lt;- x; df$R3haddock &lt;- y\n    mgcv::gam(\n      R3haddock ~ s(codTSB,bs=\"tp\") + s(T12,bs=\"tp\") + s(I5,bs=\"tp\") +\n                  s(NAOspring,bs=\"tp\") + s(haddock68,bs=\"tp\"),\n      data=df, method=\"REML\", select=TRUE, ...\n    )\n  },\n  predict = function(modelFit, newdata, submodels = NULL) {\n    predict(modelFit, newdata = newdata, type = \"response\")\n  },\n  prob = NULL, sort = function(x) x\n)\n\n# 2.4 Обучение моделей\n# Подсказка: разные методы по-разному чувствительны к масштабу, числу признаков\n# и мультиколлинеарности. Мы применяем одинаковые фолды CV для честного сравнения.\n\n# --- 1. Линейная регрессия (LM)\n# Учебный смысл: базовая линейная модель; ориентир для сравнения.\n# ПОЯСНЕНИЕ: LM предполагает линейную зависимость между предикторами и откликом.\n# Это простая модель, которая служит \"нижней планкой\" - более сложные модели могут быть лучше LM. \nlm_model    &lt;- caret::train(R3haddock ~ ., data = train, method = \"lm\", trControl = ctrl)\n\n# --- 2. Обобщённая линейная модель (GLM: Gamma с лог-ссылкой)\n# Учебный смысл: модель для положительных откликов; допускает нелинейность в шкале log.\n# ПОЯСНЕНИЕ: GLM с Gamma-распределением подходит для положительных непрерывных данных \n# (как размер популяции), где дисперсия зависит от среднего значения.\nglm_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"glm\",\n                            family = Gamma(link = \"log\"), trControl = ctrl)\n\n# --- 3. Обобщённая аддитивная модель (GAM, mgcv: bs=\"tp\", REML, select=TRUE)\n# Учебный смысл: гибкие гладкие нелинейности по каждому предиктору.\n# ПОЯСНЕНИЕ: GAM позволяет моделировать нелинейные зависимости с помощью гладких функций (splines),\n# сохраняя интерпретируемость отдельных эффектов. Это компромисс между простотой LM и сложностью ML.\ngam_model   &lt;- caret::train(x = train[, -which(names(train)==\"R3haddock\")],\n                            y = train$R3haddock, method = gam_spec, trControl = ctrl)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\n# --- 4. Random Forest (rf: ntree=1000, mtry=1)\n# Учебный смысл: ансамбль деревьев; устойчив к шуму; нелинейности/взаимодействия \"из коробки\".\n# ПОЯСНЕНИЕ: Random Forest строит множество деревьев решений и усредняет их результаты.\n# Это мощный метод, который автоматически улавливает нелинейные зависимости и взаимодействия. \nrf_model    &lt;- caret::train(R3haddock ~ ., data = train, method = \"rf\", trControl = ctrl,\n                            ntree = 1000, tuneGrid = data.frame(mtry = 1), importance = TRUE)\n\n# --- 5. XGBoost (xgbTree) \n# Учебный смысл: бустинг деревьев; сильная ML-модель, легко переобучается без валидации.\n# ПОЯСНЕНИЕ: XGBoost - это градиентный бустинг над деревьями решений, который последовательно \n# строит деревья, исправляя ошибки предыдущих. Требует тщательной настройки параметров.\nxgb_grid    &lt;- expand.grid(nrounds=100, max_depth=4, eta=0.1, gamma=0,\n                           colsample_bytree=0.8, min_child_weight=1, subsample=0.8)\nxgb_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"xgbTree\",\n                            trControl = ctrl, tuneGrid = xgb_grid, verbose = 0)\n\n# --- 6. Нейросеть (MLP, nnet: линейный выход, стандартизация)\n# Учебный смысл: универсальный аппроксиматор; чувствителен к масштабу; требует регуляризации.\n# ПОЯСНЕНИЕ: Нейронные сети могут моделировать сложные нелинейные отношения. \n# Используемая архитектура (1 скрытый слой) - компромисс между гибкостью и риском переобучения.\n# Линейный выходной слой подходит для регрессии.\nnnet_model  &lt;- caret::train(R3haddock ~ ., data = train, method = \"nnet\",\n                            trControl = ctrl, preProcess = c(\"center\",\"scale\"),\n                            tuneGrid = expand.grid(size = 5, decay = 0.1),\n                            linout = TRUE, trace = FALSE, MaxNWts = 5000)\n\n# --- 7. Elastic Net (glmnet)\n# Учебный смысл: регуляризация (L1/L2), борьба с мультиколлинеарностью, частичный отбор признаков.\n# ПОЯСНЕНИЕ: Комбинирует L1 (лассо) и L2 (ридж) регуляризации. Автоматически отбирает признаки \n# и уменьшает влияние мультиколлинеарности. Параметр alpha балансирует между лассо и риджем.\nglmnet_model&lt;- caret::train(R3haddock ~ ., data = train, method = \"glmnet\",\n                            trControl = ctrl, preProcess = c(\"center\",\"scale\"), tuneLength = 10)\n\n# --- 8. MARS (earth)\n# Учебный смысл: кусочно-линейные сплайны + простые взаимодействия; гибкая интерпретация.\n# ПОЯСНЕНИЕ: Многомерные адаптивные регрессионные сплайны (MARS) строят кусочно-линейные модели \n# с автоматическим выбором точек излома. Поддерживает взаимодействия ограниченного порядка.\nearth_model &lt;- caret::train(R3haddock ~ ., data = train, method = \"earth\",\n                            trControl = ctrl, tuneLength = 10)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\n# --- 9. SVM с радиальным ядром (svmRadial)\n# Учебный смысл: ядровой метод; улавливает сложные нелинейности; важна стандартизация.\n# ПОЯСНЕНИЕ: Метод опорных векторов с радиальным ядром проецирует данные в пространство \n# высокой размерности, где становится возможным линейное разделение. Параметр gamma управляет \n# гибкостью границы решения.\nsvm_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"svmRadial\",\n                            trControl = ctrl, preProcess = c(\"center\",\"scale\"), tuneLength = 8)\n\n# --- 10. k-ближайших соседей (kNN)\n# Учебный смысл: простая интуитивная нелинейная модель на расстояниях; чувствительна к масштабу.\n# ПОЯСНЕНИЕ: Предсказание основано на усреднении значений k ближайших наблюдений. \n# Требует вычисления попарных расстояний, что может быть ресурсоемким при больших данных.\nknn_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"knn\",\n                            trControl = ctrl, preProcess = c(\"center\",\"scale\"), tuneLength = 15)\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 23 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 25 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 27 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 29 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 31 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 33 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 23 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 25 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 27 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 29 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 31 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 33 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59980040948893,\n-0.134264066935858, : k = 25 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59980040948893,\n-0.134264066935858, : k = 27 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59980040948893,\n-0.134264066935858, : k = 29 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59980040948893,\n-0.134264066935858, : k = 31 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59980040948893,\n-0.134264066935858, : k = 33 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 23 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 25 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 27 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 29 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 31 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 33 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.07995722209223,\n-0.0328010741655768, : k = 25 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.07995722209223,\n-0.0328010741655768, : k = 27 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.07995722209223,\n-0.0328010741655768, : k = 29 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.07995722209223,\n-0.0328010741655768, : k = 31 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.07995722209223,\n-0.0328010741655768, : k = 33 exceeds number 23 of patterns\n\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\n# --- 11. Ranger (быстрый Random Forest)\n# Учебный смысл: альтернативная/быстрая реализация леса; сравнить с randomForest.\n# ПОЯСНЕНИЕ: Оптимизированная реализация Random Forest на C++. Поддерживает распараллеливание \n# и эффективную работу с категориальными переменными. Важен параметр mtry (число признаков в узле).\nranger_model&lt;- caret::train(R3haddock ~ ., data = train, method = \"ranger\",\n                            trControl = ctrl, tuneLength = 3, importance = \"impurity\")\n\n# --- 12. GBM (классический градиентный бустинг)\n# Учебный смысл: другой бустинг деревьев; полезно сравнить с XGBoost.\n# ПОЯСНЕНИЕ: Градиентный бустинг строит деревья последовательно, где каждое новое дерево \n# корректирует ошибки предыдущих. Параметр shrinkage (темп обучения) контролирует скорость обучения.\ngbm_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"gbm\",\n                            trControl = ctrl,\n                            tuneGrid = expand.grid(n.trees=100, interaction.depth=1,\n                                                   shrinkage=0.1, n.minobsinnode=2),\n                            distribution = \"gaussian\", bag.fraction = 1, verbose = FALSE)\n\n# --- 13. PLS (Partial Least Squares)\n# Учебный смысл: проекция на скрытые компоненты с учетом отклика; решает мультиколлинеарность.\n# ПОЯСНЕНИЕ: Частные наименьшие квадраты (PLS) проецируют предикторы в латентное пространство, \n# максимизируя ковариацию с откликом. Эффективен при высокой корреляции признаков.\npls_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"pls\",\n                            trControl = ctrl, preProcess = c(\"center\",\"scale\"), tuneLength = 10)\n\n# --- 14. Cubist (правила + деревья)\n# Учебный смысл: интерпретируемые правила с комитетами; часто силен на табличных данных.\n# ПОЯСНЕНИЕ: Cubist объединяет деревья решений с линейными моделями в листьях. Генерирует \n# набор правил \"если-то\", что улучшает интерпретируемость. Комитеты (комитеты) уменьшают дисперсию.\ncubist_model&lt;- caret::train(R3haddock ~ ., data = train, method = \"cubist\",\n                            trControl = ctrl, tuneLength = 5)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\n# 2.5 Метрики и оценка на тесте\n# Замечание: RMSE/MAE — абсолютные ошибки; R2 — доля объяснённой вариации;\n# MAPE/sMAPE — относительные ошибки (осторожно при малых значениях отклика).\nrmse  &lt;- function(a, p) sqrt(mean((a - p)^2, na.rm = TRUE))\nmae   &lt;- function(a, p) mean(abs(a - p), na.rm = TRUE)\nr2    &lt;- function(a, p) 1 - sum((a - p)^2, na.rm = TRUE) / sum((a - mean(a))^2, na.rm = TRUE)\nmape  &lt;- function(a, p) mean(abs((a - p) / a), na.rm = TRUE) * 100\nsmape &lt;- function(a, p) mean(2 * abs(p - a) / (abs(a) + abs(p)), na.rm = TRUE) * 100\nmetrics_vec &lt;- function(y, pred) c(RMSE=rmse(y,pred), MAE=mae(y,pred), R2=r2(y,pred),\n                                   MAPE=mape(y,pred), sMAPE=smape(y,pred))\n\n# Для оценки качества моделей мы используем несколько метрик:\n#   - RMSE (Root Mean Square Error): среднеквадратичная ошибка (чувствительна к выбросам)\n#   - MAE (Mean Absolute Error): средняя абсолютная ошибка (более интерпретируема)\n#   - R²: коэффициент детерминации (доля объясненной дисперсии)\n#   - MAPE: средняя абсолютная процентная ошибка (в процентах от фактического значения)\n#   - sMAPE: симметричная MAPE (устраняет проблему деления на ноль) \n\ny_test &lt;- test$R3haddock\npreds_test &lt;- list(\n  LM=predict(lm_model,test), GLM=predict(glm_model,test), GAM=predict(gam_model,test),\n  RF=predict(rf_model,test), XGB=predict(xgb_model,test), NNET=predict(nnet_model,test),\n  ENet=predict(glmnet_model,test), MARS=predict(earth_model,test), SVM=predict(svm_model,test),\n  kNN=predict(knn_model,test), RANGER=predict(ranger_model,test), GBM=predict(gbm_model,test),\n  PLS=predict(pls_model,test), CUBIST=predict(cubist_model,test)\n)\nmetrics_table &lt;- do.call(rbind, lapply(names(preds_test), function(nm){\n  data.frame(Model = nm, t(metrics_vec(y_test, preds_test[[nm]])), row.names = NULL)\n})) %&gt;% arrange(RMSE, MAE)\n\n# Создаем копию таблицы для округления\nmetrics_table_rounded &lt;- metrics_table\n\n# Находим индексы числовых столбцов (исключая первый столбец \"Model\")\nnumeric_cols &lt;- sapply(metrics_table_rounded, is.numeric)\n\n# Округляем только числовые столбцы до 2 знаков\nmetrics_table_rounded[numeric_cols] &lt;- round(metrics_table_rounded[numeric_cols], 2)\n\n# Выводим округленную таблицу\nprint(metrics_table_rounded)\n\n    Model      RMSE       MAE    R2   MAPE sMAPE\n1     GBM  65112.49  55805.52  0.76  34.41 27.07\n2    MARS  95150.47  78452.71  0.49  51.69 36.18\n3      RF 125056.40  97862.47  0.12  74.27 44.45\n4     PLS 130411.86 111694.80  0.04  62.51 47.69\n5      LM 131592.23 113063.92  0.02  63.62 48.29\n6     GAM 140393.68 121236.06 -0.11  70.56 48.99\n7  CUBIST 147031.92 123320.33 -0.22  50.52 46.56\n8    ENet 147470.05 124147.91 -0.23  81.27 52.96\n9  RANGER 148623.73 127762.15 -0.25  87.21 54.25\n10    kNN 149082.19 128657.28 -0.26  88.53 53.55\n11    GLM 151391.83 129510.77 -0.30  74.40 55.01\n12    SVM 167864.37 147269.48 -0.59 104.73 57.83\n13    XGB 208719.19 194346.48 -1.46  88.97 65.24\n14   NNET 216172.26 199594.44 -1.64  93.12 87.93\n\nknitr::kable(\n  metrics_table %&gt;% dplyr::mutate(dplyr::across(where(is.numeric), ~round(.x, 2))),\n  caption = \"Holdout-метрики (округлено до 2 знаков)\"\n)\n\n\nHoldout-метрики (округлено до 2 знаков)\n\n\nModel\nRMSE\nMAE\nR2\nMAPE\nsMAPE\n\n\n\n\nGBM\n65112.49\n55805.52\n0.76\n34.41\n27.07\n\n\nMARS\n95150.47\n78452.71\n0.49\n51.69\n36.18\n\n\nRF\n125056.40\n97862.47\n0.12\n74.27\n44.45\n\n\nPLS\n130411.86\n111694.80\n0.04\n62.51\n47.69\n\n\nLM\n131592.23\n113063.92\n0.02\n63.62\n48.29\n\n\nGAM\n140393.68\n121236.06\n-0.11\n70.56\n48.99\n\n\nCUBIST\n147031.92\n123320.33\n-0.22\n50.52\n46.56\n\n\nENet\n147470.05\n124147.91\n-0.23\n81.27\n52.96\n\n\nRANGER\n148623.73\n127762.15\n-0.25\n87.21\n54.25\n\n\nkNN\n149082.19\n128657.28\n-0.26\n88.53\n53.55\n\n\nGLM\n151391.83\n129510.77\n-0.30\n74.40\n55.01\n\n\nSVM\n167864.37\n147269.48\n-0.59\n104.73\n57.83\n\n\nXGB\n208719.19\n194346.48\n-1.46\n88.97\n65.24\n\n\nNNET\n216172.26\n199594.44\n-1.64\n93.12\n87.93\n\n\n\n\n# 2.6 CV-резюме\n# Сводим результаты CV по всем моделям и смотрим распределения ошибок.\nresults &lt;- caret::resamples(list(\n  LM=lm_model, GLM=glm_model, GAM=gam_model, RF=rf_model, XGB=xgb_model, NNET=nnet_model,\n  ENet=glmnet_model, MARS=earth_model, SVM=svm_model, kNN=knn_model, RANGER=ranger_model,\n  GBM=gbm_model, PLS=pls_model, CUBIST=cubist_model\n))\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: LM, GLM, GAM, RF, XGB, NNET, ENet, MARS, SVM, kNN, RANGER, GBM, PLS, CUBIST \nNumber of resamples: 5 \n\nMAE \n            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLM      57204.50 173765.7 178162.9 167545.5 210890.4 217704.1    0\nGLM    121162.89 123189.9 126872.5 161564.3 215648.4 220947.6    0\nGAM    113581.17 186001.4 194396.8 216924.0 243663.3 346977.3    0\nRF     140917.34 173192.3 183106.9 211428.5 199504.4 360421.6    0\nXGB    191739.57 196545.5 204224.0 220586.8 211582.9 298842.3    0\nNNET   210800.50 230696.3 231946.6 255228.9 268869.5 333831.8    0\nENet    95118.26 114161.5 186843.2 175901.3 203787.5 279596.0    0\nMARS   100715.25 160470.7 226674.7 224830.7 281753.1 354539.8    0\nSVM    137655.75 168734.2 186772.6 218295.0 270224.5 328087.8    0\nkNN    153725.61 173448.8 204644.5 201592.1 210974.4 265167.1    0\nRANGER 134039.92 173499.6 173657.3 188631.8 213348.0 248614.3    0\nGBM    142143.64 174377.4 182594.4 195116.5 210961.4 265505.7    0\nPLS    140180.64 169826.6 174374.4 174539.4 177221.3 211093.9    0\nCUBIST  40943.57 166731.9 172415.9 151410.2 183157.2 193802.4    0\n\nRMSE \n            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLM      83394.05 204460.2 213949.1 198791.4 234397.7 257756.2    0\nGLM    144920.86 174777.3 199683.5 211248.8 268416.8 268445.7    0\nGAM    126481.10 212799.6 231166.1 251450.9 265213.5 421594.3    0\nRF     152505.15 221538.9 237863.1 264290.3 246285.2 463259.3    0\nXGB    224482.10 230814.5 274803.3 282033.4 324393.6 355673.5    0\nNNET   262262.48 292982.1 298735.5 322398.1 312361.7 445649.0    0\nENet    96405.72 194703.2 212260.2 215307.8 227408.1 345761.6    0\nMARS   133708.92 174187.4 275486.7 264992.8 296803.1 444778.0    0\nSVM    175110.30 208575.8 211026.8 261495.0 330657.7 382104.5    0\nkNN    191674.11 194743.2 293783.4 261451.2 307657.4 319397.8    0\nRANGER 179305.47 214964.7 252239.7 245471.8 256030.3 324818.7    0\nGBM    155235.18 236112.1 265637.9 269271.4 342304.1 347067.5    0\nPLS    192456.23 204052.4 206556.2 210244.7 207702.6 240455.9    0\nCUBIST  51000.20 217680.6 220028.3 190972.0 226862.3 239288.7    0\n\nRsquared \n              Min.    1st Qu.     Median      Mean   3rd Qu.      Max. NA's\nLM     0.038191154 0.34151126 0.59570117 0.5146318 0.6446897 0.9530659    0\nGLM    0.036094303 0.25361489 0.52140959 0.5247907 0.8499520 0.9628829    0\nGAM    0.078247285 0.20151523 0.39934478 0.3553497 0.5269644 0.5706765    0\nRF     0.007930602 0.01517051 0.11630358 0.1925039 0.1207826 0.7023323    0\nXGB    0.041587807 0.09841163 0.11696588 0.2060251 0.2118923 0.5612680    0\nNNET   0.018389559 0.02872886 0.07737108 0.1597232 0.1157484 0.5583779    0\nENet   0.170405604 0.55004140 0.62684865 0.5443111 0.6741520 0.7001081    0\nMARS   0.007289893 0.04026192 0.24769274 0.3088361 0.3755802 0.8733557    0\nSVM    0.044855520 0.06565037 0.24061923 0.2637711 0.4593310 0.5083994    0\nkNN    0.001714988 0.10251708 0.41997970 0.3013771 0.4201459 0.5625279    0\nRANGER 0.006761073 0.20688755 0.22338943 0.2919063 0.4553028 0.5671905    0\nGBM    0.056729819 0.15259735 0.27867315 0.2779509 0.3246546 0.5770994    0\nPLS    0.255560646 0.39058052 0.43207618 0.5269205 0.6127945 0.9435907    0\nCUBIST 0.004399571 0.20141307 0.52557866 0.5053060 0.8675585 0.9275803    0\n\nlattice::dotplot(results, metric = \"RMSE\")\n\n\n\n\n\n\n\n# ==============================================================================\n# 3) ВЫБОР ЛУЧШЕЙ ПРОГНОСТИЧЕСКОЙ МОДЕЛИ (TIME-SLICE CV НА 3 ГОДА + ХРОНО-ТЕСТ)\n# Делим последние годы в тест, внутри train — скользящее окно, h=3.\n# ------------------------------------------------------------------------------\n# Почему time-slice: временные данные нельзя случайно перемешивать, иначе мы\n# «подсматриваем в будущее». Создаём серии обучающих/валидационных окон,\n# увеличивая тренировочный период, и тестируем на ближайшем горизонте (3 года).\n# ==============================================================================\n\n# 3.1 Данные для time-slice (с YEAR)\nmodel_data &lt;- read.csv(\"selected_predictors_dataset.csv\", header = TRUE, stringsAsFactors = FALSE)\nif (!\"YEAR\" %in% names(model_data)) {\n  model_data$YEAR &lt;- seq(1990, by = 1, length.out = nrow(model_data))\n}\n# Хронологический порядок\nmodel_data &lt;- model_data %&gt;% arrange(YEAR)\n\n# Для временных рядов обычные методы кросс-валидации (случайное разбиение) неприменимы,\n# так как это приведет к утечке информации из будущего в прошлое. [[1]]\n# Time-slice CV (скользящее окно) имитирует реальную ситуацию прогнозирования:\n#   - Мы обучаемся на данных из прошлого\n#   - Прогнозируем на несколько шагов вперед\n#   - Последовательно сдвигаем окно обучения вперед\n\n# Исходные фичи (исключаем YEAR)\nmd_for_fit &lt;- model_data %&gt;% select(codTSB, T12, I5, NAOspring, haddock68, R3haddock)\n\n# 3.2 Хронологический holdout (последние годы)\n# Идея: отложим ~20% последних лет как полностью внешний тест будущего качества.\nn &lt;- nrow(md_for_fit)\nholdout_frac &lt;- 0.2\nn_test &lt;- max(4, ceiling(n * holdout_frac))\ntrain_ts &lt;- head(md_for_fit, n - n_test)\ntest_ts  &lt;- tail(md_for_fit, n_test)\n\n# 3.3 Time-slice CV (h=3, expanding window рекомендован: fixedWindow=FALSE)\n# initialWindow — размер первого «обучающего» фрагмента; horizon — горизонт\n# валидации (здесь 3 года). Далее окно расширяется.\nn_train &lt;- nrow(train_ts)\ninitial_frac &lt;- 0.6\nhorizon      &lt;- 3\ninitialWindow &lt;- max(10, floor(initial_frac * n_train))\nif (initialWindow + horizon &gt; n_train) initialWindow &lt;- n_train - horizon\n\nslices &lt;- caret::createTimeSlices(1:n_train, initialWindow = initialWindow,\n                                  horizon = horizon, fixedWindow = FALSE)\nctrl_ts &lt;- caret::trainControl(method = \"cv\", index = slices$train, indexOut = slices$test,\n                               savePredictions = \"final\")\n\n# В нашем случае:\n#   - horizon = 3: прогнозируем на 3 года вперед\n#   - expanding window: размер обучающей выборки увеличивается с каждым шагом\n#   - initialWindow: начальный размер обучающей выборки (60% от данных)\n# Этот подход наиболее реалистичен для задач прогнозирования временных рядов в гидробиологии.\n\n\n# 3.4 Обучение (ядро набора, без GBM — он нестабилен на малом n в timeslice)\n# Примечание: используем ту же рецептуру, что и в базовом сравнении, но с\n# хронологическими срезами.\n\nfit_ts &lt;- function(method, form, data, ctrl, ...) {\n  out &lt;- try(caret::train(form, data = data, method = method, trControl = ctrl, ...), TRUE)\n  if (inherits(out,\"try-error\")) NULL else out\n}\nlm_ts   &lt;- fit_ts(\"lm\",        R3haddock ~ ., train_ts, ctrl_ts)\nglm_ts  &lt;- fit_ts(\"glm\",       R3haddock ~ ., train_ts, ctrl_ts, family = Gamma(link=\"log\"))\ngam_ts  &lt;- caret::train(x = train_ts[, -which(names(train_ts)==\"R3haddock\")],\n                        y = train_ts$R3haddock, method = gam_spec, trControl = ctrl_ts)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nrf_ts   &lt;- fit_ts(\"rf\",        R3haddock ~ ., train_ts, ctrl_ts, ntree=1000, tuneGrid=data.frame(mtry=1))\nxgb_ts  &lt;- fit_ts(\"xgbTree\",   R3haddock ~ ., train_ts, ctrl_ts, tuneGrid = xgb_grid, verbose = 0)\nrgr_ts  &lt;- fit_ts(\"ranger\",    R3haddock ~ ., train_ts, ctrl_ts, tuneLength=3)\nnnet_ts &lt;- fit_ts(\"nnet\",      R3haddock ~ ., train_ts, ctrl_ts,\n                  preProcess=c(\"center\",\"scale\"),\n                  tuneGrid=expand.grid(size=5,decay=0.1), linout=TRUE, trace=FALSE, MaxNWts=5000)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nsvm_ts  &lt;- fit_ts(\"svmRadial\", R3haddock ~ ., train_ts, ctrl_ts, preProcess=c(\"center\",\"scale\"), tuneLength=8)\nknn_ts  &lt;- fit_ts(\"knn\",       R3haddock ~ ., train_ts, ctrl_ts, preProcess=c(\"center\",\"scale\"), tuneLength=15)\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 17 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 19 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 21 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 23 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 25 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 27 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 29 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 31 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 33 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 17 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 19 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 21 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 23 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 25 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 27 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 29 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 31 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 33 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 19 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 21 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 23 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 25 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 27 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 29 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 31 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 33 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 19 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 21 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 23 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 25 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 27 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 29 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 31 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 33 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 21 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 23 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 25 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 27 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 29 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 31 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 33 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 21 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 23 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 25 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 27 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 29 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 31 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 33 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 23 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 25 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 27 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 29 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 31 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 33 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 23 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 25 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 27 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 29 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 31 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 33 exceeds number 22 of patterns\n\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nenet_ts &lt;- fit_ts(\"glmnet\",    R3haddock ~ ., train_ts, ctrl_ts, preProcess=c(\"center\",\"scale\"), tuneLength=10)\nmars_ts &lt;- fit_ts(\"earth\",     R3haddock ~ ., train_ts, ctrl_ts, tuneLength=10)\npls_ts  &lt;- fit_ts(\"pls\",       R3haddock ~ ., train_ts, ctrl_ts, preProcess=c(\"center\",\"scale\"), tuneLength=10)\ncub_ts  &lt;- fit_ts(\"cubist\",    R3haddock ~ ., train_ts, ctrl_ts, tuneLength=5)\n\nmodels_ts &lt;- list(LM=lm_ts, GLM=glm_ts, GAM=gam_ts, RF=rf_ts, XGB=xgb_ts, RANGER=rgr_ts,\n                  NNET=nnet_ts, SVM=svm_ts, kNN=knn_ts, ENet=enet_ts, MARS=mars_ts, PLS=pls_ts, CUBIST=cub_ts)\nmodels_ts &lt;- models_ts[!vapply(models_ts, is.null, logical(1))]\n\n# 3.5 Ранжирование по time-slice CV и по хронологическому тесту\n# Сначала ранжируем по средним ошибкам на валидационных срезах, затем — по внешнему тесту.\ncv_metrics &lt;- function(m) {\n  if (is.null(m$pred) || !\"Resample\" %in% names(m$pred)) return(c(RMSE=NA, MAE=NA))\n  by_slice &lt;- m$pred %&gt;% group_by(Resample) %&gt;%\n    summarise(RMSE=rmse(obs,pred), MAE=mae(obs,pred), .groups=\"drop\")\n  c(RMSE = mean(by_slice$RMSE, na.rm = TRUE), MAE = mean(by_slice$MAE, na.rm = TRUE))\n}\ncv_rank &lt;- do.call(rbind, lapply(models_ts, cv_metrics)) %&gt;% as.data.frame()\ncv_rank$Model &lt;- rownames(cv_rank)\ncv_rank &lt;- cv_rank[is.finite(cv_rank$RMSE), ] %&gt;% relocate(Model) %&gt;% arrange(RMSE, MAE)\ncat(\"\\nTime-slice CV (h=3), средние RMSE/MAE:\\n\"); print(cv_rank)\n\n\nTime-slice CV (h=3), средние RMSE/MAE:\n\n\n        Model     RMSE      MAE\nSVM       SVM 227929.6 191921.6\nkNN       kNN 234897.8 197135.0\nENet     ENet 250989.0 214248.7\nXGB       XGB 277153.4 248532.6\nRANGER RANGER 280255.1 249992.1\nGLM       GLM 280259.5 237186.9\nNNET     NNET 296856.1 264368.2\nPLS       PLS 302968.3 274707.6\nRF         RF 303710.0 263019.5\nCUBIST CUBIST 314443.4 281437.9\nLM         LM 370298.1 340883.8\nMARS     MARS 427624.1 378476.5\nGAM       GAM 714951.0 625520.3\n\npreds_ts &lt;- lapply(models_ts, function(m) try(predict(m, newdata = test_ts), TRUE))\nkeep &lt;- vapply(preds_ts, function(p) is.numeric(p) && length(p)==nrow(test_ts) && all(is.finite(p)), logical(1))\npreds_ts &lt;- preds_ts[keep]\ntest_rank &lt;- do.call(rbind, lapply(names(preds_ts), function(nm){\n  data.frame(Model=nm, t(metrics_vec(test_ts$R3haddock, preds_ts[[nm]])), row.names = NULL)\n})) %&gt;% arrange(RMSE, MAE)\ncat(\"\\nХронологический тест (последние годы), RMSE/MAE/R2:\\n\"); print(test_rank)\n\n\nХронологический тест (последние годы), RMSE/MAE/R2:\n\n\n    Model     RMSE      MAE         R2      MAPE     sMAPE\n1  CUBIST 148248.4 107629.3  0.5774780  54.93724  38.06342\n2      LM 156940.0 129604.7  0.5264822  97.30313  49.10292\n3     GAM 158131.0 125038.0  0.5192677  51.33030  42.12742\n4     PLS 176786.2 138249.0  0.3991499 123.21995  50.93728\n5     GLM 182047.7 141692.4  0.3628531  73.71724  50.32932\n6      RF 185485.8 148117.2  0.3385600  96.29497  52.15688\n7     kNN 187966.9 143164.4  0.3207460  71.77551  50.80097\n8    ENet 195260.5 161191.7  0.2670102 115.55953  56.27135\n9  RANGER 208161.4 171717.0  0.1669526 115.93679  58.76935\n10    SVM 250994.6 197801.6 -0.2111500  84.52010  67.59732\n11   MARS 273186.2 208579.6 -0.4347849  85.29788  71.46314\n12    XGB 288167.4 233693.4 -0.5964639 102.19238  78.99024\n13   NNET 345227.3 291463.2 -1.2912878 233.62022 102.82204\n\n# ==============================================================================\n# 4) ПРОГНОЗ 2022–2024 (АНСАМБЛЬ CUBIST+LM) И ГРАФИК 1990–2024 С ДИ\n# Прогнозные линии (медиана и ДИ) — пунктир; исторические — сплошные.\n# Можно задать свои сценарии предикторов (user_future); по умолчанию — средние.\n# ------------------------------------------------------------------------------\n# Логика ансамбля: комбинируем сильную нелинейную модель (Cubist) с простой и\n# устойчивой линейной (LM). Веса можно настраивать. Доверительные интервалы\n# получаем эмпирически из распределения остатков (простая и наглядная эвристика).\n# ==============================================================================\n\n# 4.1 Полные модели для прогноза (на всех данных) и вес ансамбля\nmodel_data &lt;- read.csv(\"selected_predictors_dataset.csv\", header = TRUE, stringsAsFactors = FALSE)\nif (!\"YEAR\" %in% names(model_data)) {\n  model_data$YEAR &lt;- seq(1990, by = 1, length.out = nrow(model_data))\n}\nmodel_data &lt;- model_data %&gt;% arrange(YEAR)\n\ncubist_full &lt;- caret::train(R3haddock ~ codTSB + T12 + I5 + NAOspring + haddock68,\n                            data = model_data, method = \"cubist\",\n                            trControl = caret::trainControl(method=\"none\"),\n                            tuneGrid = if (exists(\"cubist_model\")) cubist_model$bestTune else NULL,\n                            tuneLength = if (exists(\"cubist_model\")) 1 else 5)\n\nlm_full &lt;- caret::train(R3haddock ~ codTSB + T12 + I5 + NAOspring + haddock68,\n                        data = model_data, method = \"lm\",\n                        trControl = caret::trainControl(method=\"none\"))\n\nalpha_opt &lt;- if (exists(\"alpha_opt\")) alpha_opt else 0.75\npredict_ensemble &lt;- function(newdata, alpha = alpha_opt) {\n  alpha * predict(cubist_full, newdata) + (1 - alpha) * predict(lm_full, newdata)\n}\n\n# Ансамбль моделей часто дает более точные и устойчивые прогнозы, чем отдельные модели. [[8]]\n# В нашем случае:\n#   - CUBIST: мощная модель, основанная на правилах, хорошо работающая с табличными данными\n#   - LM: простая интерпретируемая модель, устойчивая к шуму\n#   - alpha_opt = 0.75: веса ансамбля (75% CUBIST, 25% LM), оптимизированные ранее (см. скрипт \"ENS_WEIGHT.R\")\n# Комбинирование моделей с разными сильными сторонами снижает риск систематических ошибок.\n\n# 4.2 Остатки для ДИ (из CV, если есть; иначе — по фитам)\n# Эмпирические квантилы остатков дают «практические» интервалы прогноза без\n# предположения нормальности ошибок (хотя строгий PI требует аккуратности).\nget_residuals_for_pi &lt;- function() {\n  if (exists(\"lm_model\") && exists(\"cubist_model\") &&\n      !is.null(lm_model$pred) && !is.null(cubist_model$pred)) {\n    pl &lt;- lm_model$pred %&gt;% select(Resample,rowIndex,obs,p_lm=pred)\n    pc &lt;- cubist_model$pred %&gt;% select(Resample,rowIndex,p_cu=pred)\n    inner_join(pl, pc, by=c(\"Resample\",\"rowIndex\")) %&gt;%\n      mutate(p_ens = alpha_opt * p_cu + (1 - alpha_opt) * p_lm,\n             resid = obs - p_ens) %&gt;%\n      pull(resid) %&gt;% .[is.finite(.)]\n  } else {\n    model_data$R3haddock - predict_ensemble(model_data)\n  }\n}\nresids &lt;- get_residuals_for_pi()\nq025 &lt;- as.numeric(quantile(resids, 0.025, na.rm = TRUE))\nq250 &lt;- as.numeric(quantile(resids, 0.250, na.rm = TRUE))\nq750 &lt;- as.numeric(quantile(resids, 0.750, na.rm = TRUE))\nq975 &lt;- as.numeric(quantile(resids, 0.975, na.rm = TRUE))\n\n# Доверительные интервалы (ДИ) показывают неопределенность прогноза.\n# Мы используем квантили остатков из кросс-валидации для построения ДИ:\n#   - PI50 (50% интервал): между 25-м и 75-м процентилями\n#   - PI95 (95% интервал): между 2.5-м и 97.5-м процентилями\n# Это непараметрический подход, не требующий предположений о нормальности ошибок.\n\n# 4.3 Сценарии будущего (по умолчанию — средние; можно переопределить user_future)\nfc_start &lt;- 2022\npred_cols &lt;- c(\"codTSB\",\"T12\",\"I5\",\"NAOspring\",\"haddock68\")\ntrain_period &lt;- model_data %&gt;% filter(YEAR &gt; 1989 & YEAR &lt; fc_start)\nmu &lt;- train_period %&gt;% summarise(across(all_of(pred_cols), ~mean(.x, na.rm = TRUE))) %&gt;% as.list()\n\n# Пример пользовательского сценария:\n# user_future &lt;- tibble::tribble(\n#   ~YEAR, ~codTSB, ~T12, ~I5, ~NAOspring, ~haddock68,\n#   2022, 2100000, 5.1, 48,  0.3, 120000,\n#   2023, 2050000, 4.8, 50, -0.1, 115000,\n#   2024, 2150000, 5.0, 47,  0.2, 118000\n# )\nif (!exists(\"user_future\")) user_future &lt;- NULL\n\nbuild_future &lt;- function(years, mu, user_df=NULL) {\n  df &lt;- tibble::tibble(YEAR = years)\n  for (v in pred_cols) df[[v]] &lt;- mu[[v]]\n  if (!is.null(user_df)) {\n    for (i in seq_len(nrow(user_df))) {\n      yr &lt;- user_df$YEAR[i]\n      if (yr %in% years) {\n        idx &lt;- which(df$YEAR == yr)\n        for (v in intersect(pred_cols, names(user_df))) {\n          val &lt;- user_df[[v]][i]\n          if (!is.na(val)) df[[v]][idx] &lt;- val\n        }\n      }\n    }\n  }\n  df\n}\nfuture_years &lt;- fc_start:2024\nscenario_future &lt;- build_future(future_years, mu, user_future)\n\n# 4.4 Прогноз и таблица ДИ\npred_future &lt;- predict_ensemble(scenario_future)\nforecast_tbl &lt;- tibble::tibble(\n  YEAR      = scenario_future$YEAR,\n  pred_mean = as.numeric(pred_future),\n  PI50_low  = pred_future + q250, PI50_high = pred_future + q750,\n  PI95_low  = pred_future + q025, PI95_high = pred_future + q975\n)\n\n#### Таблица прогноза 2022–2024\nprint(forecast_tbl)\n\n# A tibble: 3 x 6\n   YEAR pred_mean PI50_low PI50_high PI95_low PI95_high\n  &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  2022   253815.   63865.   536150.  -46219.   668189.\n2  2023   253815.   63865.   536150.  -46219.   668189.\n3  2024   253815.   63865.   536150.  -46219.   668189.\n\n# По умолчанию мы используем средние значения предикторов для прогноза.\n# Однако вы можете определить собственный сценарий (user_future), указав конкретные значения\n# для каждого года и каждого предиктора. Это позволяет моделировать различные экологические сценарии. \n\n# 4.5 Непрерывный ряд 1990–2024 и график: ленты сплошные; линии медианы/ДИ — сплошные до 2021, пунктир с 2022\npred_df &lt;- bind_rows(\n  model_data %&gt;% select(YEAR, all_of(pred_cols)),\n  scenario_future\n) %&gt;% distinct(YEAR, .keep_all = TRUE) %&gt;% arrange(YEAR)\n\npred_df$Pred      &lt;- as.numeric(predict_ensemble(pred_df))\npred_df$PI50_low  &lt;- pred_df$Pred + q250\npred_df$PI50_high &lt;- pred_df$Pred + q750\npred_df$PI95_low  &lt;- pred_df$Pred + q025\npred_df$PI95_high &lt;- pred_df$Pred + q975\n\nhist_df &lt;- model_data %&gt;% select(YEAR, R3haddock)\n\nggplot() +\n  geom_ribbon(data = pred_df, aes(x = YEAR, ymin = PI95_low, ymax = PI95_high),\n              fill = \"grey80\", alpha = 0.25) +\n  geom_ribbon(data = pred_df, aes(x = YEAR, ymin = PI50_low, ymax = PI50_high),\n              fill = \"grey60\", alpha = 0.35) +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = PI95_low),\n            color = \"grey45\", linewidth = 0.6) +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = PI95_high),\n            color = \"grey45\", linewidth = 0.6) +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = PI50_low),\n            color = \"grey35\", linewidth = 0.6) +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = PI50_high),\n            color = \"grey35\", linewidth = 0.6) +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = PI95_low),\n            color = \"grey45\", linewidth = 0.6, linetype = \"dashed\") +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = PI95_high),\n            color = \"grey45\", linewidth = 0.6, linetype = \"dashed\") +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = PI50_low),\n            color = \"grey35\", linewidth = 0.6, linetype = \"dashed\") +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = PI50_high),\n            color = \"grey35\", linewidth = 0.6, linetype = \"dashed\") +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = Pred),\n            color = \"steelblue4\", linewidth = 1) +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = Pred),\n            color = \"steelblue4\", linewidth = 1, linetype = \"dashed\") +\n  geom_point(data = hist_df, aes(x = YEAR, y = R3haddock),\n             color = \"black\", size = 2, alpha = 0.9) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0))) +\n  labs(\n    title = \"Пополнение R3haddock: факт (1990–2021) и прогноз (2022–2024)\\nАнсамбль CUBIST+LM; непрерывные ДИ, прогноз — пунктир\",\n    x = \"Год\", y = \"R3haddock\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# На графике:\n#   - Черные точки: исторические данные (1990-2021)\n#   - Сплошная синяя линия: прогнозные значения (1990-2021)\n#   - Пунктирная синяя линия: прогноз на 2022-2024\n#   - Серые ленты: 50% и 95% доверительные интервалы\n# Такая визуализация позволяет легко интерпретировать как исторические данные, \n# так и будущие прогнозы с учетом неопределенности.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 9.html",
    "href": "chapter 9.html",
    "title": "10  Модели пространственного распределения видов (SDM)",
    "section": "",
    "text": "10.1 Введение",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Модели пространственного распределения видов (SDM)</span>"
    ]
  },
  {
    "objectID": "chapter 9.html#загрузка-данных-и-первичный-осмотр",
    "href": "chapter 9.html#загрузка-данных-и-первичный-осмотр",
    "title": "10  Модели пространственного распределения видов (SDM)",
    "section": "10.2 Загрузка данных и первичный осмотр",
    "text": "10.2 Загрузка данных и первичный осмотр\nТекст. Текст. Текст.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Модели пространственного распределения видов (SDM)</span>"
    ]
  },
  {
    "objectID": "chapter 9.html#описательная-статистика-и-визуализация",
    "href": "chapter 9.html#описательная-статистика-и-визуализация",
    "title": "10  Модели пространственного распределения видов (SDM)",
    "section": "10.3 Описательная статистика и визуализация",
    "text": "10.3 Описательная статистика и визуализация\nТекст. Текст. Текст. Текст.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Модели пространственного распределения видов (SDM)</span>"
    ]
  }
]