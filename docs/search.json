[
  {
    "objectID": "chapter28.html",
    "href": "chapter28.html",
    "title": "29  Байесовский подход",
    "section": "",
    "text": "29.1 Введение\nЕсли частотная статистика — это формальная наука о том, что говорят данные, то байесовский подход — это искусство ведения диалога между данными и нашими предварительными “априорными” знаниями. Байесовская философия радикально проста: мы начинаем не с чистого листа, а с формализованного опыта — априорного распределения. Затем данные вносят свои коррективы, и мы получаем апостериорное распределение — обновлённые знания, которые уже учитывают и экспертизу, и наблюдения. Фундаментальное соотношение байесовского вывода задаётся формулой Байеса:\n\\[\nP(\\theta \\mid D) = \\frac{P(D \\mid \\theta) \\cdot P(\\theta)}{P(D)}\n\\]\nгде:\nНа практике часто используют пропорциональную форму, так как \\(P(D)\\) не зависит от \\(\\theta\\):\n\\[\nP(\\theta \\mid D) \\propto P(D \\mid \\theta) \\cdot P(\\theta)\n\\] В контексте оценки, например, роста рыб это означает: мы можем использовать знания о похожих видах, предыдущие исследования или даже экспертные оценки (“байки бывалых ихтиологов”) , чтобы задать более или менее реалистичные ожидания для параметров уравнения Берталанфи — до того, как увидим первую точку данных.\nВ практике оценки водных биоресурсов мы редко сталкиваемся с полными, сбалансированными и «чистыми» данными. Чаще всего перед нами — разрозненные уловы, фрагментарные съёмки, практически полное отсутствие возрастной структуры, неточная методика сбора и, как следствие, высокая неопределённость. В таких условиях классические частотные методы — опирающиеся на идею «единственной истинной оценки» — часто дают иллюзию точности, скрывая за красивыми p‑значениями хрупкость выводов.\nБайесовский подход предлагает иной путь — не бороться с неопределённостью, а моделировать её явно. Его суть лежит не столько в формулах, сколько в мировоззрении:\nЭто особенно важно в гидробиологии, где экспертные суждения ихтиологов, данные по близкородственным видам, исторические аналоги и даже здравый смысл часто содержат больше информации, чем несколько десятков наблюдений. Байесовский формализм позволяет превратить субъективное мнение в количественную гипотезу, которую можно проверить, скорректировать и объединить с эмпирическими данными.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Байесовский подход</span>"
    ]
  },
  {
    "objectID": "chapter28.html#введение",
    "href": "chapter28.html#введение",
    "title": "29  Байесовский подход",
    "section": "",
    "text": "\\(P(\\theta)\\) — априорная вероятность, отражающая наши знания или убеждения о параметре \\(\\theta\\) до наблюдения данных;\n\\(P(D \\mid \\theta)\\) — правдоподобие, то есть вероятность наблюдать данные \\(D\\) при заданном значении параметра \\(\\theta\\);\n\\(P(\\theta \\mid D)\\) — апостериорная вероятность, представляющая обновлённые знания о \\(\\theta\\) после учёта данных \\(D\\);\n\\(P(D)\\) — нормировочная константа (маргинальное правдоподобие), обеспечивающая, чтобы апостериорное распределение интегрировалось к единице.\n\n\n\n\n\n\n«Все знания условны. Мы начинаем с того, что знаем (априорное знание), обновляем это знание по мере поступления данных (правдоподобие) и получаем то, что знаем теперь (апостериорное знание)».",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Байесовский подход</span>"
    ]
  },
  {
    "objectID": "chapter28.html#почему-именно-байесовский-подход-в-оценке-роста",
    "href": "chapter28.html#почему-именно-байесовский-подход-в-оценке-роста",
    "title": "29  Байесовский подход",
    "section": "29.2 Почему именно байесовский подход в оценке роста?",
    "text": "29.2 Почему именно байесовский подход в оценке роста?\nУравнение фон Берталанффи — классический инструмент описания роста рыб, но его параметры (L∞ и K) чрезвычайно чувствительны к структуре данных. При малом числе возрастных групп или смещённой выборке (например, только молодые особи) классическая нелинейная регрессия может выдать биологически неправдоподобные оценки: L∞ = 200 см у трески или K = 0.01 у короткоживущей сельди. Байесовский подход решает эту проблему через ограничение пространства поиска с помощью априорных распределений, основанных на биологических знаниях.\nНапример, если эксперт говорит: «У этого вида предельная длина, скорее всего, около 55 см, но может колебаться от 40 до 70 см», мы не игнорируем это как «ненаучное», а записываем в виде нормального распределения L∞ ~ N(55, 10). Это не догма — это рабочая гипотеза, которую данные могут усилить, ослабить или полностью опровергнуть.\n\ncurve(dnorm(x, 55, 10), 10, 100, \n       xlab = \"K\", ylab = \"Плотность\",\n       main = \"Прайер для Linf\")\n\n\n\n\n\n\nЧто даёт байесовский подход на практике?\n\nКоличественная оценка неопределённости.\nВместо одного числа (например, L∞ = 62.2 см) мы получаем полное распределение вероятностей, из которого можно извлечь доверительные интервалы, стандартные ошибки и даже вероятность того, что L∞ &gt; 70 см. Это важно при управлении запасами: если 95% интервал для L∞ включает значения, при которых MSY падает на 30%, это сигнал к осторожности (в моделях оценки Lopt).\nУстойчивость при малых выборках.\nДаже при 4–6 наблюдениях байесовская модель не «ломается», а выдаёт обоснованную оценку, основанную на компромиссе между данными и априором. Это делает её идеальной для редких, труднодоступных или недавно осваиваемых видов.\nПрозрачность допущений.\nВсе исходные предположения — о диапазоне K, форме распределения ошибок, степени корреляции параметров — задаются явно. Это позволяет легко проводить анализ чувствительности: что изменится, если эксперт ошибся на 10%? Как повлияет изменение формы прайера (априора)?\nИнтеграция разных источников информации.\nБайесовский каркас естественно объединяет данные съёмок, промысла, лабораторных экспериментов и литературных обзоров в единую модель. Это особенно ценно в условиях Data-Limited Fisheries, где каждый фрагмент информации на вес золота.\n\nБайес как мост между наукой и управлением\nУправленческие решения в рыболовстве редко принимаются в условиях полной определённости. Регуляторы хотят знать не только «какой ОДУ (общий допустимый улов) установить», но и «с какой вероятностью мы не переловим при таком ОДУ». Байесовский подход отвечает на этот вопрос напрямую: он даёт распределение возможных траекторий запаса, а не одну «среднюю» линию. Это позволяет строить предосторожные стратегии, учитывающие худший, средний и лучший сценарии.\nКроме того, байесовские результаты легко интерпретировать даже для нестатистиков:\n\n«С вероятностью 90% предельная длина находится между 58 и 66 см» — это понятнее, чем «коэффициент значим при p &lt; 0.05».\n\nДисциплина скромности\nБайесовский подход — это не просто набор формул, а интеллектуальная дисциплина. Он заставляет нас признать:\n\nМы не знаем всего.\nНаши данные неполны.\nНаши модели — упрощения реальности.\n\nНо вместо того чтобы скрывать эти ограничения, байесовский анализ делает их частью процесса. Он учит нас думать вероятностно, принимать решения, устойчивые к неопределённости, и честно сообщать о границах нашего знания.\nВ этом практическом занятии мы не просто «прогоним скрипт», а пройдём полный цикл байесовского анализа: от формализации экспертного мнения до визуализации апостериорных распределений и интерпретации результатов в контексте устойчивого управления. Мы увидим, как даже скудные данные, в сочетании с биологическим смыслом, позволяют построить надёжную оценку параметров роста — и, что важнее, понять, насколько этой оценке можно доверять.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Байесовский подход</span>"
    ]
  },
  {
    "objectID": "chapter28.html#что-даёт-байесовский-подход-на-практике",
    "href": "chapter28.html#что-даёт-байесовский-подход-на-практике",
    "title": "29  Байесовский подход",
    "section": "29.3 Что даёт байесовский подход на практике?",
    "text": "29.3 Что даёт байесовский подход на практике?\n\nКоличественная оценка неопределённости.\nВместо одного числа (например, L∞ = 62.2 см) мы получаем полное распределение вероятностей, из которого можно извлечь доверительные интервалы, стандартные ошибки и даже вероятность того, что L∞ &gt; 70 см. Это важно при управлении запасами: если 95% интервал для L∞ включает значения, при которых MSY падает на 30%, это сигнал к осторожности (в моделях оценки Lopt).\nУстойчивость при малых выборках.\nДаже при 4–6 наблюдениях байесовская модель не «ломается», а выдаёт обоснованную оценку, основанную на компромиссе между данными и априором. Это делает её идеальной для редких, труднодоступных или недавно осваиваемых видов.\nПрозрачность допущений.\nВсе исходные предположения — о диапазоне K, форме распределения ошибок, степени корреляции параметров — задаются явно. Это позволяет легко проводить анализ чувствительности: что изменится, если эксперт ошибся на 10%? Как повлияет изменение формы прайера (априора)?\nИнтеграция разных источников информации.\nБайесовский каркас естественно объединяет данные съёмок, промысла, лабораторных экспериментов и литературных обзоров в единую модель. Это особенно ценно в условиях Data-Limited Fisheries, где каждый фрагмент информации на вес золота.\n\n\n29.3.1 Байес как мост между наукой и управлением\nУправленческие решения в рыболовстве редко принимаются в условиях полной определённости. Регуляторы хотят знать не только «какой ОДУ (общий допустимый улов) установить», но и «с какой вероятностью мы не переловим при таком ОДУ». Байесовский подход отвечает на этот вопрос напрямую: он даёт распределение возможных траекторий запаса, а не одну «среднюю» линию. Это позволяет строить предосторожные стратегии, учитывающие худший, средний и лучший сценарии.\nКроме того, байесовские результаты легко интерпретировать даже для нестатистиков:\n\n«С вероятностью 90% предельная длина находится между 58 и 66 см» — это понятнее, чем «коэффициент значим при p &lt; 0.05».",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Байесовский подход</span>"
    ]
  },
  {
    "objectID": "chapter28.html#дисциплина-скромности",
    "href": "chapter28.html#дисциплина-скромности",
    "title": "29  Байесовский подход",
    "section": "29.4 Дисциплина скромности",
    "text": "29.4 Дисциплина скромности\nБайесовский подход — это не просто набор формул, а интеллектуальная дисциплина. Он заставляет нас признать:\n\nМы не знаем всего.\nНаши данные неполны.\nНаши модели — упрощения реальности.\n\nНо вместо того чтобы скрывать эти ограничения, байесовский анализ делает их частью процесса. Он учит нас думать вероятностно, принимать решения, устойчивые к неопределённости, и честно сообщать о границах нашего знания.\nВ этом практическом занятии мы не просто «прогоним скрипт», а пройдём полный цикл байесовского анализа: от формализации экспертного мнения до визуализации апостериорных распределений и интерпретации результатов в контексте устойчивого управления. Мы увидим, как даже скудные данные, в сочетании с биологическим смыслом, позволяют построить надёжную оценку параметров роста — и, что важнее, понять, насколько этой оценке можно доверять.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Байесовский подход</span>"
    ]
  },
  {
    "objectID": "chapter28.html#что-делаем",
    "href": "chapter28.html#что-делаем",
    "title": "29  Байесовский подход",
    "section": "29.3 Что делаем?",
    "text": "29.3 Что делаем?\nСкрипт целиком.\nПрактическое занятие посвящено байесовскому подходу в оценке параметров роста рыбы на примере уравнения фон Берталанффи в условиях ограниченных данных. Основная цель — наглядно продемонстрировать, как априорные знания эксперта, объединенные с небольшим набором наблюдений, формируют уточненное апостериорное знание о биологических параметрах. Биологическая задача заключается в оценке параметров L∞ или Linf (предельная длина) и K (константа роста) для гипотетического вида рыбы при очень скудной выборке измерений длины и возраста. Уравнение роста используется в упрощенной форме без учета t0: Lage = Linf × (1 - exp(-K × age)).\nКлючевая идея байесовского подхода выражается формулой: апостериорное распределение пропорционально правдоподобию данных, умноженному на априорное распределение. Это означает, что мы начинаем не с чистого листа, а с формализованного опыта, который затем корректируется наблюдаемыми данными. Для реализации подхода в R используются пакеты ggplot2 для визуализации, dplyr для манипуляций с данными и MASS для вспомогательных статистических функций.\nНа первом этапе создается искусственная “истинная” популяция с параметрами Linf = 60 см и K = 0.27/год, чтобы иметь эталон для сравнения или чтобы понимать, к чему мы стремимся в оценке. Генерируется полная кривая роста для возрастов от 1 до 15 лет, которая визуализируется как эталонный образец. Затем моделируется реальная ситуация недостатка данных — шесть промеров “реальной” рыбы. Выбираются всего шесть возрастов (1, 2, 4, 6, 10 и 14 лет), для которых генерируются “наблюдаемые” данные с добавлением случайной ошибки измерения (стандартное отклонение 2 см). Визуализация показывает, насколько ограниченные данные не позволяют точно восстановить истинную кривую роста классическими методами.\n\n# ПРАКТИЧЕСКОЕ ЗАНЯТИЕ: БАЙЕСОВСКИЙ ПОДХОД \n# Оценка параметров роста рыбы по уравнению фон Берталанффи при недостатке данных\n\n# Установка и загрузка необходимых пакетов\nrequired_packages &lt;- c(\"ggplot2\", \"dplyr\", \"tidyr\", \"gridExtra\", \"MASS\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gridExtra)\nlibrary(MASS)\n\n# -------------------- ЧАСТЬ 1: ГЕНЕРАЦИЯ ДАННЫХ --------------------\n\nset.seed(175)\n\n# Истинные параметры\ntrue_Linf &lt;- 60.0\ntrue_K &lt;- 0.27\ntrue_sigma &lt;- 6.0\n\n# Уравнение фон Берталанффи\nvon_bertalanffy &lt;- function(age, Linf, K) {\n  Linf * (1 - exp(-K * age))\n}\n\n# Генерация данных\nages_full &lt;- seq(1, 15, length.out = 100)\nlengths_true &lt;- von_bertalanffy(ages_full, true_Linf, true_K)\n\nsample_ages &lt;- c(1, 2, 4, 6, 10, 14)\nsample_lengths &lt;- von_bertalanffy(sample_ages, true_Linf, true_K) + \n  rnorm(length(sample_ages), 0, true_sigma)\n\nfull_data &lt;- data.frame(Age = ages_full, Length = lengths_true)\nobserved_data &lt;- data.frame(Age = sample_ages, Length = sample_lengths)\n\n# -------------------- ЧАСТЬ 2: ВИЗУАЛИЗАЦИЯ ПРОБЛЕМЫ --------------------\n\nplot_data &lt;- ggplot() +\n  geom_line(data = full_data, aes(x = Age, y = Length, color = \"Истинная кривая\"), \n            linewidth = 1.5, alpha = 0.8) +\n  geom_point(data = observed_data, aes(x = Age, y = Length, color = \"Наблюдения\"), \n             size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"Истинная кривая\" = \"#3366CC\", \"Наблюдения\" = \"#FF4444\")) +\n  labs(title = \"ПРОБЛЕМА НЕДОСТАТКА ДАННЫХ\",\n       subtitle = paste(\"Всего\", length(sample_ages), \"наблюдений для оценки кривой роста\"),\n       y = \"Длина тела (см)\", x = \"Возраст (лет)\",\n       color = \"\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(face = \"bold\", size = 16),\n        plot.subtitle = element_text(color = \"gray40\"))\n\nprint(plot_data)\n\n\n\n\n\n\nЭкспертные знания формализуются в виде априорных распределений: для Linf задается нормальное распределение с средним 55 см и стандартным отклонением 10 см, для K — нормальное распределение с средним 0.25/год и стандартным отклонением 0.08/год. Эти прайеры (априоры или priors) отражают знания гидробиолога о похожих видах в данном водоеме или более ранние сообщения о поимках крупных особей этого же вида “бывалыми” ихтиологами. Создается сетка возможных значений параметров (150 × 150 точек), на которой вычисляются плотности априорных распределений. Визуализация включает маргинальные распределения для каждого параметра и совместное распределение, где отмечаются истинные значения параметров и мнение эксперта.\n\n# -------------------- ЧАСТЬ 3: АПРИОРНЫЕ РАСПРЕДЕЛЕНИЯ --------------------\n\n# Знания эксперта-ихтиолога о похожих видах\nexpert_Linf_mean &lt;- 55    # Ожидаемая предельная длина\nexpert_Linf_sd &lt;- 10      # Неопределенность оценки\nexpert_K_mean &lt;- 0.25      # Ожидаемая константа роста  \nexpert_K_sd &lt;- 0.08       # Неопределенность оценки\n\ncat(\"ЭКСПЕРТНЫЕ ЗНАНИЯ ДЛЯ ОБОИХ ПАРАМЕТРОВ:\\n\")\ncat(\"• Linf ~ N(mean =\", expert_Linf_mean, \", sd =\", expert_Linf_sd, \")\\n\")\ncat(\"• K ~ N(mean =\", expert_K_mean, \", sd =\", expert_K_sd, \")\\n\")\ncat(\"• Истинные значения: Linf =\", true_Linf, \", K =\", true_K, \"\\n\\n\")\n\n# Создаем сетку параметров\nparam_grid &lt;- expand.grid(\n  Linf = seq(40, 70, length.out = 150),\n  K = seq(0.2, 0.4, length.out = 150)\n)\n\n# Априорные распределения (нормальные)\nparam_grid$prior &lt;- dnorm(param_grid$Linf, expert_Linf_mean, expert_Linf_sd) *\n                   dnorm(param_grid$K, expert_K_mean, expert_K_sd)\nparam_grid$prior &lt;- param_grid$prior / sum(param_grid$prior)\n\n# -------------------- ЧАСТЬ 4: ВИЗУАЛИЗАЦИЯ АПРИОРОВ --------------------\n\n# Маргинальные распределения - Linf\nprior_linf_plot &lt;- ggplot(data.frame(x = seq(40, 70, length.out = 100)), aes(x = x)) +\n  geom_area(aes(y = dnorm(x, expert_Linf_mean, expert_Linf_sd)), \n            fill = \"#3366CC\", alpha = 0.6) +\n  geom_line(aes(y = dnorm(x, expert_Linf_mean, expert_Linf_sd)), \n            color = \"#3366CC\", linewidth = 1.2) +\n  geom_vline(xintercept = true_Linf, linetype = \"dashed\", color = \"#FF4444\", linewidth = 1) +\n  geom_vline(xintercept = expert_Linf_mean, linetype = \"dashed\", color = \"#3366CC\", linewidth = 1) +\n  annotate(\"text\", x = true_Linf, y = 0.03, label = \"Истинное значение\", \n           hjust = -0.1, color = \"#FF4444\", size = 3) +\n  annotate(\"text\", x = expert_Linf_mean, y = 0.025, label = \"Мнение эксперта\", \n           hjust = 1.1, color = \"#3366CC\", size = 3) +\n  labs(title = \"АПРИОРНОЕ РАСПРЕДЕЛЕНИЕ: Linf\",\n       subtitle = \"Предельная длина тела\",\n       y = \"Плотность вероятности\", x = \"Linf (см)\") +\n  theme_minimal()\n\n# Маргинальные распределения - K (ИСПРАВЛЕНО: добавлена линия мнения эксперта)\nprior_k_plot &lt;- ggplot(data.frame(x = seq(0.2, 0.4, length.out = 100)), aes(x = x)) +\n  geom_area(aes(y = dnorm(x, expert_K_mean, expert_K_sd)), \n            fill = \"#3366CC\", alpha = 0.6) +\n  geom_line(aes(y = dnorm(x, expert_K_mean, expert_K_sd)), \n            color = \"#3366CC\", linewidth = 1.2) +\n  geom_vline(xintercept = true_K, linetype = \"dashed\", color = \"#FF4444\", linewidth = 1) +\n  geom_vline(xintercept = expert_K_mean, linetype = \"dashed\", color = \"#3366CC\", linewidth = 1) +  # ДОБАВЛЕНО\n  annotate(\"text\", x = true_K, y = 4.5, label = \"Истинное значение\", \n           hjust = -0.1, color = \"#FF4444\", size = 3) +\n  annotate(\"text\", x = expert_K_mean, y = 4.0, label = \"Мнение эксперта\", \n           hjust = 1.1, color = \"#3366CC\", size = 3) +  # ДОБАВЛЕНО\n  labs(title = \"АПРИОРНОЕ РАСПРЕДЕЛЕНИЕ: K\",\n       subtitle = \"Константа роста\",\n       y = \"Плотность вероятности\", x = \"K (1/год)\") +\n  theme_minimal()\n\n# Совместное распределение\njoint_prior_plot &lt;- ggplot(param_grid, aes(x = Linf, y = K)) +\n  geom_raster(aes(fill = prior), interpolate = TRUE) +\n  geom_contour(aes(z = prior), color = \"white\", alpha = 0.5, linewidth = 0.3) +\n  annotate(\"point\", x = true_Linf, y = true_K, color = \"#FF4444\", size = 3, shape = 4) +\n  annotate(\"point\", x = expert_Linf_mean, y = expert_K_mean, color = \"#3366CC\", size = 3, shape = 16) +\n  scale_fill_gradient(low = \"#E6F2FF\", high = \"#003366\", name = \"Плотность\") +\n  labs(title = \"СОВМЕСТНОЕ АПРИОРНОЕ РАСПРЕДЕЛЕНИЕ\",\n       subtitle = \"Синий круг - мнение эксперта, Красный крест - истинные значения\",\n       y = \"K (1/год)\", x = \"Linf (см)\") +\n  theme_minimal()\n\ngrid.arrange(prior_linf_plot, prior_k_plot, joint_prior_plot, \n             layout_matrix = rbind(c(1, 2), c(3, 3)),\n             top = \"БАЙЕСОВСКИЙ ПОДХОД: АПРИОРНЫЕ ЗНАНИЯ ЭКСПЕРТА ДЛЯ ОБОИХ ПАРАМЕТРОВ\")\n\n\n\n\n\n\nФункция правдоподобия вычисляется на основе нормального распределения ошибок наблюдений с стандартным отклонением 2.5. Для каждой комбинации параметров сетки рассчитывается логарифм правдоподобия, который затем преобразуется в шкалу вероятностей. Визуализация правдоподобия показывает, какие комбинации параметров лучше всего объясняют наблюдаемые данные. Важно отметить, что из-за малого количества данных функция правдоподобия довольно широкая, что отражает высокую неопределенность оценок.\n\n# -------------------- ЧАСТЬ 5: ФУНКЦИЯ ПРАВДОПОДОБИЯ --------------------\n\ncalculate_likelihood &lt;- function(Linf, K, sigma) {\n  predicted &lt;- von_bertalanffy(sample_ages, Linf, K)\n  sum(dnorm(sample_lengths, predicted, sigma, log = TRUE))\n}\n\nsigma_prior &lt;- 2.5\nparam_grid$log_likelihood &lt;- apply(param_grid, 1, function(row) {\n  calculate_likelihood(row[\"Linf\"], row[\"K\"], sigma_prior)\n})\n\n# Преобразуем в likelihood\nparam_grid$likelihood &lt;- exp(param_grid$log_likelihood - max(param_grid$log_likelihood))\nparam_grid$likelihood &lt;- param_grid$likelihood / sum(param_grid$likelihood, na.rm = TRUE)\n\n# Визуализация правдоподобия\nlikelihood_plot &lt;- ggplot(param_grid, aes(x = Linf, y = K)) +\n  geom_raster(aes(fill = likelihood), interpolate = TRUE) +\n  geom_contour(aes(z = likelihood), color = \"white\", alpha = 0.6, linewidth = 0.4) +\n  annotate(\"point\", x = true_Linf, y = true_K, color = \"#FF4444\", size = 3, shape = 4) +\n  annotate(\"point\", x = expert_Linf_mean, y = expert_K_mean, color = \"#3366CC\", size = 3, shape = 16) +  # ДОБАВЛЕНО\n  scale_fill_gradient(low = \"#E6FFE6\", high = \"#006600\", name = \"Правдоподобие\") +\n  labs(title = \"ФУНКЦИЯ ПРАВДОПОДОБИЯ\",\n       subtitle = \"Вероятность наблюдать данные при заданных параметрах\\nКрасный крест - истинные значения, Синий круг - мнение эксперта\",\n       y = \"K (1/год)\", x = \"Linf (см)\") +\n  theme_minimal()\n\nprint(likelihood_plot)\n\n\n\n\n\n\nАпостериорное распределение вычисляется как произведение априорного распределения и правдоподобия с последующей нормализацией. Визуализация апостериорного распределения демонстрирует, как сочетание априорных знаний и данных сужает область вероятных значений параметров. Маргинализация позволяет получить распределения для каждого параметра в отдельности, на которых хорошо видно смещение оценок в сторону как априорных ожиданий, так и наблюдаемых данных.\n\n# -------------------- ЧАСТЬ 6: АПОСТЕРИОРНОЕ РАСПРЕДЕЛЕНИЕ --------------------\n\n# Удаляем NA значения\nparam_grid &lt;- param_grid[complete.cases(param_grid), ]\n\n# Вычисляем апостериорное распределение\nparam_grid$posterior &lt;- param_grid$prior * param_grid$likelihood\nparam_grid$posterior &lt;- param_grid$posterior / sum(param_grid$posterior, na.rm = TRUE)\n\n# Визуализация апостериорного распределения\nposterior_plot &lt;- ggplot(param_grid, aes(x = Linf, y = K)) +\n  geom_raster(aes(fill = posterior), interpolate = TRUE) +\n  geom_contour(aes(z = posterior), color = \"white\", alpha = 0.6, linewidth = 0.4) +\n  annotate(\"point\", x = true_Linf, y = true_K, color = \"#FF4444\", size = 3, shape = 4) +\n  scale_fill_gradient(low = \"#FFE6E6\", high = \"#CC0000\", name = \"Плотность\") +\n  labs(title = \"АПОСТЕРИОРНОЕ РАСПРЕДЕЛЕНИЕ\",\n       subtitle = \"Обновленные знания после учета данных\",\n       y = \"K (1/год)\", x = \"Linf (см)\") +\n  theme_minimal()\n\nprint(posterior_plot)\n\n\n\n\n\n\nРассчитываются точечные оценки параметров как средние апостериорные значения: Linf = 62.2 см (истинное значение 60 см), K = 0.263/год (истинное значение 0.27/год). Стандартные отклонения апостериорных распределений составляют 2.3 см для Linf и 0.026/год для K. Методом сэмплирования строятся 95% доверительные интервалы: для Linf от 57.9 до 67.0 см, для K от 0.215 до 0.317/год. Эти интервалы количественно характеризуют неопределенность оценок.\n\n# -------------------- ЧАСТЬ 7: МАРГИНАЛЬНЫЕ РАСПРЕДЕЛЕНИЯ --------------------\n\n# Маргинализация\nmarginal_linf &lt;- param_grid %&gt;%\n  group_by(Linf) %&gt;%\n  summarise(prior = sum(prior),\n            posterior = sum(posterior))\n\nmarginal_k &lt;- param_grid %&gt;%\n  group_by(K) %&gt;%\n  summarise(prior = sum(prior),\n            posterior = sum(posterior))\n\n# Графики маргинальных распределений\nmarginal_linf_plot &lt;- ggplot(marginal_linf) +\n  geom_area(aes(x = Linf, y = prior, fill = \"Априорное\"), alpha = 0.6) +\n  geom_line(aes(x = Linf, y = prior, color = \"Априорное\"), linewidth = 1.2) +\n  geom_area(aes(x = Linf, y = posterior, fill = \"Апостериорное\"), alpha = 0.6) +\n  geom_line(aes(x = Linf, y = posterior, color = \"Апостериорное\"), linewidth = 1.2) +\n  geom_vline(xintercept = true_Linf, linetype = \"dashed\", color = \"#FF4444\", linewidth = 1) +\n  scale_fill_manual(values = c(\"Априорное\" = \"#3366CC\", \"Апостериорное\" = \"#CC0000\"), \n                    name = \"Распределение\") +\n  scale_color_manual(values = c(\"Априорное\" = \"#3366CC\", \"Апостериорное\" = \"#CC0000\"), \n                     name = \"Распределение\") +\n  labs(title = \"МАРГИНАЛЬНОЕ РАСПРЕДЕЛЕНИЕ: Linf\",\n       y = \"Плотность вероятности\", x = \"Linf (см)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Скрываем легенду у первого графика\n\nmarginal_k_plot &lt;- ggplot(marginal_k) +\n  geom_area(aes(x = K, y = prior, fill = \"Априорное\"), alpha = 0.6) +\n  geom_line(aes(x = K, y = prior, color = \"Априорное\"), linewidth = 1.2) +\n  geom_area(aes(x = K, y = posterior, fill = \"Апостериорное\"), alpha = 0.6) +\n  geom_line(aes(x = K, y = posterior, color = \"Апостериорное\"), linewidth = 1.2) +\n  geom_vline(xintercept = true_K, linetype = \"dashed\", color = \"#FF4444\", linewidth = 1) +\n  scale_fill_manual(values = c(\"Априорное\" = \"#3366CC\", \"Апостериорное\" = \"#CC0000\"), \n                    name = \"Распределение\") +\n  scale_color_manual(values = c(\"Априорное\" = \"#3366CC\", \"Апостериорное\" = \"#CC0000\"), \n                     name = \"Распределение\") +\n  labs(title = \"МАРГИНАЛЬНОЕ РАСПРЕДЕЛЕНИЕ: K\",\n       y = \"Плотность вероятности\", x = \"K (1/год)\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")  # Размещаем легенду внизу у второго графика\n\n# Объединяем графики\ngrid.arrange(marginal_linf_plot, marginal_k_plot, ncol = 2)\n\n\n\n\n\n\nФинальная визуализация показывает 150 случайных кривых роста, сгенерированных из апостериорного распределения. Тонкие красные линии иллюстрируют неопределенность оценки, темно-красная линия показывает среднюю апостериорную кривую, синяя линия — истинную кривую, а зеленые точки — наблюдаемые данные. Такой подход наглядно демонстрирует байесовскую философию: мы получаем не единственную “лучшую” кривую, а целое семейство возможных кривых с соответствующими вероятностями, что позволяет адекватно оценивать неопределенность прогнозов.\n\n# -------------------- ЧАСТЬ 8: РАСЧЕТ СТАТИСТИК --------------------\n\n# Точечные оценки\nposterior_mean_linf &lt;- sum(param_grid$Linf * param_grid$posterior, na.rm = TRUE)\nposterior_mean_k &lt;- sum(param_grid$K * param_grid$posterior, na.rm = TRUE)\n\n# Стандартные отклонения\nposterior_sd_linf &lt;- sqrt(sum(param_grid$posterior * (param_grid$Linf - posterior_mean_linf)^2, na.rm = TRUE))\nposterior_sd_k &lt;- sqrt(sum(param_grid$posterior * (param_grid$K - posterior_mean_k)^2, na.rm = TRUE))\n\n# 95% доверительные интервалы\nlinf_samples &lt;- sample(param_grid$Linf, size = 10000, replace = TRUE, prob = param_grid$posterior)\nk_samples &lt;- sample(param_grid$K, size = 10000, replace = TRUE, prob = param_grid$posterior)\n\nlinf_quantiles &lt;- quantile(linf_samples, probs = c(0.025, 0.975))\nk_quantiles &lt;- quantile(k_samples, probs = c(0.025, 0.975))\n\n# Вывод результатов\ncat(\"РЕЗУЛЬТАТЫ БАЙЕСОВСКОГО АНАЛИЗА:\\n\")\ncat(\"================================\\n\\n\")\n\ncat(\"ТОЧЕЧНЫЕ ОЦЕНКИ:\\n\")\ncat(\"• Linf:\", round(posterior_mean_linf, 1), \"см (истинное:\", true_Linf, \"см)\\n\")\ncat(\"• K:\", round(posterior_mean_k, 3), \"/год (истинное:\", true_K, \"/год)\\n\\n\")\n\ncat(\"НЕОПРЕДЕЛЕННОСТЬ ОЦЕНОК:\\n\")\ncat(\"• SD(Linf):\", round(posterior_sd_linf, 1), \"см\\n\")\ncat(\"• SD(K):\", round(posterior_sd_k, 3), \"/год\\n\\n\")\n\ncat(\"95% ДОВЕРИТЕЛЬНЫЕ ИНТЕРВАЛЫ:\\n\")\ncat(\"• Linf: [\", round(linf_quantiles[1], 1), \",\", round(linf_quantiles[2], 1), \"] см\\n\")\ncat(\"• K: [\", round(k_quantiles[1], 3), \",\", round(k_quantiles[2], 3), \"] /год\\n\\n\")\n\n# -------------------- ЧАСТЬ 9: ФИНАЛЬНАЯ ВИЗУАЛИЗАЦИЯ --------------------\n\n# Генерируем кривые роста на основе апостериорного распределения\nset.seed(456)\nn_curves &lt;- 150\nsample_indices &lt;- sample(1:nrow(param_grid), n_curves, prob = param_grid$posterior)\n\ncurve_data &lt;- data.frame()\nfor (i in 1:n_curves) {\n  idx &lt;- sample_indices[i]\n  curve &lt;- data.frame(\n    Age = ages_full,\n    Length = von_bertalanffy(ages_full, param_grid$Linf[idx], param_grid$K[idx]),\n    Curve = i\n  )\n  curve_data &lt;- rbind(curve_data, curve)\n}\n\nfinal_plot &lt;- ggplot() +\n  geom_line(data = curve_data, aes(x = Age, y = Length, group = Curve), \n            color = \"#FF6666\", alpha = 0.08, linewidth = 0.3) +\n  geom_line(data = full_data, aes(x = Age, y = Length, color = \"Истинная кривая\"), \n            linewidth = 2, alpha = 0.9) +\n  geom_point(data = observed_data, aes(x = Age, y = Length, color = \"Наблюдения\"), \n             size = 3, alpha = 0.9) +\n  stat_summary(data = curve_data, aes(x = Age, y = Length), \n               fun = mean, geom = \"line\", color = \"#CC0000\", linewidth = 1.5) +\n  scale_color_manual(values = c(\"Истинная кривая\" = \"#3366CC\", \"Наблюдения\" = \"#00AA00\")) +\n  labs(title = \"БАЙЕСОВСКАЯ ОЦЕНКА КРИВОЙ РОСТА\",\n       subtitle = paste(\"Красные линии - возможные кривые (n =\", n_curves, \")\\nТемно-красная - средняя апостериорная оценка\"),\n       y = \"Длина тела (см)\", x = \"Возраст (лет)\",\n       color = \"\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(face = \"bold\", size = 16),\n        plot.subtitle = element_text(color = \"gray40\"))\n\nprint(final_plot)\n\n\n\n\n\n\nЗаключение подчеркивает преимущества байесовского подхода: эффективное сочетание экспертных знаний с ограниченными данными, количественную оценку неопределенности и интерпретируемость результатов в терминах вероятностей. В условиях малых выборок, типичных для гидробиологических исследований, такой подход позволяет получать более реалистичные и надежные оценки параметров роста рыб.\n\ncat(\"\\nЗАКЛЮЧЕНИЕ:\\n\")\ncat(\"==========\\n\")\ncat(\"Байесовский подход позволяет эффективно сочетать:\\n\")\ncat(\"• Экспертные знания (априорные распределения)\\n\")\ncat(\"• Ограниченные данные наблюдений (правдоподобие)\\n\")\ncat(\"• Количественную оценку неопределенности\\n\")\ncat(\"• Интерпретируемые результаты в терминах вероятностей\\n\\n\")",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Байесовский подход</span>"
    ]
  },
  {
    "objectID": "chapter29.html",
    "href": "chapter29.html",
    "title": "30  Фильтр Калмана",
    "section": "",
    "text": "30.1 Введение\nЭто занятие — про то, как перестать путать «то, что мы меряем», с «тем, что в системе происходит на самом деле». В промысловой статистике CPUE — это не запас, а мутный его отпечаток: одновременно в нём “сидят” и изъятие, и миграции, и случайные колебания наблюдений. Фильтр Калмана — это рекурсивный метод оценивания скрытых состояний динамической системы, в которой сама динамика и измерения зашумлены. Он раздельно учитывает две неопределённости — в процессах и в наблюдениях, на каждом шаге строит прогноз состояния, сравнивает его с новым фактом и корректирует оценку, оптимально взвешивая доверие к модели и данным. В гидробиологии и рыбохозяйственной науке эта логика особенно полезна там, где прямых наблюдений запаса нет, а есть индексы и оперативная промысловая информация: по временным рядам CPUE и выловов можно последовательно восстанавливать траектории численности/биомассы, пополнения, миграции и улавливаемости; объединять несколько источников данных с разной точностью; устойчиво проходить через пропуски и нерегулярные наблюдения; получать не только оценки, но и их дисперсии и доверительные интервалы. Именно поэтому фильтр Калмана может стать рабочей лошадкой для задач локальных открытых запасов, анализа истощения, последовательных моделей по возрасту/длине, а также структурных временных рядов, где есть медленно меняющиеся «скрытые» факторы продуктивности.\nКлючевое препятствие классическим регрессионным моделям истощения в реальных промысловых ситуациях — открытость локальных запасов и шумность индексов. Когда через границы полигона идут разнонаправленные миграции, а наблюдения CPUE подвержены дополнительной вариабельности, наклон «линии Лесли» становится неустойчивым и плохо интерпретируемым, а в формулировке Де Лури добавляется ещё и методическая смещённость из‑за лог-линеаризации. В такой постановке нельзя честно отделить влияние вылова от неучтённых факторов и ошибок измерений.\nЭту методическую развилку аккуратно разобрал Александр Аркадьевич Михеев в работе «Приложение модели открытой эксплуатируемой популяции к оценке локальных запасов» (Труды ВНИРО, 2014, т. 151: 95–108). Он показал, что регрессионные методы не позволяют корректно оценивать открытый локальный запас, и предложил обобщённую модель Лесли с фильтром Калмана (ОМЛ ФК) как диагностическую модель для периода промысла.\nСуть обобщения проста и физически прозрачна. Запас \\(N_t\\) эволюционирует согласно балансовому уравнению:\n\\[\nN_t = N_{t-1} - C_t + m_{t-1} + \\varepsilon_t,\n\\]\nгде: - \\(C_t\\) — известный вылов в момент времени \\(t\\), - \\(m_t\\) — скрытый результирующий приток/отток (включающий миграции, неучтённый вылов и прочие неизмеряемые эффекты), - \\(\\varepsilon_t\\) — случайная ошибка процесса.\nДинамика скрытого компонента \\(m_t\\) задаётся как процесс с затухающей памятью:\n\\[\nm_t = a\\, m_{t-1} + \\delta_t, \\qquad |a| &lt; 1,\n\\]\nгде \\(a\\) — коэффициент автокорреляции, а \\(\\delta_t\\) — белый шум.\nНаблюдения представляют собой зашумлённый индекс биомассы:\n\\[\ny_t = q\\, N_t + w_t,\n\\]\nгде \\(q\\) — коэффициент улавливаемости (catchability), а \\(w_t\\) — шум измерения.\nВ формализме пространства состояний такая модель естественным образом обрабатывается фильтром Калмана: на каждом шаге последовательно вычисляются: - инновации (ошибки прогноза), - их дисперсии, - коэффициент усиления (Kalman gain).\nПараметры системы (\\(a\\), \\(q\\)) и характеристики шумов (\\(\\mathrm{Var}(\\varepsilon_t)\\), \\(\\mathrm{Var}(\\delta_t)\\), \\(\\mathrm{Var}(w_t)\\)) оцениваются методом максимального правдоподобия, используя разложение логарифмической функции правдоподобия по ошибкам предсказания.\nОтдельно отметим вклад Александра Аркадьевича: он показал, что попытка оценить вклад вылова и неучтённых факторов с помощью множественной регрессии на накопленные величины \\(K_t = \\sum_{s=1}^t C_s\\) и \\(M_t = \\sum_{s=1}^t m_s\\) обречена на неудачу из‑за сильной мультиколлинеарности между регрессорами. Введение скрытого состояния \\(m_t\\) позволяет корректно разделить влияние управляемого вылова \\(C_t\\) и неуправляемых экзогенных факторов, обеспечивая как идентифицируемость, так и интерпретируемость модели.\nПрактически важная часть его работы — обработка нерегулярных промысловых рядов: регуляризация времени через весовые матрицы для ежедневной шкалы, что позволяет фильтру корректно работать с «рваными» данными ловушечных промыслов. В приложении автор выводит уравнения фильтра для конкретной модели, даёт интерпретацию оценок в шкале наблюдений через статистические веса, связывает их с усилением Калмана и доказывает теорему об эффективности оценок для ОМЛ ФК. На эмпирических примерах из Сахалино‑Курильского региона он демонстрирует, что ОМЛ ФК возвращает более узкие и менее смещённые оценки по сравнению с классической Лесли, устойчиво работает при иммиграции/эмиграции и позволяет явно оценивать величину «скрытого» потока и погрешность наблюдений; именно эта модель легла в основу метода полигонов при подготовке оперативных прогнозов для беспозвоночных. В рамках нашей практики мы будем исходить из этой логики: трактовать CPUE как шумный отпечаток скрытого состояния, отделять процесс от измерения, последовательно обновлять знание о запасе и миграции и опираться на проверяемые статистические предпосылки.",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Фильтр Калмана</span>"
    ]
  },
  {
    "objectID": "chapter29.html#введение",
    "href": "chapter29.html#введение",
    "title": "30  Байесовский подход",
    "section": "",
    "text": "\\(P(\\theta)\\) — априорная вероятность, отражающая наши знания или убеждения о параметре \\(\\theta\\) до наблюдения данных;\n\\(P(D \\mid \\theta)\\) — правдоподобие, то есть вероятность наблюдать данные \\(D\\) при заданном значении параметра \\(\\theta\\);\n\\(P(\\theta \\mid D)\\) — апостериорная вероятность, представляющая обновлённые знания о \\(\\theta\\) после учёта данных \\(D\\);\n\\(P(D)\\) — нормировочная константа (маргинальное правдоподобие), обеспечивающая, чтобы апостериорное распределение интегрировалось к единице.\n\n\n\n\n\n\n«Все знания условны. Мы начинаем с того, что знаем (априорное знание), обновляем это знание по мере поступления данных (правдоподобие) и получаем то, что знаем теперь (апостериорное знание)».",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Байесовский подход</span>"
    ]
  },
  {
    "objectID": "chapter29.html#почему-именно-байесовский-подход-в-оценке-роста",
    "href": "chapter29.html#почему-именно-байесовский-подход-в-оценке-роста",
    "title": "30  Байесовский подход",
    "section": "30.2 Почему именно байесовский подход в оценке роста?",
    "text": "30.2 Почему именно байесовский подход в оценке роста?\nУравнение фон Берталанффи — классический инструмент описания роста рыб, но его параметры (L∞ и K) чрезвычайно чувствительны к структуре данных. При малом числе возрастных групп или смещённой выборке (например, только молодые особи) классическая нелинейная регрессия может выдать биологически неправдоподобные оценки: L∞ = 200 см у трески или K = 0.01 у короткоживущей сельди. Байесовский подход решает эту проблему через ограничение пространства поиска с помощью априорных распределений, основанных на биологических знаниях.\nНапример, если эксперт говорит: «У этого вида предельная длина, скорее всего, около 55 см, но может колебаться от 40 до 70 см», мы не игнорируем это как «ненаучное», а записываем в виде нормального распределения L∞ ~ N(55, 10). Это не догма — это рабочая гипотеза, которую данные могут усилить, ослабить или полностью опровергнуть.\n\ncurve(dnorm(x, 55, 10), 10, 100, \n       xlab = \"K\", ylab = \"Плотность\",\n       main = \"Прайер для Linf\")\n\n\n\n\n\n\nЧто даёт байесовский подход на практике?\n\nКоличественная оценка неопределённости.\nВместо одного числа (например, L∞ = 62.2 см) мы получаем полное распределение вероятностей, из которого можно извлечь доверительные интервалы, стандартные ошибки и даже вероятность того, что L∞ &gt; 70 см. Это важно при управлении запасами: если 95% интервал для L∞ включает значения, при которых MSY падает на 30%, это сигнал к осторожности (в моделях оценки Lopt).\nУстойчивость при малых выборках.\nДаже при 4–6 наблюдениях байесовская модель не «ломается», а выдаёт обоснованную оценку, основанную на компромиссе между данными и априором. Это делает её идеальной для редких, труднодоступных или недавно осваиваемых видов.\nПрозрачность допущений.\nВсе исходные предположения — о диапазоне K, форме распределения ошибок, степени корреляции параметров — задаются явно. Это позволяет легко проводить анализ чувствительности: что изменится, если эксперт ошибся на 10%? Как повлияет изменение формы прайера (априора)?\nИнтеграция разных источников информации.\nБайесовский каркас естественно объединяет данные съёмок, промысла, лабораторных экспериментов и литературных обзоров в единую модель. Это особенно ценно в условиях Data-Limited Fisheries, где каждый фрагмент информации на вес золота.\n\nБайес как мост между наукой и управлением\nУправленческие решения в рыболовстве редко принимаются в условиях полной определённости. Регуляторы хотят знать не только «какой ОДУ (общий допустимый улов) установить», но и «с какой вероятностью мы не переловим при таком ОДУ». Байесовский подход отвечает на этот вопрос напрямую: он даёт распределение возможных траекторий запаса, а не одну «среднюю» линию. Это позволяет строить предосторожные стратегии, учитывающие худший, средний и лучший сценарии.\nКроме того, байесовские результаты легко интерпретировать даже для нестатистиков:\n\n«С вероятностью 90% предельная длина находится между 58 и 66 см» — это понятнее, чем «коэффициент значим при p &lt; 0.05».\n\nДисциплина скромности\nБайесовский подход — это не просто набор формул, а интеллектуальная дисциплина. Он заставляет нас признать:\n\nМы не знаем всего.\nНаши данные неполны.\nНаши модели — упрощения реальности.\n\nНо вместо того чтобы скрывать эти ограничения, байесовский анализ делает их частью процесса. Он учит нас думать вероятностно, принимать решения, устойчивые к неопределённости, и честно сообщать о границах нашего знания.\nВ этом практическом занятии мы не просто «прогоним скрипт», а пройдём полный цикл байесовского анализа: от формализации экспертного мнения до визуализации апостериорных распределений и интерпретации результатов в контексте устойчивого управления. Мы увидим, как даже скудные данные, в сочетании с биологическим смыслом, позволяют построить надёжную оценку параметров роста — и, что важнее, понять, насколько этой оценке можно доверять.",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Байесовский подход</span>"
    ]
  },
  {
    "objectID": "chapter29.html#что-делаем",
    "href": "chapter29.html#что-делаем",
    "title": "30  Фильтр Калмана",
    "section": "30.2 Что делаем?",
    "text": "30.2 Что делаем?\nСкрипт целиком.\nМы идём от генерации учебных данных к классической регрессии Лесли и далее к фильтру Калмана в постановке обобщённой модели Лесли, последовательно рассчитываем оценки состояний и сравниваем подходы на одних и тех же рядах. Сначала подготавливаем окружение и подключаем пакеты для визуализации и манипуляций с данными. Далее создаём синтетический «мир», в котором известна истинная траектория запаса, чтобы наглядно видеть, что из неё восстанавливают разные методы. Для этого фиксируем генератор случайных чисел set.seed(123) ради воспроизводимости и задаём временную шкалу из 20 последовательных дней. Истинный запас формируем как комбинацию экспоненциального спада от стартового уровня 1000 с коэффициентом 0.08 в день, добавляем шаблон миграционных всплесков и оттоков и добавляем белый шум rnorm с дисперсией 25^2, символизирующий непросчитанные факторы процесса. Миграционный шаблон задан вектором, в котором сначала идёт спокойный период, затем три положительных всплеска, снова спокойный отрезок и три отрицательных импульса, затем ещё несколько нулей. Обратите внимание: в примере длина migration_pattern равна 19 при 20 днях, поэтому R предупреждает о неполном кратном «переразмножении» вектора и рециклирует его, что порождает предупреждение; чтобы его избежать, длину шаблона следует сделать ровно 20, добавив один ноль в конец. На основе истинного запаса формируем наблюдаемые промысловые ряды. Суточный вылов задаём как 2–5% от текущего запаса: для каждого дня генерируется коэффициент из равномерного распределения runif(0.02, 0.05), перемножается на true_stock, результат округляется и обрезается снизу pmax(…, 0), чтобы исключить отрицательные величины. CPUE, наш «индекс обилия», создаём как линейную функция от запаса с коэффициентом улавливаемости 0.002 и добавочным шумом измерений rnorm(sd = 0.1); это важно, потому что дальше в фильтре Калмана мы будем использовать тот же q, и таким образом обеспечиваем согласование шкал. Сразу выводим простейшие описательные характеристики: средний CPUE, суммарный вылов и диапазон суточного вылова, чтобы понимать порядок величин. Визуализируем исходные данные на одном графике с двумя шкалами: столбцами показываем суточный вылов, масштабированный делением на 50 для совместимости по оси Y с CPUE, точки и пунктирная линия — это наблюдения CPUE. Вторичная ось возвращает масштаб выловов умножением на 50, подписи и аккуратная тема помогают мгновенно считывать структуру рядов.\n\n# ПРАКТИКУМ: ФИЛЬТР КАЛМАНА ДЛЯ ОЦЕНКИ ЗАПАСА ГИДРОБИОНТОВ\n# =====================================================================================================================\n# Автор: Баканев С.В. Для курса \"Оценка водных биоресурсов при недостатке данных в среде R (для начинающих)\"\n# Цель: Показать преимущества фильтра Калмана перед классическими методами\n# =====================================================================================================================\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# ЧАСТЬ 1: ГЕНЕРАЦИЯ И ВИЗУАЛИЗАЦИЯ СИНТЕТИЧЕСКИХ ДАННЫХ\n# ======================================================\n# В реальных условиях мы не знаем истинного состояния запаса,\n# но для демонстрации методов создадим синтетические данные\n\ncat(\"=== ЧАСТЬ 1: ГЕНЕРАЦИЯ И ВИЗУАЛИЗАЦИЯ ДАННЫХ ===\\n\\n\")\n\n# Устанавливаем seed для воспроизводимости результатов\nset.seed(123)\ndays &lt;- 1:20  # 20 дней наблюдений\n\n# СОЗДАЕМ РЕАЛИСТИЧНУЮ ДИНАМИКУ ЗАПАСА:\n# 1. Экспоненциальное снижение (естественная смертность + вылов)\n# 2. Миграционные всплески (приток/отток особей)\n# 3. Случайные колебания (неучтенные факторы)\n\ncat(\"Создание модели истинного запаса...\\n\")\nmigration_pattern &lt;- c(rep(0,5), 50, 80, 30, rep(0,5), -40, -60, -20, rep(0,3))\ntrue_stock &lt;- 1000 * exp(-0.08 * days) + migration_pattern + rnorm(20, sd = 25)\n\ncat(\"Генерация данных промысла...\\n\")\n# Создаем dataframe с данными наблюдений\ndata &lt;- data.frame(\n  Day = days,\n  # Суточный вылов (2-5% от текущего запаса)\n  Catch = pmax(round(true_stock * runif(20, 0.02, 0.05)), 0),\n  # CPUE (Catch Per Unit Effort) - основной наблюдаемый показатель\n  # В реальности зависит от множества факторов, здесь - линейная зависимость + шум\n  CPUE = true_stock * 0.002 + rnorm(20, sd = 0.1)\n)\n\n# БАЗОВАЯ СТАТИСТИКА ДАННЫХ\ncat(\"Средний улов на усилие (CPUE):\", round(mean(data$CPUE), 3), \"\\n\")\ncat(\"Общий вылов за период:\", sum(data$Catch), \"условных единиц\\n\")\ncat(\"Максимальный суточный вылов:\", max(data$Catch), \"\\n\")\ncat(\"Минимальный суточный вылов:\", min(data$Catch), \"\\n\\n\")\n\n# ВИЗУАЛИЗАЦИЯ ИСХОДНЫХ ДАННЫХ\ncat(\"Построение графика исходных данных...\\n\")\nplot_raw_data &lt;- ggplot(data, aes(x = Day)) +\n  # Столбцы - суточный вылов (масштабируем для совмещения с CPUE)\n  geom_col(aes(y = Catch/50), fill = \"lightblue\", alpha = 0.7, width = 0.7) +\n  # Точки и линия - наблюдения CPUE\n  geom_point(aes(y = CPUE), color = \"darkred\", size = 3) +\n  geom_line(aes(y = CPUE), color = \"darkred\", linetype = \"dashed\", alpha = 0.7) +\n  # Две оси Y для разных масштабов\n  scale_y_continuous(\n    name = \"Улов на усилие (CPUE)\",\n    sec.axis = sec_axis(~ . * 50, name = \"Суточный улов (особи)\")\n  ) +\n  labs(title = \"Исходные данные промысла\",\n       subtitle = \"Красные точки/линия - CPUE (индекс обилия), синие столбцы - уловы\",\n       caption = \"CPUE используется как прокси-переменная для оценки запаса\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\nprint(plot_raw_data)\n\n\n\n\n\n\nДалее демонстрируем классический «линейный» взгляд на истощение через модель Лесли в её регрессионной версии на время: строим прямую CPUE ~ Day с помощью lm, сохраняем предсказания в таблицу и печатаем ключевые статистики — наклон, R^2 и p-значение. Такая формулировка намеренно упрощённая по сравнению с канонической регрессией на накопленный вылов, но она хорошо иллюстрирует идею «одной линии по шумным точкам», о которой говорилось во введении. Строим отдельный график, где на исходные точки CPUE накладываем линию предсказаний регрессии и её стандартную полосу через geom_smooth(method = “lm”), чтобы визуально увидеть, как прямая «усредняет» колебания.\n\n# ЧАСТЬ 2: КЛАССИЧЕСКАЯ МОДЕЛЬ ЛЕСЛИ\n# ===================================\n# Модель Лесли - традиционный подход для оценки динамики популяций\n# Предполагает линейное изменение CPUE во времени\n\ncat(\"\\n=== ЧАСТЬ 2: КЛАССИЧЕСКАЯ МОДЕЛЬ ЛЕСЛИ ===\\n\\n\")\n\n# Линейная регрессия CPUE от времени\ncat(\"Построение линейной регрессии CPUE ~ Day...\\n\")\nlm_model &lt;- lm(CPUE ~ Day, data = data)\n\n# Добавляем предсказания модели в dataframe\ndata$LM_Prediction &lt;- predict(lm_model)\n\n# ВЫВОД РЕЗУЛЬТАТОВ РЕГРЕССИИ\ncat(\"РЕЗУЛЬТАТЫ ЛИНЕЙНОЙ РЕГРЕССИИ:\\n\")\ncat(\"Наклон (темп снижения CPUE):\", round(coef(lm_model)[2], 5), \"\\n\")\ncat(\"R² (доля объясненной дисперсии):\", round(summary(lm_model)$r.squared, 3), \"\\n\")\ncat(\"p-значение:\", round(summary(lm_model)$coefficients[2,4], 5), \"\\n\\n\")\n\n# ВИЗУАЛИЗАЦИЯ МОДЕЛИ ЛЕСЛИ\ncat(\"Визуализация результатов линейной модели...\\n\")\nplot_leslie &lt;- ggplot(data, aes(x = Day, y = CPUE)) +\n  geom_point(color = \"darkred\", size = 3, alpha = 0.7) +\n  geom_line(aes(y = LM_Prediction), color = \"darkblue\", size = 1.2, \n            linetype = \"solid\") +\n  geom_smooth(method = \"lm\", se = TRUE, alpha = 0.2, color = \"blue\", \n              fill = \"lightblue\") +\n  labs(title = \"Классическая модель Лесли: линейная регрессия CPUE\",\n       subtitle = paste(\"Наклон =\", round(coef(lm_model)[2], 4), \n                       \", R² =\", round(summary(lm_model)$r.squared, 3)),\n       x = \"День промысла\", \n       y = \"Улов на усилие (CPUE)\") +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n\nprint(plot_leslie)\n\n\n\n\n\n\nПосле этого переходим к центральной части — фильтру Калмана для оценки скрытых состояний «запас N» и «миграция m» в линейной гауссовой модели состояния. Настройки фильтра задаются явно. Коэффициент улавливаемости q фиксируем равным 0.002, тем самым связывая шкалу CPUE и запаса: модель наблюдения имеет вид \\(y_t = q N_t + w_t\\). Дисперсии шумов задаём как \\(sigma_N = 30\\) для процесса запаса и \\(sigma_m = 20\\) для процесса миграции; это стандартные отклонения, а в матрицу процесса Q входят их квадраты, так что фактические вариации по состояниям — 900 и 400 соответственно. Измерительная ошибка CPUE задаётся через \\(sigma_y = 0.08\\), а в матрице R фигурирует квадрат 0.0064. Начальное состояние оцениваем из первого наблюдения CPUE простым пересчётом \\(N_0 ≈ CPUE_1 / q\\), что эквивалентно «жёсткой» априорной информации о q и отсутствии смещения в первом измерении; начальную миграцию полагаем нулевой \\(m_0 = 0\\). Начальную ковариацию ошибок оценивания \\(P_0\\) берём диагональной с элементами 100 и 100, что означает умеренную начальную неопределённость по обеим компонентам; по сравнению с процессными дисперсиями это не слишком «жёсткое» начальное доверие, но и не полностью неинформативное. Этот выбор влияет на первые шаги фильтра: чем меньше \\(P_0\\), тем сильнее фильтр доверяет стартовым значениям и плавнее сходитится к устойчивому режиму, чем больше — тем быстрее «подхватывает» структуру шума из Q и данных.\nОсновной цикл фильтра проходит по всем дням. На этапе прогноза мы обновляем состояния по уравнениям процесса: запас прогнозируется как Npred = Nest − Catcht + mest, что реализует баланс Лесли с учётом миграции, а миграция ведёт себя как авторегрессия первого порядка mpred = 0.9mest, где 0.9 — это коэффициент затухания a, означающий, что миграционные импульсы затухают с временем, но не мгновенно. Одновременно формируем матрицу перехода F = [[1, 1], [0, 0.9]] и матрицу процессного шума Q = diag(sigmaN^2, sigmam^2) и рассчитываем прогнозную ковариацию Ppred = F P F^T + Q, которая накапливает неопределённость от процесса. На этапе коррекции связываем состояния с наблюдением через матрицу H = [q, 0], поскольку в CPUE напрямую «виден» только запас. Прогноз измерения есть ypred = q Npred; сравнивая его с фактом CPUEt, получаем инновацию innovation = CPUEt − ypred и её дисперсию S = H Ppred H^T + sigmay^2; именно S задаёт, сколько «шума» мы ожидаем в обновляющем процессе и от модели, и от наблюдения одновременно. Коэффициент усиления Калмана K = Ppred H^T S^{-1} вычисляет оптимальные веса для обновления: если модельная неопределённость велика, а измерение точное, K «тянет» оценку к факту, и наоборот. Далее выполняем обновление состояния: прибавляем к прогнозу вектор приращений stateupdate = K × innovation, тем самым получаем Nest и mest уже с учётом наблюдения, и обновляем ковариацию ошибок оценивания по формуле P = (I − K H) Ppred, что уменьшает неопределённость там, где поступила новая информация. На каждом шаге складываем в результирующую таблицу день, наблюдённый CPUE, фильтровые оценки запаса и миграции и предсказанный CPUE ypred. Важно понимать, что в столбец CPUEEstimated сохраняется именно одношаговый прогноз ypred до коррекции, то есть это линия «что ожидала модель перед тем, как увидеть факт». Если нужно визуализировать «отфильтрованный» CPUE, соответствующий уже обновлённой оценке состояния, логичнее использовать q × Nest после коррекции; оба варианта корректны, но выражают разные величины — прогноз и фильтровую оценку.\nПосле завершения цикла объединяем исходные данные и результаты фильтра в один датафрейм для удобства анализа. Дальше подводим оперативные итоги: печатаем оценку начального и конечного запаса, абсолютное и относительное снижение за период, среднюю миграцию и экстремальные значения миграционного потока. Эти величины непосредственно поступают из траекторий Nest и mest, и по знаку миграции легко интерпретировать периоды чистого притока и оттока. Строим три графика. Первый сравнивает три линии в шкале CPUE: точки фактического CPUE, пунктир классической регрессии Лесли и сплошную линию одношаговых прогнозов фильтра; здесь удобно видеть, как фильтр, учитывая вылов, миграцию и неопределённости, «ведёт» свою линию мимо краткоживущих всплесков, тогда как прямая по определению усредняет всё линейно. Второй график показывает динамику оценённого запаса и миграции на одной картинке: чтобы совместить масштабы, миграция умножается на 10 и сдвигается вверх на 400 единиц, а на правой оси задана обратная трансформация; это чисто визуальный приём, сдвиг не имеет биологического смысла, он лишь помогает видеть оба процесса одновременно. Третий график детализирует миграцию во времени с разметкой по знакам, подчёркивая, в какие дни фильтр «видел» преобладание иммиграции, а в какие — эмиграции.\n\n# ЧАСТЬ 3: ФИЛЬТР КАЛМАНА ДЛЯ ОЦЕНКИ ЗАПАСА И МИГРАЦИИ\n# ====================================================\n# Фильтр Калмана позволяет оценивать ненаблюдаемые состояния системы\n# (запас, миграцию) на основе зашумленных наблюдений (CPUE)\n\ncat(\"\\n=== ЧАСТЬ 3: ФИЛЬТР КАЛМАНА ===\\n\\n\")\n\n# ПАРАМЕТРЫ ФИЛЬТРА КАЛМАНА\ncat(\"Инициализация параметров фильтра Калмана...\\n\")\nq &lt;- 0.002      # Коэффициент улавливаемости (связь между запасом и CPUE)\nsigma_N &lt;- 30   # Шум процесса для запаса (неопределенность в динамике запаса)\nsigma_m &lt;- 20   # Шум процесса для миграции (неопределенность в миграции)\nsigma_y &lt;- 0.08 # Шум измерений для CPUE (ошибка наблюдений)\n\n# НАЧАЛЬНЫЕ УСЛОВИЯ\nN_est &lt;- data$CPUE[1] / q  # Начальная оценка запаса из первого наблюдения CPUE\nm_est &lt;- 0                 # Начальная оценка миграции (предполагаем 0)\nP &lt;- matrix(c(100, 0, 0, 100), nrow = 2)  # Начальная матрица ковариации ошибок\n\n# ДЛЯ ХРАНЕНИЯ РЕЗУЛЬТАТОВ\nresults &lt;- data.frame(\n  Day = integer(),\n  CPUE_Observed = numeric(),\n  Stock_Estimate = numeric(),\n  Migration_Estimate = numeric(),\n  CPUE_Estimated = numeric()\n)\n\ncat(\"Запуск основного цикла фильтра Калмана...\\n\")\n\n# ОСНОВНОЙ ЦИКЛ ФИЛЬТРА КАЛМАНА (для каждого дня наблюдений)\nfor (t in 1:nrow(data)) {\n  \n  # █████ ШАГ 1: ПРОГНОЗ (PREDICTION STEP) █████\n  # Прогнозируем состояние системы на следующий день\n  \n  # Уравнения состояния:\n  N_pred &lt;- N_est - data$Catch[t] + m_est  # Запас: предыдущий - вылов + миграция\n  m_pred &lt;- 0.9 * m_est                    # Миграция: авторегрессия 1-го порядка\n  \n  # Матрица перехода состояний (описывает динамику системы)\n  F_matrix &lt;- matrix(c(1, 1,     # Влияние миграции на запас\n                       0, 0.9),  # Авторегрессия для миграции\n                     nrow = 2, byrow = TRUE)\n  \n  # Матрица ковариации шума процесса\n  Q &lt;- matrix(c(sigma_N^2, 0, \n                0, sigma_m^2), nrow = 2)\n  \n  # Прогноз ковариации ошибок\n  P_pred &lt;- F_matrix %*% P %*% t(F_matrix) + Q\n  \n  # █████ ШАГ 2: КОРРЕКЦИЯ (UPDATE STEP) █████\n  # Обновляем прогноз на основе новых наблюдений\n  \n  # Матрица измерений (связь состояний с наблюдениями)\n  H &lt;- matrix(c(q, 0), nrow = 1)  # Только запас влияет на CPUE\n  \n  # Прогнозируемое значение CPUE\n  y_pred &lt;- q * N_pred\n  \n  # Инновация (невязка) - разница между наблюдением и прогнозом\n  innovation &lt;- data$CPUE[t] - y_pred\n  \n  # Ковариация инноваций\n  S &lt;- H %*% P_pred %*% t(H) + sigma_y^2\n  \n  # Коэффициент усиления Калмана (оптимальные веса для обновления)\n  K &lt;- P_pred %*% t(H) %*% solve(S)\n  \n  # ОБНОВЛЕНИЕ ОЦЕНОК СОСТОЯНИЙ\n  state_update &lt;- K %*% innovation\n  N_est &lt;- N_pred + state_update[1,1]  # Обновляем оценку запаса\n  m_est &lt;- m_pred + state_update[2,1]  # Обновляем оценку миграции\n  \n  # ОБНОВЛЕНИЕ КОВАРИАЦИИ ОШИБОК\n  P &lt;- (diag(2) - K %*% H) %*% P_pred\n  \n  # СОХРАНЕНИЕ РЕЗУЛЬТАТОВ ДЛЯ ЭТОГО ДНЯ\n  results &lt;- rbind(results, data.frame(\n    Day = t,\n    CPUE_Observed = data$CPUE[t],\n    Stock_Estimate = N_est,\n    Migration_Estimate = m_est,\n    CPUE_Estimated = y_pred\n  ))\n}\n\ncat(\"Фильтр Калмана успешно завершил работу!\\n\\n\")\n\n# ОБЪЕДИНЕНИЕ ВСЕХ ДАННЫХ ДЛЯ АНАЛИЗА\nfinal_data &lt;- cbind(data, results[, c(\"Stock_Estimate\", \"Migration_Estimate\", \"CPUE_Estimated\")])\n\n# ЧАСТЬ 4: ЗАКЛЮЧИТЕЛЬНАЯ АНАЛИТИКА И ВИЗУАЛИЗАЦИЯ\n# ================================================\n\ncat(\"=== ЧАСТЬ 4: СРАВНИТЕЛЬНЫЙ АНАЛИЗ РЕЗУЛЬТАТОВ ===\\n\\n\")\n\n# ОСНОВНЫЕ РЕЗУЛЬТАТЫ ФИЛЬТРА КАЛМАНА\ncat(\"ОЦЕНКИ ФИЛЬТРА КАЛМАНА:\\n\")\ncat(\"• Начальный запас:\", round(results$Stock_Estimate[1], 1), \"особей\\n\")\ncat(\"• Конечный запас:\", round(results$Stock_Estimate[20], 1), \"особей\\n\")\ncat(\"• Снижение запаса за период:\", \n    round(results$Stock_Estimate[1] - results$Stock_Estimate[20], 1), \n    \"особей (\", \n    round((results$Stock_Estimate[1] - results$Stock_Estimate[20]) / results$Stock_Estimate[1] * 100, 1),\n    \"%)\\n\")\ncat(\"• Средняя миграция:\", round(mean(results$Migration_Estimate), 1), \"особей/день\\n\")\ncat(\"• Максимальный приток:\", round(max(results$Migration_Estimate), 1), \"особей/день\\n\")\ncat(\"• Максимальный отток:\", round(min(results$Migration_Estimate), 1), \"особей/день\\n\\n\")\n\n# ГРАФИК 1: СРАВНЕНИЕ МЕТОДОВ ОЦЕНКИ CPUE\ncat(\"Построение сравнительных графиков...\\n\")\nplot_comparison &lt;- ggplot(final_data) +\n  # Наблюдения\n  geom_point(aes(x = Day, y = CPUE, color = \"Наблюдения CPUE\"), size = 3, alpha = 0.7) +\n  # Модель Лесли\n  geom_line(aes(x = Day, y = LM_Prediction, color = \"Модель Лесли\"), \n            linetype = \"dashed\", size = 1.2, alpha = 0.8) +\n  # Фильтр Калмана\n  geom_line(aes(x = Day, y = CPUE_Estimated, color = \"Фильтр Калмана\"), \n            size = 1.5, alpha = 0.9) +\n  # Настройки графика\n  labs(title = \"Сравнение методов оценки динамики запаса\",\n       subtitle = \"Фильтр Калмана vs Классическая модель Лесли\",\n       x = \"День промысла\", \n       y = \"Улов на усилие (CPUE)\",\n       color = \"Метод\") +\n  scale_color_manual(values = c(\n    \"Наблюдения CPUE\" = \"darkred\",\n    \"Модель Лесли\" = \"darkblue\", \n    \"Фильтр Калмана\" = \"darkgreen\"\n  )) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\n        legend.position = \"bottom\")\n\nprint(plot_comparison)\n\n# ГРАФИК 2: ДИНАМИКА ЗАПАСА И МИГРАЦИИ\nplot_stock_migration &lt;- ggplot(final_data) +\n  # Оценка запаса\n  geom_line(aes(x = Day, y = Stock_Estimate, color = \"Оценка запаса\"), \n            size = 1.3) +\n  # Миграционный поток (масштабируем для визуализации)\n  geom_line(aes(x = Day, y = Migration_Estimate * 10 + 400, \n                color = \"Миграция (x10)\"), size = 1.1, alpha = 0.8) +\n  # Две оси Y\n  scale_y_continuous(\n    name = \"Оценка запаса (особи)\",\n    sec.axis = sec_axis(~ (. - 400) / 10, name = \"Миграционный поток (особи/день)\")\n  ) +\n  labs(title = \"Динамика запаса и миграционного потока\",\n       subtitle = \"Результаты работы фильтра Калмана\",\n       x = \"День промысла\",\n       color = \"Переменные\") +\n  scale_color_manual(values = c(\n    \"Оценка запаса\" = \"steelblue\",\n    \"Миграция (x10)\" = \"orange\"\n  )) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\n        legend.position = \"bottom\")\n\nprint(plot_stock_migration)\n\n# ГРАФИК 3: ДЕТАЛЬНЫЙ АНАЛИЗ МИГРАЦИИ\nplot_migration_detail &lt;- ggplot(final_data, aes(x = Day, y = Migration_Estimate)) +\n  geom_line(color = \"firebrick\", size = 1.3) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5, size = 1) +\n  geom_point(aes(color = ifelse(Migration_Estimate &gt; 0, \"Иммиграция\", \"Эмиграция\")), \n             size = 3) +\n  labs(title = \"Оценка миграционного потока фильтром Калмана\",\n       subtitle = \"Положительные значения - приток, отрицательные - отток\",\n       x = \"День промысла\", \n       y = \"Чистый миграционный поток (особи/день)\",\n       color = \"Тип миграции\") +\n  scale_color_manual(values = c(\"Иммиграция\" = \"green4\", \"Эмиграция\" = \"red3\")) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\n        legend.position = \"bottom\")\n\nprint(plot_migration_detail)\n\n# ТАБЛИЦА РЕЗУЛЬТАТОВ\ncat(\"=== ТАБЛИЦА РЕЗУЛЬТАТОВ (первые 10 дней) ===\\n\")\nresults_table &lt;- head(final_data[, c(\"Day\", \"Catch\", \"CPUE\", \"Stock_Estimate\", \"Migration_Estimate\")], 10) %&gt;%\n  mutate(across(where(is.numeric), round, 1))\n\nprint(results_table)\n\n# СТАТИСТИКА КАЧЕСТВА ОЦЕНОК\ncat(\"\\n=== СТАТИСТИКА КАЧЕСТВА ОЦЕНОК ===\\n\")\n\n# Среднеквадратичные ошибки\nmse_leslie &lt;- mean((final_data$CPUE - final_data$LM_Prediction)^2)\nmse_kalman &lt;- mean((final_data$CPUE - final_data$CPUE_Estimated)^2)\n\ncat(\"СРЕДНЕКВАДРАТИЧНЫЕ ОШИБКИ (MSE):\\n\")\ncat(\"• Модель Лесли:\", round(mse_leslie, 4), \"\\n\")\ncat(\"• Фильтр Калмана:\", round(mse_kalman, 4), \"\\n\")\ncat(\"• Улучшение:\", round((mse_leslie - mse_kalman) / mse_leslie * 100, 1), \"%\\n\\n\")\n\n# ЗАКЛЮЧЕНИЕ\ncat(\"=== ВЫВОДЫ И РЕКОМЕНДАЦИИ ===\\n\")\ncat(\"1. Фильтр Калмана позволяет оценивать ненаблюдаемые параметры (запас, миграцию)\\n\")\ncat(\"2. Учитывает не только наблюдения, но и знания о динамике системы\\n\")\ncat(\"3. Обеспечивает более точные оценки по сравнению с классическими методами\\n\")\ncat(\"4. Особенно полезен при наличии миграционных процессов\\n\")\ncat(\"5. Требует тщательной настройки параметров шумов\\n\\n\")\n\ncat(\"Анализ завершен! Для углубленного изучения рекомендуется:\\n\")\ncat(\"• Экспериментировать с параметрами шумов (sigma_N, sigma_m, sigma_y)\\n\")\ncat(\"• Добавить сезонные компоненты в модель\\n\")\ncat(\"• Рассмотреть нелинейные версии фильтра Калмана\\n\")\n\n   Чтобы увязать качество подгонки в шкале наблюдений, считаем среднеквадратические ошибки для регрессии Лесли и для линии CPUEEstimated. В данном прогоне MSE (Mean Squared Error) фильтра близка к MSE регрессии и может быть чуть хуже, что нормально, поскольку мы сравниваем не «подогнанную линию» к факту, а одношаговые прогнозы, которые по определению не используют нынешнее измерение. Если вместо прогноза использовать q × Nest после обновления, MSE обычно снижается и становится сопоставимой или лучшей, а настройка шума измерений sigmay и процессных шумов sigmaN и sigmam позволяет управлять балансом доверия между моделью и данными. Финальные текстовые выводы подчёркивают ключевую практическую мысль: фильтр Калмана позволяет оценивать ненаблюдаемые компоненты баланса (запас и миграцию), опираясь одновременно на физику процесса и на данные; он особенно полезен при наличии миграций и шумных индексов и требует аккуратного выбора дисперсий, что в прикладных задачах делается по максимальному правдоподобию инноваций, как это реализовано в ОМЛ ФК по Михееву. В этом учебном скрипте параметры заданы вручную ради прозрачности, шаг контроля «пропусков» не демонстрируется, поскольку синтетический ряд полный; однако та же схема фильтра устойчиво работает и при отсутствующих наблюдениях, если на таких шагах пропускать блок коррекции и не менять P_pred формулой обновления.",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Фильтр Калмана</span>"
    ]
  }
]