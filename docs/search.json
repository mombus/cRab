[
  {
    "objectID": "chapter 9.html",
    "href": "chapter 9.html",
    "title": "10  Стандартизация CPUE: GLM, GAM, GAMM",
    "section": "",
    "text": "10.1 Введение\nНачнём с привычной ловушки. Когда на столе лежит «сырое» CPUE и график по годам, очень хочется увидеть в нём прямое отражение численности запаса и, тем самым, ручку управления промыслом. Это эффект удобной истории: «Система 1» по Канеману мгновенно дорисовывает причинность там, где в данных много посторонней вариации — сезон, район, глубина, тип орудий, «почерк» судна, длительность постановок. Если эту вариацию не отделить, CPUE превращается в термометр, который меняет показания вместе с погодой в комнате. Наша задача в этом занятии — аккуратно «изолировать» измерение: стандартизировать CPUE так, чтобы оно преимущественно отражало динамику запаса, а не всё остальное.\nЧто именно мы стандартизируем. CPUE — это маркер доступной части популяции при данных условиях промысла. Мы хотим извлечь из него индекс, сопоставимый между годами, сведя к «норме» всё, что не про численность или биомассу запаса: различия по месяцам, районам, глубинам, орудиям, судам. Ключ — заранее определить целевой «эталон»: к чему приводим? К маргинальным средним по дизайну (сбалансированная виртуальная съёмка) или к фиксированному референс‑уровню (базовый месяц/район/судно)? Первый вариант лучше отражает «среднедоступную» систему, второй — удобен для прозрачной интерпретации. Оба корректны, если оговорены и последовательно применяются.\nПочему GLM/GAM/GAMM. GLM с гамма‑распределением и лог‑ссылкой — рабочая лошадка для положительных, правоскошенных величин; лог‑связь естественно переводит эффекты в относительные (мультипликативные) изменения. GAM добавляет гибкость: там, где линейные контрасты по глубине или температуре ломают картину, гладкие функции честно показывают оптимумы и изгибы. GAMM вводит случайные эффекты (например, по судну), отделяя структурные тенденции от «личной биографии» флотилии — то, что в фиксированных эффектах превращается в лишний шум и завышенную уверенность. Выбор не догматичен: начинаем с простого GLM, расширяем до GAM, добавляем случайные эффекты только тогда, когда это подтверждается диагностикой и улучшает калибровку индекса.\nПро подводные камни, которые мы будем ловить. Во‑первых, нули и «почти нули»: для гамма‑семейства понадобится аккуратная положительная поправка; если нулей много, разумна двухступенчатая дельта‑постановка (биномиальная часть + положительная часть). Во‑вторых, дрейф уловистости (q‑drift): изменение орудий и практик может имитировать «падение запаса». Мы не устраняем его магией, но делаем явным — через факторы/ковариаты и, при необходимости, случайные эффекты. В‑третьих, утечка информации из будущего: сравнивая индексы, мы придерживаемся блокировок по годам и не «подмешиваем» композиционные сдвиги выборки (например, если флот переместился в другой район). В‑четвёртых, concurvity/мультиколлинеарность: GAM может «переобъяснить» одно и то же несколькими гладкими — проверяем concurvity и упрощаем формулу. И, конечно, диагностика остатков (DHARMa), проверка дисперсии и влияния наблюдений — обязательна перед любыми управленческими выводами.\nКак читать результат. Стандартизированный индекс — это не абсолютная биомасса и не прогноз улова; это аккуратно отчищенный маркер относительной доступности/численности, сопоставимый между годами при прочих равных. Он публикуется с доверительными интервалами и явной оговоркой о референсе (маргинальные средние или фиксированный профиль). Нормировки «к среднему» или «к первому году» — это про удобство сравнения, а не про «истинную шкалу». Чувствительность к выбору формулы (GLM vs GAM vs GAMM), к набору факторов и к способу усреднения — часть честного отчёта: если индексы согласованы, доверие растёт; если нет — разбираем, где скрыт драйвер расхождений.\nЧто делаем по шагам. Загружаем и чистим данные (включая корректную работу с нулями и факторами), строим GLM (Gamma‑log) с ключевыми факторами (год, сезон, район, судно), проверяем остатки и считаем маргинальные индексы с интервалами. Затем ставим GAM там, где предметная логика допускает нелинейности, и повторяем диагностику (gam.check, concurvity). После — GAMM со случайным эффектом по судну, чтобы отделить «командный почерк» от тренда, и оцениваем индекс через предсказания на сбалансированной сетке и бутстреп‑интервалы. Финально сводим индексы на одном графике вместе с фактическими медианами CPUE и сравниваем модели по AIC/BIC и адекватности остатков. Если «сложнее» не лучше — остаёмся на более простой, но калиброванной модели.\nИ напоследок — про уверенность. Хорошая стандартизация CPUE — это не способ «победить» неопределённость, а способ честно на неё смотреть. Мы покажем не только линию, но и ленту интервалов; не только медиану, но и альтернативы формул; не только тренд, но и диагностику. Это ровно тот прогресс, который работает в долгую: меньше иллюзий контроля, больше прозрачности, лучше решения.\nПолный скрипт можно скачать по ссылке.\nДля работы скрипта:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Стандартизация CPUE: GLM, GAM, GAMM</span>"
    ]
  },
  {
    "objectID": "chapter 9.html#введение",
    "href": "chapter 9.html#введение",
    "title": "10  Стандартизация CPUE: GLM, GAM, GAMM",
    "section": "",
    "text": "Скачайте файл данных (KARTOGRAPHIC.xlsx)\nУстановите рабочую директорию в setwd()\nУстановите необходимые пакеты : install.packages(c(\"readxl\", \"tidyverse, \"mgcv\", \"gamm4\", \"DHARMa\" )) и др.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Стандартизация CPUE: GLM, GAM, GAMM</span>"
    ]
  },
  {
    "objectID": "chapter 9.html#пошаговое-описание-скрипта",
    "href": "chapter 9.html#пошаговое-описание-скрипта",
    "title": "10  Стандартизация CPUE: GLM, GAM, GAMM",
    "section": "10.2 Пошаговое описание скрипта",
    "text": "10.2 Пошаговое описание скрипта\nСкрипт начинается с загрузки необходимых пакетов для обработки данных, построения моделей и визуализации результатов. Среда настраивается путем установки рабочей директории и фиксации случайного зерна для обеспечения воспроизводимости всех последующих вычислений.\nНа следующем этапе происходит загрузка исходных данных из файла Excel. Данные представляют собой промысловую статистику, содержащую информацию о году, месяце, судне, районе, величине улова на усилие (CPUE) и др. Выполняется их предварительная обработка: фильтрация по осенним месяцам, преобразование типов переменных в факторы и числовой формат, а также удаление пропущенных значений. Поскольку для моделирования с гамма-распределением требуются строго положительные значения, для нулевых и отрицательных величин CPUE рассчитывается и добавляется малая поправка. Для первичного ознакомления с данными строится диаграмма размаха, показывающая распределение CPUE по годам.\nДалее определяются вспомогательные функции. Одна функция предназначена для нормировки рассчитанных индексов либо на среднее значение, либо на значение первого года. Другая функция использует метод маргинальных средних для расчета стандартизированных индексов и их доверительных интервалов на основе подобранной модели. Третья функция реализует расчет индексов и оценку неопределенности через бутстреп для моделей со сложной структурой.\nОсновная часть скрипта посвящена построению и анализу трех типов моделей. Первой подбирается обобщенная линейная модель (GLM) с гамма-распределением ошибок и логарифмической связью. В качестве предикторов используются факторы: год, месяц, судно и район. Для визуальной и численной диагностики адекватности модели выводятся ее сводка, таблица коэффициентов, стандартные диагностические графики и графики остатков, проверенные с помощью пакета DHARMa.\nСледующей строится обобщенная аддитивная модель (GAM). На этом этапе используется та же формула и семейство распределений, что и в GLM, но метод подбора гиперпараметров отличается. Проводится аналогичная диагностика модели с помощью функций summary и gam.check.\nЗатем подбирается обобщенная аддитивная смешанная модель (GAMM), которая дополнительно включает случайный эффект от судна. Это позволяет учесть вариацию, вызванную индивидуальными особенностями каждого судна, которые не описываются другими факторами. Диагностика этой модели более сложна и включает анализ остатков, проверку случайных эффектов и тест на гетероскедастичность.\nПосле построения всех моделей для каждой из них рассчитываются стандартизированные индексы CPUE и их доверительные интервалы. Для GLM и GAM это делается с помощью функции, основанной на маргинальных средних, а для GAMM применяется метод бутстрепа.\nФинальный этап включает объединение результатов всех трех моделей в единую таблицу и их визуальное сравнение на графике. На этот же график добавляются фактические медианные значения CPUE по годам из исходных данных для сопоставления со стандартизированными кривыми. В заключение модели сравниваются по информационным критериям (AIC, BIC) и другим метрикам, чтобы дать рекомендации по выбору наиболее адекватной из них.\nНиже приводится скрипт, а ниже скрипта - описание результатов моделирования.\n\n# ========================================================================================================================\n# ПРАКТИЧЕСКОЕ ЗАНЯТИЕ: СТАНДАРТИЗАЦИЯ CPUE С ИСПОЛЬЗОВАНИЕМ GLM, GAM И GAMM\n# Курс: \"Оценка водных биоресурсов в среде R (для начинающих)\"\n# Автор: Баканев С.В. \n# Дата: 20.08.2025\n# \n# Структура:\n# 1) Загрузка пакетов и настройка среды\n# 2) Загрузка и предварительная обработка данных\n# 3) Вспомогательные функции для расчета индексов\n# 4) Моделирование GLM (Gamma с лог-ссылкой)\n# 5) Моделирование GAM (обобщенная аддитивная модель)\n# 6) Моделирование GAMM (смешанная модель со случайными эффектами)\n# 7) Сравнение моделей и финальная визуализация результатов\n# ========================================================================================================================\n\n\n# ==============================================================================\n# БЛОК 1: ЗАГРУЗКА ПАКЕТОВ И НАСТРОЙКА СРЕДЫ\n# ==============================================================================\n\n# Отключаем вспомогательные сообщения при загрузке пакетов\nsuppressPackageStartupMessages({\n  library(tidyverse)   # Основные пакеты для обработки данных и визуализации\n  library(readxl)      # Чтение данных из Excel-файлов\n  library(mgcv)        # Обобщенные аддитивные модели (GAM)\n  library(gamm4)       # GAM со смешанными эффектами\n  library(emmeans)     # Расчет маргинальных средних и контрастов\n  library(broom)       # Преобразование результатов моделей в таблицы\n  library(broom.mixed) # Поддержка смешанных моделей для broom\n  library(DHARMa)      # Диагностика остатков обобщенных моделей\n  library(knitr)       # Форматирование таблиц для отчетов\n})\n\n# Установка рабочей директории\nsetwd(\"C:/GLM/\")\n\n# Фиксируем случайное зерно для воспроизводимости результатов\nset.seed(42)\n\n# ==============================================================================\n# БЛОК 2: ЗАГРУЗКА И ПРЕДОБРАБОТКА ДАННЫХ\n# ==============================================================================\n\n# Определяем путь к файлу с данными\nDATA_PATH &lt;- \"C:/GLM/data/KARTOGRAPHIC.xlsx\"\n\n# Чтение данных из листа \"FISHERY\" и фильтрация осенних месяцев\nDATA &lt;- read_excel(DATA_PATH, sheet = \"FISHERY\") %&gt;%\n  as_tibble() %&gt;%  # Преобразуем в современный формат таблицы\n  filter(MONTH &gt; 8 & MONTH &lt; 12)  # Сентябрь-ноябрь (осенний сезон)\n\n# Преобразование типов переменных и обработка пропусков\nDATA &lt;- DATA %&gt;%\n  mutate(\n    YEAR = as.factor(YEAR),           # Год как категориальная переменная\n    MONTH = as.factor(MONTH),         # Месяц как фактор\n    CALL = as.factor(CALL),           # Идентификатор судна\n    REGION = as.factor(REGION),       # Рыбохозяйственный район\n    VESSELNUMBER = as.factor(VESSELNUMBER),  # Номер судна\n    CPUE = as.numeric(CPUE)           # Целевой показатель - улов на усилие\n  ) %&gt;%\n  filter(!is.na(CPUE))  # Удаление строк с пропусками в CPUE\n\n# Обработка нулевых значений CPUE для Gamma-моделей\nif (any(DATA$CPUE &lt;= 0, na.rm = TRUE)) {\n  min_pos &lt;- min(DATA$CPUE[DATA$CPUE &gt; 0], na.rm = TRUE)  # Минимальный положительный улов\n  offset &lt;- min_pos / 2  # Величина поправки\n  DATA &lt;- DATA %&gt;% \n    mutate(CPUE_POS = if_else(CPUE &lt;= 0, CPUE + offset, CPUE))  # Добавляем поправку\n} else {\n  DATA &lt;- DATA %&gt;% \n    mutate(CPUE_POS = CPUE)  # Исходные данные если нулей нет\n}\n\n# Рассчитываем медианные значения CPUE по годам из исходных данных\nactual_medians &lt;- DATA %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(median_cpue = median(CPUE, na.rm = TRUE))\n# Рассчитываем медианные значения CPUE по годам из исходных данных для последующих графиков\nactual_medians\n\n# A tibble: 6 x 2\n  YEAR  median_cpue\n  &lt;fct&gt;       &lt;dbl&gt;\n1 2019        200. \n2 2020        116. \n3 2021        132. \n4 2022         84  \n5 2023         79.4\n6 2024         58.3\n\n# Экспресс-визуализация распределения CPUE по годам\nDATA %&gt;%\n  ggplot(aes(x = YEAR, y = CPUE)) +\n  geom_boxplot(outlier.alpha = 0.2) +\n  labs(title = \"Распределение CPUE по годам\", \n       x = \"Год\", \n       y = \"CPUE (улов на усилие)\")\n\n\n\n\n\n\n\n# ==============================================================================\n# БЛОК 3: ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ ДЛЯ РАСЧЕТА ИНДЕКСОВ\n# ==============================================================================\n\n# Функция нормировки индексов\nscale_to_index &lt;- function(values, method = c(\"mean\", \"first\")) {\n  method &lt;- match.arg(method)\n  if (method == \"mean\") {\n    # Нормировка на среднее значение\n    return(as.numeric(values) / mean(as.numeric(values), na.rm = TRUE))\n  }\n  if (method == \"first\") {\n    # Нормировка на значение первого года\n    return(as.numeric(values) / as.numeric(values[1]))\n  }\n}\n\n# Функция расчета индексов для GLM/GAM через маргинальные средние\nemmeans_standardized_index &lt;- function(model, variable = \"YEAR\") {\n  out &lt;- suppressWarnings(\n    emmeans(model, \n            specs = as.formula(paste0(\"~ \", variable)), \n            type = \"response\")\n  )\n  df &lt;- as_tibble(out) %&gt;% \n    select(!!sym(variable), response = response, lower.CL, upper.CL)\n  colnames(df) &lt;- c(\"YEAR\", \"value\", \"lcl\", \"ucl\")\n  df\n}\n\n# Функция расчета индексов для GAMM через бутстреп\ncompute_standardized_index &lt;- function(model, base_data, year_levels, predict_fun,\n                                      response_transform = identity, \n                                      n_boot = 200L, \n                                      seed = 7L) {\n  set.seed(seed)\n  acc &lt;- vector(\"list\", length(year_levels))\n  for (i in seq_along(year_levels)) {\n    newdata &lt;- base_data\n    newdata$YEAR &lt;- factor(year_levels[i], levels = levels(base_data$YEAR))\n    preds &lt;- suppressWarnings(predict_fun(model, newdata))\n    mu &lt;- mean(response_transform(preds), na.rm = TRUE)\n    # Бутстреп для оценки неопределенности\n    boot_vals &lt;- replicate(n_boot, {\n      idx &lt;- sample.int(nrow(base_data), nrow(base_data), replace = TRUE)\n      bd &lt;- newdata[idx, , drop = FALSE]\n      p &lt;- suppressWarnings(predict_fun(model, bd))\n      mean(response_transform(p), na.rm = TRUE)\n    })\n    ci &lt;- quantile(boot_vals, c(0.025, 0.975), na.rm = TRUE)\n    acc[[i]] &lt;- tibble(YEAR = year_levels[i], value = mu, lcl = ci[[1]], ucl = ci[[2]])\n  }\n  bind_rows(acc)\n}\n\n# ==============================================================================\n# БЛОК 4: МОДЕЛИРОВАНИЕ GLM (GAMMA С ЛОГ-ССЫЛКОЙ)\n# ==============================================================================\n\n# Подбор модели с фиксированными эффектами\nglm_gamma_fit &lt;- glm(\n  CPUE_POS ~ YEAR + MONTH + CALL + REGION,  # Формула с факторными предикторами\n  family = Gamma(link = \"log\"),            # Гамма-распределение с логарифмической связью\n  data = DATA\n)\n\n# Диагностика модели\nsummary(glm_gamma_fit)  # Стандартная сводка модели\n\n\nCall:\nglm(formula = CPUE_POS ~ YEAR + MONTH + CALL + REGION, family = Gamma(link = \"log\"), \n    data = DATA)\n\nCoefficients:\n                                                  Estimate Std. Error t value\n(Intercept)                                        5.57642    0.07942  70.216\nYEAR2020                                          -0.22713    0.04586  -4.953\nYEAR2021                                          -0.22438    0.04557  -4.924\nYEAR2022                                          -0.64329    0.04345 -14.806\nYEAR2023                                          -0.77311    0.04647 -16.637\nYEAR2024                                          -1.12817    0.05157 -21.879\nMONTH10                                           -0.13725    0.02443  -5.617\nMONTH11                                           -0.13714    0.03540  -3.874\nCALLUAAK                                          -0.29534    0.06092  -4.848\nCALLUAKC                                          -0.53490    0.06381  -8.382\nCALLUBEV                                          -3.67233    0.12187 -30.133\nCALLUBQQ                                          -0.35433    0.07057  -5.021\nCALLUBSR                                          -0.33516    0.06266  -5.349\nCALLUBUR                                          -0.58246    0.06175  -9.433\nCALLUBYT                                          -0.21520    0.06088  -3.535\nCALLUCFF                                          -0.06756    0.09825  -0.688\nCALLUCXF                                          -2.34302    0.10638 -22.025\nCALLUDII                                          -0.46287    0.05710  -8.107\nCALLUDUT                                          -1.02162    0.07550 -13.532\nCALLUDWM                                          -2.42502    0.09480 -25.582\nCALLUEBK                                           0.25852    0.13371   1.934\nCALLUEMO                                          -0.17024    0.08103  -2.101\nCALLUENZ                                           0.04928    0.06442   0.765\nCALLUFIK                                           0.29700    0.13043   2.277\nCALLUGXE                                          -0.56384    0.06265  -9.000\nCALLUIVO                                          -0.21572    0.06500  -3.319\nREGIONCEB.-ЦEHTPAЛЬHЫЙ P-H                         0.16521    0.46672   0.354\nREGIONCEB.CKЛOH MУPMAHCKOГO MEЛKOBOДЬЯ            -0.07615    0.15513  -0.491\nREGIONCEBEPO-KAHИHCKAЯ БAHKA                       0.16332    0.08236   1.983\nREGIONKAHИHCKAЯ БAHKA                              0.06291    0.07875   0.799\nREGIONKAHИHCKO- KOЛГУEBCKOE MEЛKOBOДЬE(CEB.CKЛOH)  0.21310    0.08519   2.501\nREGIONKAHИHCKO-KOЛГУEBCKOE MEЛKOBOДЬE              0.18223    0.07757   2.349\nREGIONMУPMAHCKOE MEЛKOBOДЬE                        0.07737    0.07753   0.998\nREGIONЗAП.-ПPИБPEЖHЫЙ P-H                          0.72882    0.46327   1.573\nREGIONЗAП.-ЦEHTPAЛЬHЫЙ P-H                         0.13328    0.08416   1.584\nREGIONЗAП.CKЛOH ГУCИHOЙ БAHKИ                      0.64238    0.11646   5.516\nREGIONЮЖ.CKЛOH ГУCИHOЙ БAHKИ                       0.69320    0.13068   5.305\n                                                  Pr(&gt;|t|)    \n(Intercept)                                        &lt; 2e-16 ***\nYEAR2020                                          7.62e-07 ***\nYEAR2021                                          8.85e-07 ***\nYEAR2022                                           &lt; 2e-16 ***\nYEAR2023                                           &lt; 2e-16 ***\nYEAR2024                                           &lt; 2e-16 ***\nMONTH10                                           2.08e-08 ***\nMONTH11                                           0.000109 ***\nCALLUAAK                                          1.30e-06 ***\nCALLUAKC                                           &lt; 2e-16 ***\nCALLUBEV                                           &lt; 2e-16 ***\nCALLUBQQ                                          5.36e-07 ***\nCALLUBSR                                          9.37e-08 ***\nCALLUBUR                                           &lt; 2e-16 ***\nCALLUBYT                                          0.000413 ***\nCALLUCFF                                          0.491691    \nCALLUCXF                                           &lt; 2e-16 ***\nCALLUDII                                          6.93e-16 ***\nCALLUDUT                                           &lt; 2e-16 ***\nCALLUDWM                                           &lt; 2e-16 ***\nCALLUEBK                                          0.053247 .  \nCALLUEMO                                          0.035718 *  \nCALLUENZ                                          0.444299    \nCALLUFIK                                          0.022839 *  \nCALLUGXE                                           &lt; 2e-16 ***\nCALLUIVO                                          0.000913 ***\nREGIONCEB.-ЦEHTPAЛЬHЫЙ P-H                        0.723377    \nREGIONCEB.CKЛOH MУPMAHCKOГO MEЛKOBOДЬЯ            0.623533    \nREGIONCEBEPO-KAHИHCKAЯ БAHKA                      0.047425 *  \nREGIONKAHИHCKAЯ БAHKA                             0.424416    \nREGIONKAHИHCKO- KOЛГУEBCKOE MEЛKOBOДЬE(CEB.CKЛOH) 0.012413 *  \nREGIONKAHИHCKO-KOЛГУEBCKOE MEЛKOBOДЬE             0.018863 *  \nREGIONMУPMAHCKOE MEЛKOBOДЬE                       0.318379    \nREGIONЗAП.-ПPИБPEЖHЫЙ P-H                         0.115753    \nREGIONЗAП.-ЦEHTPAЛЬHЫЙ P-H                        0.113361    \nREGIONЗAП.CKЛOH ГУCИHOЙ БAHKИ                     3.69e-08 ***\nREGIONЮЖ.CKЛOH ГУCИHOЙ БAHKИ                      1.19e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.4172272)\n\n    Null deviance: 2980.2  on 3890  degrees of freedom\nResidual deviance: 1785.6  on 3854  degrees of freedom\nAIC: 42851\n\nNumber of Fisher Scoring iterations: 11\n\n# Таблица коэффициентов в форматированном виде\nbroom::tidy(glm_gamma_fit) %&gt;%\n  mutate(across(estimate:statistic, ~round(.x, 4))) %&gt;%\n  kable(caption = \"Коэффициенты GLM модели\", align = \"lrrrr\")\n\n\nКоэффициенты GLM модели\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n5.5764\n0.0794\n70.2164\n0.0000000\n\n\nYEAR2020\n-0.2271\n0.0459\n-4.9530\n0.0000008\n\n\nYEAR2021\n-0.2244\n0.0456\n-4.9236\n0.0000009\n\n\nYEAR2022\n-0.6433\n0.0434\n-14.8059\n0.0000000\n\n\nYEAR2023\n-0.7731\n0.0465\n-16.6372\n0.0000000\n\n\nYEAR2024\n-1.1282\n0.0516\n-21.8785\n0.0000000\n\n\nMONTH10\n-0.1372\n0.0244\n-5.6170\n0.0000000\n\n\nMONTH11\n-0.1371\n0.0354\n-3.8736\n0.0001090\n\n\nCALLUAAK\n-0.2953\n0.0609\n-4.8479\n0.0000013\n\n\nCALLUAKC\n-0.5349\n0.0638\n-8.3823\n0.0000000\n\n\nCALLUBEV\n-3.6723\n0.1219\n-30.1334\n0.0000000\n\n\nCALLUBQQ\n-0.3543\n0.0706\n-5.0213\n0.0000005\n\n\nCALLUBSR\n-0.3352\n0.0627\n-5.3488\n0.0000001\n\n\nCALLUBUR\n-0.5825\n0.0618\n-9.4326\n0.0000000\n\n\nCALLUBYT\n-0.2152\n0.0609\n-3.5347\n0.0004130\n\n\nCALLUCFF\n-0.0676\n0.0982\n-0.6877\n0.4916914\n\n\nCALLUCXF\n-2.3430\n0.1064\n-22.0254\n0.0000000\n\n\nCALLUDII\n-0.4629\n0.0571\n-8.1065\n0.0000000\n\n\nCALLUDUT\n-1.0216\n0.0755\n-13.5319\n0.0000000\n\n\nCALLUDWM\n-2.4250\n0.0948\n-25.5817\n0.0000000\n\n\nCALLUEBK\n0.2585\n0.1337\n1.9335\n0.0532474\n\n\nCALLUEMO\n-0.1702\n0.0810\n-2.1009\n0.0357185\n\n\nCALLUENZ\n0.0493\n0.0644\n0.7650\n0.4442989\n\n\nCALLUFIK\n0.2970\n0.1304\n2.2770\n0.0228387\n\n\nCALLUGXE\n-0.5638\n0.0627\n-8.9996\n0.0000000\n\n\nCALLUIVO\n-0.2157\n0.0650\n-3.3187\n0.0009126\n\n\nREGIONCEB.-ЦEHTPAЛЬHЫЙ P-H\n0.1652\n0.4667\n0.3540\n0.7233768\n\n\nREGIONCEB.CKЛOH MУPMAHCKOГO MEЛKOBOДЬЯ\n-0.0762\n0.1551\n-0.4909\n0.6235329\n\n\nREGIONCEBEPO-KAHИHCKAЯ БAHKA\n0.1633\n0.0824\n1.9831\n0.0474248\n\n\nREGIONKAHИHCKAЯ БAHKA\n0.0629\n0.0788\n0.7989\n0.4244161\n\n\nREGIONKAHИHCKO- KOЛГУEBCKOE MEЛKOBOДЬE(CEB.CKЛOH)\n0.2131\n0.0852\n2.5013\n0.0124133\n\n\nREGIONKAHИHCKO-KOЛГУEBCKOE MEЛKOBOДЬE\n0.1822\n0.0776\n2.3492\n0.0188631\n\n\nREGIONMУPMAHCKOE MEЛKOBOДЬE\n0.0774\n0.0775\n0.9979\n0.3183790\n\n\nREGIONЗAП.-ПPИБPEЖHЫЙ P-H\n0.7288\n0.4633\n1.5732\n0.1157534\n\n\nREGIONЗAП.-ЦEHTPAЛЬHЫЙ P-H\n0.1333\n0.0842\n1.5836\n0.1133609\n\n\nREGIONЗAП.CKЛOH ГУCИHOЙ БAHKИ\n0.6424\n0.1165\n5.5160\n0.0000000\n\n\nREGIONЮЖ.CKЛOH ГУCИHOЙ БAHKИ\n0.6932\n0.1307\n5.3046\n0.0000001\n\n\n\n\n# Графики диагностики остатков\npar(mfrow = c(2, 2))\nplot(glm_gamma_fit)  # Стандартные диагностические графики GLM\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n# Диагностика остатков GLM с использованием DHARMa\nsim_glm &lt;- simulateResiduals(glm_gamma_fit, n = 1000, refit = FALSE)\nplot(sim_glm, main = \"GLM\")\n\n\n\n\n\n\n\n# Расчет и визуализация индексов\nidx_glm &lt;- emmeans_standardized_index(glm_gamma_fit) %&gt;%\n  mutate(model = \"GLM_Gamma\",\n         index_mean = scale_to_index(value, \"mean\"),\n         index_first = scale_to_index(value, \"first\"))\n\n# Добавление доверительных интервалов\nidx_glm &lt;- idx_glm %&gt;%\n  mutate(\n    lcl_index_mean = scale_to_index(lcl, \"mean\"),\n    ucl_index_mean = scale_to_index(ucl, \"mean\")\n  )\n\n# Визуализация индексов GLM\n\nidx_glm %&gt;%\n  ggplot(aes(x = YEAR, y = value, group = 1)) +\n  geom_line() +\n  geom_point() +\n  geom_ribbon(aes(ymin = lcl, ymax = ucl), alpha = 0.3) +\n  geom_point(data = actual_medians, \n           aes(x = YEAR, y = median_cpue), \n           shape = 4,  # 4 соответствует крестику (x)\n           size = 3, \n           color = \"black\", \n           inherit.aes = FALSE)+\nlabs(title = \"Индексы CPUE по GLM модели (крестики - факт)\", \n       x = \"Год\", \n       y = \"Стандартизированный индекс\")\n\n\n\n\n\n\n\n# ==============================================================================\n# БЛОК 5: МОДЕЛИРОВАНИЕ GAM\n# ==============================================================================\n\n# Подбор обобщенной аддитивной модели\ngam_fit &lt;- gam(\n  CPUE_POS ~ YEAR + MONTH + CALL + REGION,  # Линейная формула без сглаживания\n  family = Gamma(link = \"log\"),            # Аналогичное GLM распределение\n  method = \"REML\",                         # Метод оптимизации гиперпараметров\n  data = DATA\n)\n\nsummary(gam_fit)  # Сводка модели\n\n\nFamily: Gamma \nLink function: log \n\nFormula:\nCPUE_POS ~ YEAR + MONTH + CALL + REGION\n\nParametric coefficients:\n                                                  Estimate Std. Error t value\n(Intercept)                                        5.57646    0.07942  70.217\nYEAR2020                                          -0.22712    0.04586  -4.953\nYEAR2021                                          -0.22438    0.04557  -4.924\nYEAR2022                                          -0.64329    0.04345 -14.806\nYEAR2023                                          -0.77309    0.04647 -16.637\nYEAR2024                                          -1.12812    0.05156 -21.878\nMONTH10                                           -0.13725    0.02443  -5.617\nMONTH11                                           -0.13714    0.03540  -3.874\nCALLUAAK                                          -0.29528    0.06092  -4.847\nCALLUAKC                                          -0.53484    0.06381  -8.381\nCALLUBEV                                          -3.67226    0.12187 -30.133\nCALLUBQQ                                          -0.35427    0.07057  -5.020\nCALLUBSR                                          -0.33512    0.06266  -5.348\nCALLUBUR                                          -0.58242    0.06175  -9.432\nCALLUBYT                                          -0.21516    0.06088  -3.534\nCALLUCFF                                          -0.06750    0.09825  -0.687\nCALLUCXF                                          -2.34296    0.10638 -22.025\nCALLUDII                                          -0.46282    0.05710  -8.106\nCALLUDUT                                          -1.02155    0.07550 -13.531\nCALLUDWM                                          -2.42497    0.09479 -25.581\nCALLUEBK                                           0.25858    0.13371   1.934\nCALLUEMO                                          -0.17018    0.08103  -2.100\nCALLUENZ                                           0.04934    0.06442   0.766\nCALLUFIK                                           0.29706    0.13043   2.278\nCALLUGXE                                          -0.56375    0.06265  -8.998\nCALLUIVO                                          -0.21565    0.06500  -3.318\nREGIONCEB.-ЦEHTPAЛЬHЫЙ P-H                         0.16508    0.46672   0.354\nREGIONCEB.CKЛOH MУPMAHCKOГO MEЛKOBOДЬЯ            -0.07627    0.15513  -0.492\nREGIONCEBEPO-KAHИHCKAЯ БAHKA                       0.16322    0.08236   1.982\nREGIONKAHИHCKAЯ БAHKA                              0.06281    0.07875   0.798\nREGIONKAHИHCKO- KOЛГУEBCKOE MEЛKOBOДЬE(CEB.CKЛOH)  0.21299    0.08519   2.500\nREGIONKAHИHCKO-KOЛГУEBCKOE MEЛKOBOДЬE              0.18213    0.07757   2.348\nREGIONMУPMAHCKOE MEЛKOBOДЬE                        0.07728    0.07753   0.997\nREGIONЗAП.-ПPИБPEЖHЫЙ P-H                          0.72877    0.46327   1.573\nREGIONЗAП.-ЦEHTPAЛЬHЫЙ P-H                         0.13318    0.08416   1.583\nREGIONЗAП.CKЛOH ГУCИHOЙ БAHKИ                      0.64226    0.11646   5.515\nREGIONЮЖ.CKЛOH ГУCИHOЙ БAHKИ                       0.69310    0.13068   5.304\n                                                  Pr(&gt;|t|)    \n(Intercept)                                        &lt; 2e-16 ***\nYEAR2020                                          7.62e-07 ***\nYEAR2021                                          8.85e-07 ***\nYEAR2022                                           &lt; 2e-16 ***\nYEAR2023                                           &lt; 2e-16 ***\nYEAR2024                                           &lt; 2e-16 ***\nMONTH10                                           2.08e-08 ***\nMONTH11                                           0.000109 ***\nCALLUAAK                                          1.30e-06 ***\nCALLUAKC                                           &lt; 2e-16 ***\nCALLUBEV                                           &lt; 2e-16 ***\nCALLUBQQ                                          5.39e-07 ***\nCALLUBSR                                          9.39e-08 ***\nCALLUBUR                                           &lt; 2e-16 ***\nCALLUBYT                                          0.000414 ***\nCALLUCFF                                          0.492100    \nCALLUCXF                                           &lt; 2e-16 ***\nCALLUDII                                          6.98e-16 ***\nCALLUDUT                                           &lt; 2e-16 ***\nCALLUDWM                                           &lt; 2e-16 ***\nCALLUEBK                                          0.053187 .  \nCALLUEMO                                          0.035785 *  \nCALLUENZ                                          0.443735    \nCALLUFIK                                          0.022810 *  \nCALLUGXE                                           &lt; 2e-16 ***\nCALLUIVO                                          0.000916 ***\nREGIONCEB.-ЦEHTPAЛЬHЫЙ P-H                        0.723578    \nREGIONCEB.CKЛOH MУPMAHCKOГO MEЛKOBOДЬЯ            0.623006    \nREGIONCEBEPO-KAHИHCKAЯ БAHKA                      0.047563 *  \nREGIONKAHИHCKAЯ БAHKA                             0.425185    \nREGIONKAHИHCKO- KOЛГУEBCKOE MEЛKOBOДЬE(CEB.CKЛOH) 0.012456 *  \nREGIONKAHИHCKO-KOЛГУEBCKOE MEЛKOBOДЬE             0.018931 *  \nREGIONMУPMAHCKOE MEЛKOBOДЬE                       0.318981    \nREGIONЗAП.-ПPИБPEЖHЫЙ P-H                         0.115776    \nREGIONЗAП.-ЦEHTPAЛЬHЫЙ P-H                        0.113615    \nREGIONЗAП.CKЛOH ГУCИHOЙ БAHKИ                     3.72e-08 ***\nREGIONЮЖ.CKЛOH ГУCИHOЙ БAHKИ                      1.20e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.397   Deviance explained = 40.1%\n-REML =  21455  Scale est. = 0.41722   n = 3891\n\n# Проверка адекватности модели (графики остатков)\nmgcv::gam.check(gam_fit)\n\n\n\n\n\n\n\n\n\nMethod: REML   Optimizer: outer newton\nfull convergence after 5 iterations.\nGradient range [-0.0003574844,-0.0003574844]\n(score 21454.84 & scale 0.4172217).\nHessian positive definite, eigenvalue range [2198.061,2198.061].\nModel rank =  37 / 37 \n\n# Диагностика остатков GAM с использованием DHARMa\nsim_gam &lt;- simulateResiduals(gam_fit, n = 1000, refit = FALSE)\n\nRegistered S3 method overwritten by 'mgcViz':\n  method from   \n  +.gg   ggplot2\n\nplot(sim_gam, main = \"GAM\")\n\n\n\n\n\n\n\n# Расчет индексов\nidx_gam &lt;- emmeans_standardized_index(gam_fit) %&gt;%\n  mutate(model = \"GAM\",\n         index_mean = scale_to_index(value, \"mean\"),\n         index_first = scale_to_index(value, \"first\"))\n\n# Доверительные интервалы\nidx_gam &lt;- idx_gam %&gt;%\n  mutate(\n    lcl_index_mean = scale_to_index(lcl, \"mean\"),\n    ucl_index_mean = scale_to_index(ucl, \"mean\")\n  )\n\n\n# Визуализация\nidx_gam %&gt;%\n  ggplot(aes(x = YEAR, y = value, group = 1)) +\n  geom_line() +\n  geom_point() +\n  geom_ribbon(aes(ymin = lcl, ymax = ucl), alpha = 0.3) +\n  geom_point(data = actual_medians, \n           aes(x = YEAR, y = median_cpue), \n           shape = 4,  # 4 соответствует крестику (x)\n           size = 3, \n           color = \"black\", \n           inherit.aes = FALSE)+\n  labs(title = \"Индексы CPUE по GAM модели (крестики - факт\", \n       x = \"Год\", \n       y = \"Стандартизированный индекс\")\n\n\n\n\n\n\n\n# ==============================================================================\n# БЛОК 6: МОДЕЛИРОВАНИЕ GAMM (СМЕШАННАЯ МОДЕЛЬ)\n# ==============================================================================\n\n# Подбор модели со смешанными эффектами\ngamm_fit &lt;- gamm4::gamm4(\n  formula = CPUE_POS ~ YEAR + MONTH + REGION,  # Фиксированные эффекты\n  random = ~ (1 | CALL),                       # Случайный эффект для судна\n  family = Gamma(link = \"log\"),               # Распределение\n  data = DATA\n)\n\n# 1. График остатков от предсказанных значений\nplot(fitted(gamm_fit$gam), residuals(gamm_fit$gam, type = \"deviance\"),\n     xlab = \"Предсказанные значения\", ylab = \"Девиансные остатки\",\n     main = \"Остатки GAMM vs. Предсказания\")\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n# 2. QQ-plot для остатков\nqqnorm(residuals(gamm_fit$gam, type = \"deviance\"),\n       main = \"QQ-plot для остатков GAMM\")\nqqline(residuals(gamm_fit$gam, type = \"deviance\"), col = \"red\")\n\n\n\n\n\n\n\n# 3. Диагностика случайных эффектов\ncat(\"\\nСлучайные эффекты (CALL):\\n\")\n\n\nСлучайные эффекты (CALL):\n\nprint(summary(ranef(gamm_fit$mer)$CALL))\n\n  (Intercept)      \n Min.   :-2.92229  \n 1st Qu.: 0.09295  \n Median : 0.32996  \n Mean   : 0.00306  \n 3rd Qu.: 0.54065  \n Max.   : 0.93271  \n\n# График случайных эффектов\nrandom_effects &lt;- ranef(gamm_fit$mer)$CALL\nplot(density(random_effects[,1]), main = \"Распределение случайных эффектов\",\n     xlab = \"Случайный эффект\", ylab = \"Плотность\")\n\n\n\n\n\n\n\n# 5. Проверка гетероскедастичности\nlibrary(lmtest)\n\nЗагрузка требуемого пакета: zoo\n\n\n\nПрисоединяю пакет: 'zoo'\n\n\nСледующие объекты скрыты от 'package:base':\n\n    as.Date, as.Date.numeric\n\nbptest(gamm_fit$gam$y ~ fitted(gamm_fit$gam)) %&gt;% \n  print()\n\n\n    studentized Breusch-Pagan test\n\ndata:  gamm_fit$gam$y ~ fitted(gamm_fit$gam)\nBP = 222.14, df = 1, p-value &lt; 2.2e-16\n\n# 6. Сводка по модели\ncat(\"\\nСводка GAMM модели:\\n\")\n\n\nСводка GAMM модели:\n\nprint(summary(gamm_fit$gam))\n\n\nFamily: Gamma \nLink function: log \n\nFormula:\nCPUE_POS ~ YEAR + MONTH + REGION\n\nParametric coefficients:\n                                                  Estimate Std. Error t value\n(Intercept)                                        4.91649    0.17171  28.632\nYEAR2020                                          -0.23162    0.04580  -5.058\nYEAR2021                                          -0.22798    0.04552  -5.008\nYEAR2022                                          -0.64763    0.04338 -14.928\nYEAR2023                                          -0.77570    0.04641 -16.716\nYEAR2024                                          -1.13070    0.05152 -21.947\nMONTH10                                           -0.13620    0.02445  -5.571\nMONTH11                                           -0.13630    0.03542  -3.848\nREGIONCEB.-ЦEHTPAЛЬHЫЙ P-H                         0.16149    0.46710   0.346\nREGIONCEB.CKЛOH MУPMAHCKOГO MEЛKOBOДЬЯ            -0.08013    0.15524  -0.516\nREGIONCEBEPO-KAHИHCKAЯ БAHKA                       0.15971    0.08238   1.939\nREGIONKAHИHCKAЯ БAHKA                              0.05886    0.07877   0.747\nREGIONKAHИHCKO- KOЛГУEBCKOE MEЛKOBOДЬE(CEB.CKЛOH)  0.20859    0.08522   2.448\nREGIONKAHИHCKO-KOЛГУEBCKOE MEЛKOBOДЬE              0.17834    0.07759   2.299\nREGIONMУPMAHCKOE MEЛKOBOДЬE                        0.07353    0.07755   0.948\nREGIONЗAП.-ПPИБPEЖHЫЙ P-H                          0.72926    0.46366   1.573\nREGIONЗAП.-ЦEHTPAЛЬHЫЙ P-H                         0.12967    0.08419   1.540\nREGIONЗAП.CKЛOH ГУCИHOЙ БAHKИ                      0.63836    0.11653   5.478\nREGIONЮЖ.CKЛOH ГУCИHOЙ БAHKИ                       0.68988    0.13078   5.275\n                                                  Pr(&gt;|t|)    \n(Intercept)                                        &lt; 2e-16 ***\nYEAR2020                                          4.44e-07 ***\nYEAR2021                                          5.74e-07 ***\nYEAR2022                                           &lt; 2e-16 ***\nYEAR2023                                           &lt; 2e-16 ***\nYEAR2024                                           &lt; 2e-16 ***\nMONTH10                                           2.70e-08 ***\nMONTH11                                           0.000121 ***\nREGIONCEB.-ЦEHTPAЛЬHЫЙ P-H                        0.729566    \nREGIONCEB.CKЛOH MУPMAHCKOГO MEЛKOBOДЬЯ            0.605763    \nREGIONCEBEPO-KAHИHCKAЯ БAHKA                      0.052628 .  \nREGIONKAHИHCKAЯ БAHKA                             0.455013    \nREGIONKAHИHCKO- KOЛГУEBCKOE MEЛKOBOДЬE(CEB.CKЛOH) 0.014424 *  \nREGIONKAHИHCKO-KOЛГУEBCKOE MEЛKOBOДЬE             0.021580 *  \nREGIONMУPMAHCKOE MEЛKOBOДЬE                       0.343099    \nREGIONЗAП.-ПPИБPEЖHЫЙ P-H                         0.115842    \nREGIONЗAП.-ЦEHTPAЛЬHЫЙ P-H                        0.123604    \nREGIONЗAП.CKЛOH ГУCИHOЙ БAHKИ                     4.57e-08 ***\nREGIONЮЖ.CKЛOH ГУCИHOЙ БAHKИ                      1.40e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.172   \nglmer.ML =   1786  Scale est. = 0.41793   n = 3891\n\nprint(summary(gamm_fit$mer))\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: Gamma  ( log )\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n  42933.5   43065.1  -21445.7   42891.5      3870 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5364 -0.6769 -0.1721  0.4716 11.0577 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n CALL     (Intercept) 0.4320   0.6573  \n Residual             0.4179   0.6465  \nNumber of obs: 3891, groups:  CALL, 19\n\nFixed effects:\n                                                   Estimate Std. Error t value\nX(Intercept)                                        4.91649    0.23488  20.932\nXYEAR2020                                          -0.23162    0.02474  -9.363\nXYEAR2021                                          -0.22798    0.02513  -9.073\nXYEAR2022                                          -0.64763    0.02299 -28.167\nXYEAR2023                                          -0.77570    0.02376 -32.652\nXYEAR2024                                          -1.13070    0.02603 -43.443\nXMONTH10                                           -0.13620    0.01914  -7.116\nXMONTH11                                           -0.13630    0.02359  -5.777\nXREGIONCEB.-ЦEHTPAЛЬHЫЙ P-H                         0.16149    0.46456   0.348\nXREGIONCEB.CKЛOH MУPMAHCKOГO MEЛKOBOДЬЯ            -0.08013    0.03506  -2.285\nXREGIONCEBEPO-KAHИHCKAЯ БAHKA                       0.15971    0.02670   5.982\nXREGIONKAHИHCKAЯ БAHKA                              0.05886    0.02653   2.218\nXREGIONKAHИHCKO- KOЛГУEBCKOE MEЛKOBOДЬE(CEB.CKЛOH)  0.20859    0.02808   7.428\nXREGIONKAHИHCKO-KOЛГУEBCKOE MEЛKOBOДЬE              0.17834    0.02200   8.108\nXREGIONMУPMAHCKOE MEЛKOBOДЬE                        0.07353    0.02318   3.172\nXREGIONЗAП.-ПPИБPEЖHЫЙ P-H                          0.72926    0.46533   1.567\nXREGIONЗAП.-ЦEHTPAЛЬHЫЙ P-H                         0.12967    0.02736   4.739\nXREGIONЗAП.CKЛOH ГУCИHOЙ БAHKИ                      0.63836    0.03291  19.400\nXREGIONЮЖ.CKЛOH ГУCИHOЙ БAHKИ                       0.68988    0.03447  20.015\n                                                   Pr(&gt;|z|)    \nX(Intercept)                                        &lt; 2e-16 ***\nXYEAR2020                                           &lt; 2e-16 ***\nXYEAR2021                                           &lt; 2e-16 ***\nXYEAR2022                                           &lt; 2e-16 ***\nXYEAR2023                                           &lt; 2e-16 ***\nXYEAR2024                                           &lt; 2e-16 ***\nXMONTH10                                           1.11e-12 ***\nXMONTH11                                           7.60e-09 ***\nXREGIONCEB.-ЦEHTPAЛЬHЫЙ P-H                         0.72813    \nXREGIONCEB.CKЛOH MУPMAHCKOГO MEЛKOBOДЬЯ             0.02229 *  \nXREGIONCEBEPO-KAHИHCKAЯ БAHKA                      2.20e-09 ***\nXREGIONKAHИHCKAЯ БAHKA                              0.02654 *  \nXREGIONKAHИHCKO- KOЛГУEBCKOE MEЛKOBOДЬE(CEB.CKЛOH) 1.10e-13 ***\nXREGIONKAHИHCKO-KOЛГУEBCKOE MEЛKOBOДЬE             5.16e-16 ***\nXREGIONMУPMAHCKOE MEЛKOBOДЬE                        0.00151 ** \nXREGIONЗAП.-ПPИБPEЖHЫЙ P-H                          0.11707    \nXREGIONЗAП.-ЦEHTPAЛЬHЫЙ P-H                        2.15e-06 ***\nXREGIONЗAП.CKЛOH ГУCИHOЙ БAHKИ                      &lt; 2e-16 ***\nXREGIONЮЖ.CKЛOH ГУCИHOЙ БAHKИ                       &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCorrelation matrix not shown by default, as p = 19 &gt; 12.\nUse print(summary(gamm_fit$mer), correlation=TRUE)  or\n    vcov(summary(gamm_fit$mer))        if you need it\n\n# Создание сетки для предсказания\nnewdata_grid &lt;- expand.grid(\n  YEAR = levels(DATA$YEAR),\n  MONTH = levels(DATA$MONTH),\n  REGION = levels(DATA$REGION),\n  CALL = levels(DATA$CALL)[1]  # Фиксированное значение для случайного эффекта\n)\n\n# Предсказание на сетке\nnewdata_grid$pred &lt;- predict(gamm_fit$gam, \n                            newdata = newdata_grid, \n                            type = \"response\")\n\n# Усреднение предсказаний по годам\nidx_gamm &lt;- newdata_grid %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(value = mean(pred, na.rm = TRUE)) %&gt;%\n  mutate(\n    model = \"GAMM (mgcv)\",\n    index_mean = scale_to_index(value, \"mean\"),\n    index_first = scale_to_index(value, \"first\")\n  )\n\n# Функция расчета доверительных интервалов через бутстреп\ncompute_gamm_ci &lt;- function(model, newdata, n_boot = 100) {\n  boot_means &lt;- replicate(n_boot, {\n    boot_data &lt;- newdata[sample(nrow(newdata), replace = TRUE), ]\n    preds &lt;- predict(model, newdata = boot_data, type = \"response\")\n    boot_data %&gt;%\n      mutate(pred = preds) %&gt;%\n      group_by(YEAR) %&gt;%\n      summarise(mean_pred = mean(pred, na.rm = TRUE)) %&gt;%\n      pull(mean_pred)\n  })\n  \n  ci &lt;- apply(boot_means, 1, function(x) quantile(x, c(0.025, 0.975), na.rm = TRUE))\n  return(list(mean = rowMeans(boot_means), lcl = ci[1, ], ucl = ci[2, ]))\n}\n\n# Расчет интервалов\ngamm_ci &lt;- compute_gamm_ci(gamm_fit$gam, newdata_grid)\n\n# Добавление интервалов к индексам\nidx_gamm &lt;- idx_gamm %&gt;%\n  mutate(\n    lcl = gamm_ci$lcl,\n    ucl = gamm_ci$ucl,\n    lcl_index_mean = scale_to_index(lcl, \"mean\"),\n    ucl_index_mean = scale_to_index(ucl, \"mean\")\n  )\n\n# Визуализация\nidx_gamm %&gt;%\n  ggplot(aes(x = YEAR, y = value, group = 1)) +\n  geom_line() +\n  geom_point() +\n  geom_ribbon(aes(ymin = lcl, ymax = ucl), alpha = 0.3) +\n  geom_point(data = actual_medians, \n           aes(x = YEAR, y = median_cpue), \n           shape = 4,  # 4 соответствует крестику (x)\n           size = 3, \n           color = \"black\", \n           inherit.aes = FALSE)+\nlabs(title = \"Индексы CPUE по GAMM модели (крестики - факт)\", \n       x = \"Год\", \n       y = \"Стандартизированный индекс\")\n\n\n\n\n\n\n\n# ==============================================================================\n# БЛОК 7: СРАВНЕНИЕ МОДЕЛЕЙ И ФИНАЛЬНАЯ ВИЗУАЛИЗАЦИЯ\n# ==============================================================================\n\n# Объединение результатов всех моделей\nindices_all &lt;- bind_rows(\n  idx_glm %&gt;% select(YEAR, value, lcl, ucl, model, index_mean, lcl_index_mean, ucl_index_mean),\n  idx_gam %&gt;% select(YEAR, value, lcl, ucl, model, index_mean, lcl_index_mean, ucl_index_mean),\n  idx_gamm %&gt;% select(YEAR, value, lcl, ucl, model, index_mean, lcl_index_mean, ucl_index_mean)\n) %&gt;% mutate(YEAR = factor(YEAR, levels = levels(DATA$YEAR)))\n\n# Сводная таблица результатов\nindices_all %&gt;% \n  kable(caption = \"Сравнение индексов CPUE по разным моделям\")\n\n\nСравнение индексов CPUE по разным моделям\n\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nvalue\nlcl\nucl\nmodel\nindex_mean\nlcl_index_mean\nucl_index_mean\n\n\n\n\n2019\n158.81216\n138.93290\n181.53585\nGLM_Gamma\n1.5358638\n1.5299542\n1.5417869\n\n\n2020\n126.54457\n111.15088\n144.07018\nGLM_Gamma\n1.2238057\n1.2240136\n1.2235904\n\n\n2021\n126.89292\n111.57409\n144.31498\nGLM_Gamma\n1.2271746\n1.2286741\n1.2256695\n\n\n2022\n83.46580\n73.52525\n94.75032\nGLM_Gamma\n0.8071933\n0.8096733\n0.8047160\n\n\n2023\n73.30390\n64.40714\n83.42960\nGLM_Gamma\n0.7089181\n0.7092631\n0.7085690\n\n\n2024\n51.39565\n45.26094\n58.36186\nGLM_Gamma\n0.4970445\n0.4984216\n0.4956683\n\n\n2019\n158.81218\n138.93304\n181.53572\nGAM\n1.5358554\n1.5299458\n1.5417784\n\n\n2020\n126.54503\n111.15138\n144.07058\nGAM\n1.2238033\n1.2240112\n1.2235880\n\n\n2021\n126.89280\n111.57408\n144.31473\nGAM\n1.2271665\n1.2286660\n1.2256615\n\n\n2022\n83.46561\n73.52514\n94.75002\nGAM\n0.8071869\n0.8096669\n0.8047096\n\n\n2023\n73.30495\n64.40811\n83.43072\nGAM\n0.7089242\n0.7092692\n0.7085751\n\n\n2024\n51.39792\n45.26297\n58.36439\nGAM\n0.4970637\n0.4984408\n0.4956874\n\n\n2019\n165.88930\n153.27889\n186.43794\nGAMM (mgcv)\n1.5400976\n1.5751123\n1.5695052\n\n\n2020\n131.59142\n116.54705\n144.02703\nGAMM (mgcv)\n1.2216800\n1.1976514\n1.2124741\n\n\n2021\n132.07110\n118.64017\n145.21926\nGAMM (mgcv)\n1.2261333\n1.2191606\n1.2225107\n\n\n2022\n86.80692\n79.26688\n96.71028\nGAMM (mgcv)\n0.8059057\n0.8145559\n0.8141438\n\n\n2023\n76.37202\n67.94329\n82.24327\nGAMM (mgcv)\n0.7090292\n0.6981933\n0.6923550\n\n\n2024\n53.55022\n48.20170\n58.08852\nGAMM (mgcv)\n0.4971542\n0.4953264\n0.4890111\n\n\n\n\nindices_all %&gt;%\n  ggplot(aes(x = YEAR, y = value, color = model, group = model, fill = model)) +\n  geom_line() +\n  geom_point() +\n  geom_ribbon(aes(ymin = lcl, ymax = ucl), alpha = 0.1, linetype = \"dashed\") +\ngeom_point(data = actual_medians, \n           aes(x = YEAR, y = median_cpue), \n           shape = 4,  # 4 соответствует крестику (x)\n           size = 3, \n           color = \"black\", \n           inherit.aes = FALSE)+\n  labs(title = \"Сравнение стандартизированных индексов CPUE (крестики - факт)\", \n       x = \"Год\", \n       y = \"Индекс CPUE (кг/ловушку)\", \n       color = \"Модель\", \n       fill = \"Модель\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n# ==============================================================================\n# БЛОК 9: СРАВНЕНИЕ МОДЕЛЕЙ ПО ИНФОРМАЦИОННЫМ КРИТЕРИЯМ\n# ==============================================================================\n\ncat(\"\\n=== СРАВНИТЕЛЬНЫЙ АНАЛИЗ МОДЕЛЕЙ ПО ИНФОРМАЦИОННЫМ КРИТЕРИЯМ ===\\n\")\n\n\n=== СРАВНИТЕЛЬНЫЙ АНАЛИЗ МОДЕЛЕЙ ПО ИНФОРМАЦИОННЫМ КРИТЕРИЯМ ===\n\n# Упрощенная функция для извлечения ключевых критериев из моделей\nextract_model_metrics &lt;- function(model, model_name, model_type = \"glm\") {\n  if (model_type == \"glm\") {\n    aic_val &lt;- AIC(model)\n    bic_val &lt;- BIC(model)\n    loglik_val &lt;- as.numeric(logLik(model))\n    df_val &lt;- model$rank\n    null_dev &lt;- model$null.deviance\n    dev &lt;- model$deviance\n  } else if (model_type == \"gam\") {\n    aic_val &lt;- AIC(model)\n    bic_val &lt;- BIC(model)\n    loglik_val &lt;- as.numeric(logLik(model))\n    df_val &lt;- sum(model$edf)\n    null_dev &lt;- model$null.deviance\n    dev &lt;- model$deviance\n  } else if (model_type == \"gamm\") {\n    aic_val &lt;- AIC(model$mer)\n    bic_val &lt;- BIC(model$mer)\n    loglik_val &lt;- as.numeric(logLik(model$mer))\n    df_val &lt;- length(fixef(model$mer)) + 1  # +1 для случайного эффекта\n    null_dev &lt;- model$gam$null.deviance\n    dev &lt;- model$gam$deviance\n  }\n  \n  # Вычисляем долю объясненной девиации\n  deviance_explained &lt;- ifelse(!is.null(null_dev) && !is.null(dev) && null_dev &gt; 0,\n                              (null_dev - dev) / null_dev, NA)\n  \n  data.frame(\n    Model = model_name,\n    AIC = round(aic_val, 2),\n    BIC = round(bic_val, 2),\n    LogLik = round(loglik_val, 2),\n    DF = round(df_val, 2),\n    Deviance_Explained = round(deviance_explained, 4)\n  )\n}\n\n# Извлекаем метрики для всех моделей\nmodel_metrics &lt;- bind_rows(\n  extract_model_metrics(glm_gamma_fit, \"GLM (Gamma)\", \"glm\"),\n  extract_model_metrics(gam_fit, \"GAM\", \"gam\"),\n  extract_model_metrics(gamm_fit, \"GAMM\", \"gamm\")\n)\n\n# Добавляем разницу в AIC относительно наилучшей модели\nmin_aic &lt;- min(model_metrics$AIC)\nmodel_metrics &lt;- model_metrics %&gt;%\n  mutate(Delta_AIC = AIC - min_aic,\n         AIC_Weight = exp(-0.5 * Delta_AIC) / sum(exp(-0.5 * Delta_AIC)))\n\n# Форматируем таблицу для вывода\ncomparison_table &lt;- model_metrics %&gt;%\n  mutate(across(where(is.numeric), ~round(., 3))) %&gt;%\n  arrange(AIC)  # Сортируем по AIC (лучшая модель первая)\n\n# Выводим таблицу сравнения\ncat(\"\\nТАБЛИЦА СРАВНЕНИЯ МОДЕЛЕЙ:\\n\")\n\n\nТАБЛИЦА СРАВНЕНИЯ МОДЕЛЕЙ:\n\nprint(comparison_table)\n\n        Model      AIC      BIC    LogLik DF Deviance_Explained Delta_AIC\n1         GAM 42840.91 43079.03 -21382.45 37              0.401      0.00\n2 GLM (Gamma) 42850.76 43088.88 -21387.38 37              0.401      9.85\n3        GAMM 42933.49 43065.09 -21445.75 20                 NA     92.58\n  AIC_Weight\n1      0.993\n2      0.007\n3      0.000\n\n# Выводим итоговые рекомендации\ncat(\"\\n=== ИТОГОВЫЕ РЕКОМЕНДАЦИИ ПО ВЫБОРУ МОДЕЛИ ===\\n\")\n\n\n=== ИТОГОВЫЕ РЕКОМЕНДАЦИИ ПО ВЫБОРУ МОДЕЛИ ===\n\nbest_model &lt;- comparison_table$Model[1]\ncat(\"Наилучшая модель по критерию AIC:\", best_model, \"\\n\")\n\nНаилучшая модель по критерию AIC: GAM \n\ncat(\"Вес AIC для наилучшей модели:\", round(comparison_table$AIC_Weight[1], 3), \"\\n\")\n\nВес AIC для наилучшей модели: 0.993 \n\nif (nrow(comparison_table) &gt; 1 && comparison_table$Delta_AIC[2] &gt; 2) {\n  cat(\"Наилучшая модель существенно лучше остальных (?AIC &gt; 2).\\n\")\n} else if (nrow(comparison_table) &gt; 1) {\n  cat(\"Несколько моделей имеют сходное качество (?AIC &lt; 2).\\n\")\n}\n\nНаилучшая модель существенно лучше остальных (?AIC &gt; 2).\n\ncat(\"Доля объясненной девиации наилучшей модели:\", \n    round(comparison_table$Deviance_Explained[1], 3), \"\\n\")\n\nДоля объясненной девиации наилучшей модели: 0.401",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Стандартизация CPUE: GLM, GAM, GAMM</span>"
    ]
  },
  {
    "objectID": "chapter 9.html#анализ-glm-модели-с-гамма-распределением-и-лог-ссылкой",
    "href": "chapter 9.html#анализ-glm-модели-с-гамма-распределением-и-лог-ссылкой",
    "title": "10  Стандартизация CPUE: GLM, GAM, GAMM",
    "section": "10.3 Анализ GLM модели с гамма-распределением и лог-ссылкой",
    "text": "10.3 Анализ GLM модели с гамма-распределением и лог-ссылкой\nПодобранная обобщенная линейная модель (GLM) использует гамма-распределение для ошибок и логарифмическую функцию связи. Данное распределение выбрано, поскольку CPUE представляет собой непрерывную положительную величину, а гамма-распределение хорошо описывает такие данные. Логарифмическая связь обеспечивает мультипликативность эффектов факторов, что интерпретируется как относительное изменение CPUE при изменении фактора. Модель включает четыре факторные переменные: год (YEAR), месяц (MONTH), позывной судна (CALL) и район промысла (REGION). Все переменные представлены как факторы, что означает, что для каждого уровня фактора оценивается свой коэффициент, интерпретируемый как отклонение от базового уровня. Базовыми уровнями являются: 2019 год для YEAR, сентябрь (MONTH9) для MONTH, первое судно в алфавитном порядке для CALL и первый район для REGION. Из сводки модели видно, что многие коэффициенты статистически значимы. Все годовые коэффициенты отрицательны и значимы, что указывает на снижение CPUE относительно базового 2019 года. Наибольшее снижение наблюдается в 2024 году (коэффициент -1.128). Месячные коэффициенты также отрицательны и значимы, что говорит о снижении CPUE в октябре и ноябре по сравнению с сентябрем. Большинство коэффициентов для судов значимы и отрицательны, что указывает на то, что уловы на усилие у этих судов в среднем ниже, чем у базового судна. Однако некоторые суда имеют положительные коэффициенты, что означает более высокую производительность. Для районов значимыми оказались лишь некоторые коэффициенты, в основном положительные, что говорит о более высоких уловах в этих районах по сравнению с базовым. Дисперсионный параметр для гамма-семейства равен 0.417, что указывает на умеренную дисперсию. Null deviance составляет 2980.2 при 3890 степенях свободы, а остаточная deviance — 1785.6 при 3854 степенях свободы. Снижение девиации указывает на то, что модель объясняет существенную часть вариации данных. AIC модели равен 42851. Диагностические графики стандартных остатков GLM включают график остатков против предсказанных значений, Q-Q plot, график масштаба-местоположения и график остатков против влияния. Эти графики позволяют оценить гомоскедастичность, нормальность остатков и наличие выбросов. Дополнительная диагностика с помощью пакета DHARMa показывает, что распределение остатков соответствует ожидаемому, что подтверждает адекватность выбранного семейства распределений. Расчет стандартизированных индексов с помощью функции emmeans показывает, что индекс CPUE постепенно снижается с 2019 по 2024 год, что согласуется с отрицательными годовыми коэффициентами. В 2019 году индекс составляет 159, а к 2024 падает до 51.4. Доверительные интервалы не перекрываются между крайними годами, что указывает на статистически значимое снижение. Сравнение с медианными значениями CPUE по годам из исходных данных показывает, что модель несколько сглаживает исходные данные, но общая тенденция снижения сохраняется. Например, в 2019 году медианное значение CPUE было 200, а стандартизированный индекс — 159, что может быть связано с учетом влияния других факторов. Преимущества GLM подхода включают простоту интерпретации коэффициентов, вычислительную эффективность и широкую распространенность. Недостатки заключаются в том, что GLM предполагает линейность влияния факторов на логарифм отклика, что может не всегда выполняться. Кроме того, модель с фиксированными эффектами может не учитывать некоторые источники вариации, такие как пространственно-временная автокорреляция или случайные эффекты судов. В целом, модель адекватно описывает данные и может быть использована для стандартизации CPUE, но для более сложных данных могут потребоваться более гибкие модели, такие как GAM или GAMM.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Стандартизация CPUE: GLM, GAM, GAMM</span>"
    ]
  },
  {
    "objectID": "chapter 9.html#анализ-gam-модели-с-гамма-распределением-и-логарифмической-связью",
    "href": "chapter 9.html#анализ-gam-модели-с-гамма-распределением-и-логарифмической-связью",
    "title": "10  Стандартизация CPUE: GLM, GAM, GAMM",
    "section": "10.4 Анализ GAM модели с гамма-распределением и логарифмической связью",
    "text": "10.4 Анализ GAM модели с гамма-распределением и логарифмической связью\nАнализ подобранной обобщенной аддитивной модели (GAM) с гамма-распределением и логарифмической связью показывает результаты, практически идентичные полученным ранее для GLM модели, что ожидаемо, поскольку в данной реализации GAM использовалась полностью параметрическая формула без сглаживающих функций. Модель была построена с теми же предикторами - годом, месяцем, идентификатором судна и районом промысла.\nСводка модели демонстрирует параметрические коэффициенты, которые практически не отличаются от оценок GLM модели. Все годовые коэффициенты остаются отрицательными и статистически значимыми, подтверждая устойчивую тенденцию снижения стандартизированного индекса CPUE с 2019 по 2024 год. Месячные коэффициенты также значимы и отрицательны, указывая на сезонное снижение уловов в октябре и ноябре по сравнению с сентябрем. Оценки для различных судов и районов промысла практически идентичны полученным в GLM, с сохранением статистической значимости для тех же уровней факторов.\nМодель объясняет 40.1% девиации данных, что полностью соответствует показателю GLM. Значение REML составляет 21455, а оценка дисперсии равна 0.41722, что также практически совпадает с соответствующими показателями GLM модели.\nПроверка адекватности модели с помощью функции gam.check показывает успешную сходимость алгоритма оптимизации после 5 итераций. Градиент близок к нулю, а гессиан положительно определен, что свидетельствует о достижении устойчивого решения. Поскольку в модели отсутствуют сглаживающие компоненты, диагностика не выявляет проблем, связанных с выбором базовой размерности или неадекватностью сглаживания.\nДиагностика остатков с использованием пакета DHARMa показывает равномерное распределение без систематических паттернов, что указывает на соответствие остатков теоретическому гамма-распределению. Графики остатков демонстрируют отсутствие гетероскедастичности и значимых выбросов, что подтверждает адекватность модели.\nРасчет стандартизированных индексов методом маргинальных средних дает значения, практически идентичные полученным из GLM модели. Индекс снижается с 159 в 2019 году до 51.4 в 2024 году, с доверительными интервалами, не перекрывающимися между крайними годами. Нормированные индексы относительно среднего и первого года также полностью совпадают с GLM результатами.\nОсновное преимущество использования GAM в данном случае заключается в методологическом подходе - использовании метода REML для оптимизации, который может обеспечивать более стабильные оценки параметров по сравнению с методом максимального правдоподобия, используемым в GLM. Хотя в данной конкретной реализации с полностью параметрической формулой это преимущество не реализуется в полной мере, GAM предоставляет основу для легкого включения нелинейных эффектов через сглаживающие функции, если такая необходимость возникнет в дальнейшем.\nК недостаткам данного подхода можно отнести избыточную сложность GAM для полностью параметрической модели, поскольку вычислительные затраты выше, чем для GLM, без существенного улучшения качества подгонки. Фактически, в данном случае GAM работает как GLM, но с более сложным алгоритмом оптимизации. Кроме того, диагностика GAM требует дополнительных проверок, связанных со сходимостью алгоритма и адекватностью сглаживания, которые не актуальны для параметрических моделей.\nВ целом, данная реализация GAM не демонстрирует преимуществ перед GLM моделью, но предоставляет основу для будущего расширения модели за счет включения нелинейных эффектов, если анализ данных покажет такую необходимость. Результаты стандартизации CPUE полностью согласуются с полученными ранее средствами GLM.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Стандартизация CPUE: GLM, GAM, GAMM</span>"
    ]
  },
  {
    "objectID": "chapter 9.html#анализ-gamm-модели",
    "href": "chapter 9.html#анализ-gamm-модели",
    "title": "10  Стандартизация CPUE: GLM, GAM, GAMM",
    "section": "10.5 Анализ GAMM модели",
    "text": "10.5 Анализ GAMM модели\nОсобенности смешанных моделей и случайные эффекты\nСмешанные модели, включая GAMM, расширяют возможности стандартных моделей за счет введения случайных эффектов. В то время как фиксированные эффекты оценивают среднее влияние факторов на всю популяцию, случайные эффекты позволяют учесть вариацию, связанную с отдельными группами наблюдений. В ихтиологических исследованиях случайные эффекты часто применяются для учета индивидуальных особенностей судов, различий между районами промысла, или временной автокорреляции.\nСлучайные эффекты особенно полезны, когда:\n\nДанные имеют иерархическую структуру (например, уловы по нескольким судам)\nНаблюдения внутри групп коррелированы\nКоличество уровней фактора велико, и мы хотим обобщить выводы на всю популяцию групп\nНас интересует вариация между группами, а не конкретные сравнения между отдельными уровнями\n\nВ данном случае случайный эффект для судна (CALL) позволяет учесть, что разные суда могут иметь систематические различия в эффективности промысла, не объясняемые другими переменными модели.\nАнализ результатов GAMM модели\nАнализ обобщенной аддитивной смешанной модели показывает несколько важных особенностей. Модель включает фиксированные эффекты года, месяца и района, а также случайный эффект для судна, что позволяет учесть индивидуальные различия между судами в уровне уловов.\nГрафик остатков от предсказанных значений показывает распределение девиансных остатков вокруг нулевой линии. Наблюдается некоторая гетероскедастичность - разброс остатков увеличивается с ростом предсказанных значений, что характерно для данных по уловам. QQ-plot демонстрирует отклонение распределения остатков от нормального в крайних значениях, что ожидаемо для данных с гамма-распределением.\nАнализ случайных эффектов для судов показывает существенную вариацию между разными судами. Значения случайных эффектов варьируют от -2.92 до 0.93, что указывает на значительные различия в эффективности промысла между судами после учета влияния года, месяца и района. Распределение случайных эффектов близко к нормальному с центром около нуля.\nТест Бреуша-Пагана подтверждает наличие гетероскедастичности в модели, что является общей проблемой для моделей с данными по уловам.\nСводка параметрических коэффициентов показывает, что все годовые эффекты статистически значимы и отрицательны, подтверждая общую тенденцию снижения уловов с течением времени. Месячные эффекты также значимы и отрицательны, указывая на сезонное снижение уловов в октябре и ноябре по сравнению с сентябрем. Среди районов промысла несколько показали статистически значимые отличия от базового уровня.\nМодель объясняет 17.2% дисперсии данных, что меньше, чем в предыдущих моделях, что может быть связано с учетом части вариации через случайные эффекты. Информационные критерии AIC (42933.5) и BIC (43065.1) выше, чем у GLM и GAM моделей, что указывает на худшее соответствие данных этой модели с учетом ее сложности.\nГрафик случайных эффектов с тремя модами на значениях 0.5, -1.5 и -3 демонстрирует выраженную стратификацию судов по их промысловой эффективности. Такое распределение указывает на наличие трех различных групп в промысловом флоте, каждая со своими характеристиками. Группа с модой на 0.5 представляет суда с повышенной эффективностью, чьи уловы примерно на 65% (exp(0.5) ≈ 0.65) превышают средний уровень. Эти суда, вероятно, оснащены современным оборудованием, укомплектованы опытными экипажами и работают на наиболее продуктивных участках.\nВторая группа с модой на -1.5 соответствует судам со значительно сниженной эффективностью, показывающим уловы примерно на 78% ниже среднего показателя. Такие результаты могут быть связаны с устаревшим техническим оснащением, менее оптимальными методами лова или работой в менее продуктивных районах. Третья группа с модой на -3 представляет суда с крайне низкой эффективностью, демонстрирующие уловы на 95% ниже среднего уровня. Столь значительное отставание может объясняться серьезными техническими проблемами, отсутствием современного оборудования, неопытностью экипажей или систематическими организационными трудностями.А возможно работой не на мороженном крабе, а живом - требующим другой технологической работы.\nНаличие трех четких мод в распределении случайных эффектов свидетельствует о существенной неоднородности промыслового флота. Это указывает на то, что предположение о нормальном распределении случайных эффектов не выполняется, а данные имеют выраженную групповую структуру. Модель успешно выявляет эту скрытую стратификацию, что подтверждает важность учета случайных эффектов при анализе промысловых данных. Полученные результаты подчеркивают необходимость дифференцированного подхода к анализу эффективности судов и разработки управленческих решений с учетом выявленной группировки. Различные моды могут отражать не только технические различия между судами, но и различные стратегии промысла, доступ к ресурсам или уровень организации работы.\nПреимущества и недостатки подхода GAMM\nОсновное преимущество GAMM подхода заключается в возможности учета групповой структуры данных через случайные эффекты. Это позволяет более адекватно оценить неопределенность предсказаний и избежать завышения значимости эффектов из-за псевдорепликации. Модель обеспечивает более реалистичную оценку вариации в данных, учитывая как фиксированные эффекты, так и случайную вариацию между группами.\nК недостаткам можно отнести повышенную вычислительную сложность и потенциальные проблемы со сходимостью алгоритмов оптимизации. Интерпретация результатов становится сложнее, особенно при наличии взаимодействий между фиксированными и случайными эффектами. В данном случае модель показала худшие показатели качества подгонки по сравнению с более простыми GLM и GAM моделями, что может свидетельствовать о избыточной сложности модели для данного набора данных.\nВ целом, GAMM представляет собой мощный инструмент для анализа данных с иерархической структурой, но его применение должно быть обосновано теоретически и подтверждено улучшением качества модели по сравнению с более простыми альтернативами.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Стандартизация CPUE: GLM, GAM, GAMM</span>"
    ]
  },
  {
    "objectID": "chapter 9.html#сравнительный-анализ-моделей-по-информационным-критериям",
    "href": "chapter 9.html#сравнительный-анализ-моделей-по-информационным-критериям",
    "title": "10  Стандартизация CPUE: GLM, GAM, GAMM",
    "section": "10.6 Сравнительный анализ моделей по информационным критериям",
    "text": "10.6 Сравнительный анализ моделей по информационным критериям\nВ данном разделе проводится систематическое сравнение трех альтернативных моделей - GLM, GAM и GAMM - с использованием информационных критериев и других метрик качества. Для унификации процесса сравнения создана специализированная функция extract_model_metrics, которая адаптирована для извлечения сопоставимых показателей из моделей разной структуры.\nДля GLM и GAM моделей используются стандартные методы расчета критериев, включая AIC, BIC, логарифмическое правдоподобие и долю объясненной девиации. Для GAMM модели, имеющей более сложную смешанную структуру, метрики извлекаются из компонентов mer и gam объекта, с дополнительным учетом случайных эффектов при расчете сложности модели.\nРезультаты сравнения представлены в виде структурированной таблицы, где модели упорядочены по возрастанию AIC - информационного критерия Акаике, который балансирует качество подгонки и сложность модели. Дополнительно вычисляются дельта-AIC (разница относительно наилучшей модели) и веса AIC, которые интерпретируются как вероятности того, что данная модель является наилучшей среди рассматриваемых.\nАнализ результатов показывает четкое разделение моделей по качеству. Модель GAM демонстрирует наилучшие показатели с AIC = 42840.91 и весом AIC 0.993, что означает 99.3% вероятность того, что эта модель является наилучшей среди сравниваемых. Модель GLM показывает очень близкие результаты по объясненной дисперсии (0.401), но несколько худшие значения AIC (42850.76) и минимальный вес (0.007). Модель GAMM значительно уступает по всем критериям с AIC = 42933.49 и нулевым весом в рамках данного сравнения.\nРазница в AIC между GAM и GLM составляет 9.85 единиц, что превышает пороговое значение 2, принятое для утверждения о существенном преимуществе одной модели над другой. Еще более значительная разница в 92.58 единиц между GAM и GAMM подтверждает статистически значимое превосходство GAM модели.\nНа основе проведенного анализа формулируются итоговые рекомендации по выбору модели. Модель GAM идентифицируется как наилучшая с очень высокой степенью уверенности (вес AIC 0.993). Объясняющая способность модели составляет 40.1%, что указывает на хорошее соответствие модели данным.\nДанный сравнительный подход обеспечивает объективную основу для выбора окончательной модели, позволяя учесть как качество подгонки, так и сложность модели, избегая таким образом как избыточного усложнения, так и излишнего упрощения.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Стандартизация CPUE: GLM, GAM, GAMM</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Введение",
    "section": "",
    "text": "“How data treat you?” — Аристотель\n\nНастоящий ресурс посвящен применению современных методов анализа данных для оценки водных биоресурсов, поскольку, как заметил бы Роберт Сапольски, игнорировать статистику в гидробиологии так же безрассудно, как павиану игнорировать льва в соседнем кусте. Изначально эти материалы создавались для очного курса, который пока не состоялся и, в силу разных обстоятельств, возможно, не состоится. Что ж, эволюция любит слепые переулки, и вот теперь практические занятия выставлены на всеобщее обозрение — в свободное пользование для всех заинтересованных специалистов. Лекции, быть может, последуют позже, а может, и нет — как повезет.\nМы живем в эпоху больших языковых моделей (LLM), когда любая стандартная методика может быть разжевана нейросетью, а обучающий скрипт сгенерирован за время, необходимое чтобы моргнуть. В таких условиях настоящую ценность представляют уже не шаблонные решения, а профессиональный опыт, причудливые идеи и то самое глубокое понимание, которое позволяет видеть данные изнутри. Молодые специалисты, как правило, достаточно быстро учатся собирать материал «в поле» — этому способствует и врожденная склонность человека к исследованию, и вполне себе социализированное желание проводить время на свежем воздухе. Но вот ключевой вопрос, перед которым многие замирают: а что делать с этими данными дальше? Ограничиться стандартной картой и графиком с коэффициентом корреляции — сегодня не вариант. Мир анализа данных полон мощных и поразительных инструментов, и наша задача — не только показать их, но и научить заставлять данные рассказывать о себе так, как им, возможно, не всегда хочется: по-разному, подробно, иногда даже против их воли.\nHow data treat you? — как-то раз ернически спросил меня… нет, не Аристотель, конечно, а Aristoteles, молодой специалист из Венесуэлы. Произошло это во время одной научно-исследовательской съемки у берегов Фолклендских островов. Его вопрос я понял не сразу — возможно, сказалась усталость, а может, когнитивный диссонанс от того, что столь глубинно-философское прозрение пришло к человеку в ярком плаще и с гамаком за спиной*. Но смысл его оказался точен: данные относятся к тебе так, как ты относишься к ним.\nСегодня от исследователя уже не требуется безупречного владения навыками программирования — достаточно иметь терпение, любопытство и немного смирения перед лицом технологии. LLM стали нашими компаньонами, цифровыми шимпанзе-ассистентами, способными написать, исправить, прокомментировать или продолжить почти любой скрипт. Это значительно снижает порог входа в мир R и анализа. Такой подход можно было бы назвать «vibe coding» — итеративный, почти медитативный процесс творческого диалога с машиной, где ты формулируешь задачи на естественном языке, прототипируешь идеи через ИИ, а затем оцениваешь результат, фокусируясь не на синтаксисе, а на смысле. Данный практикум стремится культивировать именно такой — более человечный — стиль работы. Многие из этих занятий родились в нескончаемых диалогах и импровизированных дискуссиях с Cursor, DeepSeek, Qwen и KIMI — моими цифровыми коллегами по цеху.\nПрактикум представляет собой своего рода путеводитель — или, если угодно, field guide — по применению современных методов анализа данных, ориентированный на начинающих специалистов. Материалы структурированы так, чтобы охватить ключевые этапы работы: от первичной загрузки и обработки данных до продвинутого моделирования и визуализации. Здесь вы найдёте подробные примеры кода, пояснения к методам и — что особенно важно — интерпретацию результатов, которая позволяет не только освоить R, но и понять, каким образом эти выводы встраиваются в более широкий биологический и управленческий контекст.\nОсобое внимание уделено работе с ограниченными и неполными данными — ситуацией, типичной для многих гидробиологических и рыбохозяйственных исследований. Потому что, let’s face it, идеальные датасеты существуют разве что в учебниках. В реальности же нам приходится иметь дело с тем, что есть — и находить красоту в несовершенстве. Практикум включает как классические статистические методы (линейные и логистические регрессии, кластеризация, сравнение групп), так и современные подходы: пространственно-временное моделирование (sdmTMB), нейронные сети и байесовские методы оценки запасов (SPiCT, JABBA). Отдельный раздел посвящен картографированию и визуализации — потому что карта всё ещё иногда говорит громче, чем сто пятьсот p-value.\nМатериалы продолжают пополняться — медленно, неравномерно, с переменным успехом — и доступны в открытом доступе. Возможно, они станут для кого-то тем самым ресурсом, которого не хватало. Приветствуются предложения по сотрудничеству и материалы от коллег — с обязательным указанием авторства. Также принимаются вопросы, идеи и даже деликатно оформленные предложения по улучшению — потому что ни один мозг, даже при поддержке LLM, не может объять необъятное.\nКонтакты для связи: Сергей Баканёв mombus@gmail.com\n*​​​​​​​- Комментарий Роберта Сапольски: это же чистейшей воды когнитивный диссонанс в его самом бытовом и потому гениальном проявлении.\nНаш мозг — великий мастер по созданию шаблонов и ярлыков. Он постоянно, за спасибо, каталогизирует реальность, чтобы нам не пришлось каждый раз с нуля решать, съедобен ли этот гриб или стоит ли бежать от этого зверя. Часть этой каталогизации — создание образов и ожиданий от этих образов.\nИ вот перед нами возникает персонаж:\n\nЯркий плащ — это атрибут чудака, художника, туриста-недотепы, который несерьёзно относится к суровым полевым условиям.\nГамак за спиной — это символ легкомыслия, отдыха, сиесты, нежелания погружаться в грязную, потную работу «настоящего» исследователя.\n\nНаш мозг, сверкая нейронами, мгновенно скатывает этого человека в категорию «несерьёзный тип», «экспонат». Мы ожидаем от него шуток, баек или, на худой конец, вопросов о том, где лучше половить рыбу на ужин.\nА он — бац! — и задает вопрос уровня зрелого философа или data scientist-ветерана. «How data treat you?» — это не вопрос про погоду. Это глубокий, почти экзистенциальный вопрос о взаимоотношениях между исследователем и его данными, о том, как наши собственные предубеждения и методы формируют тот результат, который мы в итоге получаем.\nИ наш мозг, который только что занес этого человека в папку «Разное/Неважное», сталкивается с катастрофой. Шаблон трещит по швам. Происходит тот самый когнитивный диссонанс: конфликт между ожиданием («чудак») и реальностью («мудрец»).\nПлащ и гамак здесь — это идеальная метафора этого диссонанса. Они являются видимым, материальным доказательством ошибочности наших стереотипов. Самый проницательный вопрос в вашей жизни может прийти от кого угодно и где угодно: от человека в костюме клоуна, от бармена в три часа ночи или от коллеги с гамаком на Фолклендах.\nИ мораль этой истории такова: наш мозг ленив и склонен к предубеждениям. Но настоящая мудрость часто приходит в неподходящей упаковке. Задача — пережить этот кратковременный когнитивный сбой, отбросить ярлыки и услышать сам вопрос, а не оценить костюм того, кто его задает.\nВ конечном счете, плащ и гамак — это просто детали, которые делают историю человечной и запоминающейся. Они напоминают нам, что глубокие мысли носят не только те, у кого есть ученая степень и строгий костюм, но и те, кто позволяет себе быть несерьезным, чтобы быть по-настоящему свободным в своих размышлениях.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Введение</span>"
    ]
  },
  {
    "objectID": "chapter 1.html",
    "href": "chapter 1.html",
    "title": "2  Анализ и визуализация данных улова",
    "section": "",
    "text": "2.1 Введение\nЭто занятие — про первый шаг в анализе уловов: аккуратно загрузить данные, посмотреть на них без иллюзий и задать простые, проверяемые вопросы. Мы будем работать в R, потому что он честно показывает структуру данных и не скрывает неудобные детали. Наша цель сейчас не «сделать красиво», а убедиться, что мы видим именно то, что реально записано в файле: сколько строк, какие переменные, каких они типов и не прячутся ли среди них ошибки, способные испортить весь последующий анализ.\nНачинаем с того, чтобы R «видел» правильную папку. Рабочая директория должна указывать туда, где лежит файл shrimp_catch.csv. Простая установка пути — это не бюрократия, а воспроизводимость: на другом компьютере тот же код должен читать те же данные, а не «что‑то похожее». После этого подключаем tidyverse: это набор инструментов, который унифицирует чтение, преобразование и визуализацию. read_csv считывает таблицу и сразу создаёт tibble — «вежливую» версию data.frame с чётким хранением типов. Уже на этом шаге стоит помнить о банальных, но частых ловушках: десятичный разделитель должен совпадать с вашими региональными настройками, пустые строки и «NA» в файле должны превращаться в пропуски, а не в нули или текст.\nПервичный осмотр — это короткий разговор с данными без интерпретации. Команда glimpse выдаёт компактный снимок: сколько строк, каковы названия столбцов и их классы, примеры значений. В нашем наборе ожидаем пять столбцов: id как целое число, age как целое число 1–4, length как числовая величина длины, weight как числовая величина массы и sex как текстовый признак пола. Если вы видите, что length внезапно «chr» или sex закодирован числами — это сигнал остановиться и привести типы в порядок сейчас, а не объяснять странные результаты потом. Аналогичная команда str показывает внутреннюю структуру и подтверждает, что R понимает объект так же, как и вы. Эти две команды — «микроскоп 4×»: быстро и без украшательств.\nДальше имеет смысл задать несколько контрольных вопросов, которые одновременно проверяют здравый смысл и раскрывают базовую статистику. summary покажет минимумы, медианы и квартильные точки для количественных переменных и распределение для категориальных. Если где‑то возникает отрицательный вес, нулевая длина или возраст за пределами 1–4 — это не «особенности популяции», это данные, требующие чистки. table и prop.table дадут частоты по полу; если соотношение полов выглядит нереалистично для вашей промысловой выборки — проверьте этап предобработки. Наконец, простой cor.test между длиной и весом покажет, есть ли ожидаемая сильная положительная связь; но здесь важно помнить о дисциплине: корреляция — это не причинность, и даже высокая r требует подтверждения графиком рассеяния и проверкой на аутлаеры.\nЗачем столько внимания «мелочам» до любых моделей? Потому что в прикладной биостатистике именно этот участок пути отделяет полезные выводы от красивых, но пустых графиков. Проверка типов и диапазонов, явное обращение с пропусками, подтверждение структурой — это те скромные процедуры, которые экономят часы на поздних этапах. И если позволить себе лёгкую ремарку, то лучший способ повысить интеллектуальную честность анализа — не верить по умолчанию ни себе, ни данным, пока вы не посмотрели на них под простейшим светом glimpse и str.\nКогда эти шаги пройдены, можно переходить к описательной статистике и первичной визуализации. Гистограмма длины даст быстрый набросок формы распределения, а простые группировки по возрасту покажут, как меняются средние и разброс. Но это уже следующий раздел. Сейчас важнее, чтобы R и вы одинаково понимали, что такое «наши данные», и чтобы каждый последующий результат опирался на корректно загруженную и проверенную таблицу shrimp_catch.csv.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Анализ и визуализация данных улова</span>"
    ]
  },
  {
    "objectID": "chapter 1.html#загрузка-данных-и-первичный-осмотр",
    "href": "chapter 1.html#загрузка-данных-и-первичный-осмотр",
    "title": "2  Анализ и визуализация данных улова",
    "section": "2.2 Загрузка данных и первичный осмотр",
    "text": "2.2 Загрузка данных и первичный осмотр\nссылка на файл: shrimp_catch.csv\n\n# Установка рабочей директории\nsetwd(\"C:/TEXTBOOK/\")\n# Загрузка библиотек\nlibrary(tidyverse)\n# Загрузка данных\ndata &lt;- read_csv(\"shrimp_catch.csv\")\n\nКоманда glimpse знакомит со структурой данных:\n\n# Просмотр структуры и первых строк загруженных данных\nglimpse(data)\n\n\nRows: 230\nColumns: 5\n$ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, ~\n$ age    &lt;int&gt; 2, 4, 4, 4, 1, 4, 2, 2, 4, 3, 4, 3, 2, 1, 2, 1, 2, 2, 2, 2, 3, ~\n$ length &lt;dbl&gt; 20.45450, 25.88928, 29.42257, 30.68292, 12.46059, 28.52152, 17.~\n$ weight &lt;dbl&gt; 1.28221748, 1.97476899, 2.65412595, 3.44746476, 0.13404801, 2.3~\n$ sex    &lt;chr&gt; \"M\", \"F\", \"F\", \"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"M\"~\n&gt; \n\nМожно использовать команду str — показывает внутреннюю структуру объекта , включая количество строк, столбцов, названия переменных, их типы (chr, num, int и др.), а также несколько первых значений.\n\nstr(data)\n\n\n'data.frame':   230 obs. of  5 variables:\n $ id    : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age   : int  2 4 4 4 1 4 2 2 4 3 ...\n $ length: num  20.5 25.9 29.4 30.7 12.5 ...\n $ weight: num  1.282 1.975 2.654 3.447 0.134 ...\n $ sex   : chr  \"M\" \"F\" \"F\" \"F\" ...\n&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Анализ и визуализация данных улова</span>"
    ]
  },
  {
    "objectID": "chapter 1.html#описательная-статистика-и-визуализация",
    "href": "chapter 1.html#описательная-статистика-и-визуализация",
    "title": "2  Анализ и визуализация данных улова",
    "section": "2.3 Описательная статистика и визуализация",
    "text": "2.3 Описательная статистика и визуализация\nКоманда summary выводит описательную статистику для каждой числовой переменной: минимум, 1-й квартиль, медиана, среднее, 3-й квартиль, максимум; для категориальных переменных — частоты.\n\n# Общая статистика\nsummary(data)\n\n\n       id              age            length          weight       \n Min.   :  1.00   Min.   :1.000   Min.   : 7.65   Min.   :-0.3334  \n 1st Qu.: 58.25   1st Qu.:2.000   1st Qu.:17.62   1st Qu.: 0.6320  \n Median :115.50   Median :3.000   Median :22.49   Median : 1.3660  \n Mean   :115.50   Mean   :2.509   Mean   :21.68   Mean   : 1.4933  \n 3rd Qu.:172.75   3rd Qu.:3.000   3rd Qu.:26.03   3rd Qu.: 2.1148  \n Max.   :230.00   Max.   :4.000   Max.   :36.02   Max.   : 5.1316  \n     sex           \n Length:230        \n Class :character  \n Mode  :character  \n\nПростейшими командами можно вычислить, например, соотоношение полов или корреляцию длина-вес.\n\n# Соотношение полов\nprop.table(table(data$sex)) %&gt;% round(2)\n\n\n   F    M \n0.35 0.65 \n\n\n# Корреляция длина-вес с p-value\ncor_test &lt;- cor.test(data$length, data$weight, \n                     method = \"pearson\", \n                     exact = FALSE,\n                     na.action = na.omit)\n \ncor_coef &lt;- round(cor_test$estimate, 2)\np_value &lt;- scales::pvalue(cor_test$p.value, accuracy = .001)\n \ncat(\"Корреляция Пирсона: r =\", cor_coef, \", p =\", p_value, \"\\n\")\n\n\nКорреляция Пирсона: r = 0.95 , p = &lt;0.001 \n\n\n# Распределение возраста\ntable(data$age)\n\n\n1  2  3  4 \n43 68 77 40 \n\n\n# Средние значения длины и веса по группам\ndata %&gt;%\n   group_by(age) %&gt;%\n   summarise(\n     mean_length = mean(length),\n     sd_length = sd(length),\n     mean_weight = mean(weight),\n     sd_weight = sd(weight))\n\n\n# A tibble: 4 x 5\n    age mean_length sd_length mean_weight sd_weight\n  &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1     1        12.7      1.37       0.249     0.234\n2     2        19.2      1.88       0.919     0.341\n3     3        24.8      1.72       1.88      0.424\n4     4        29.1      2.28       2.96      0.804\n&gt; \n\n\n2.3.1 Построение гистограммы для переменной ‘length’ (длина креветок)\nДля первого визуального знакомства команда hist строит гистограмму — простой график, который показывает, как распределены значения числовой переменной. В данном случае отображается распределение длин креветок из набора данных.\n\nhist(data$length, \n     main = \"Гистограмма длины креветок\",          # Заголовок графика\n     xlab = \"Длина (см)\",                          # Подпись оси X\n     ylab = \"Частота\",                             # Подпись оси Y\n     col = \"lightblue\",                            # Цвет столбцов\n     border = \"black\",                             # Цвет границ столбцов\n     breaks = 15)                                   # Количество интервалов\n\n\n\n\nРис. 1.1: Гистограмма длины креветок\n\n\n\n\n2.3.2 Визуализация в ggridges\nДля элегантных и компактных графиков подходит библиотека ggridges. Построим распределение длины креветки в зависимости от пола и возраста.\n\nlibrary(ggplot2)\nlibrary(ggridges)\n\nggplot(data, aes(x = length, \n                 y = sex, \n                 group = sex, \n                 fill = sex)) +\n  geom_density_ridges(scale = 2, alpha = 0.7) +\n  scale_y_discrete(expand = c(0, 0)) +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(\n    title = \"Распределение длины карапакса по полу\",\n    x = \"Длина карапакса (мм)\",\n    y = \"Пол\"\n  ) +\n  theme(\n    panel.border = element_blank(),  # Убирает рамку вокруг графика\n    axis.line = element_line(color = \"black\")  # Сохраняет осевые линии (опционально)\n  )\n\n\n\n\nРис. 1.2: Пол-длина креветок с использованием ggridges",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Анализ и визуализация данных улова</span>"
    ]
  },
  {
    "objectID": "chapter 1.html#выявление-аутлайеров-выбросов",
    "href": "chapter 1.html#выявление-аутлайеров-выбросов",
    "title": "2  Анализ и визуализация данных улова",
    "section": "2.4 Выявление аутлайеров (выбросов)",
    "text": "2.4 Выявление аутлайеров (выбросов)\nАутлаеры (выбросы) — наблюдения, значительно отклоняющиеся от общего распределения данных. Их идентификация критически важна, так как они могут искажать результаты анализа. Один из надёжных методов обнаружения выбросов — метод межквартильного размаха (IQR).\n\n2.4.1 Теория метода\n\nРасчёт квартилей:\n\nQ1 (25-й перцентиль): значение, ниже которого находится 25% данных.\nQ3 (75-й перцентиль): значение, ниже которого находится 75% данных.\nIQR = Q3 - Q1: мера разброса средней половины данных.\n\nГраницы аутлаеров:\n\nНижняя граница: Q1−1.5×IQRQ1−1.5×IQR\nВерхняя граница: Q3+1.5×IQRQ3+1.5×IQR\nНаблюдения за этими пределами считаются выбросами.\n\n\n\n\n2.4.2 Преимущества метода\n\nУстойчивость к асимметрии распределения.\nНе требует предположения о нормальности данных.\n\n\n# Метод межквартильного размаха\noutliers &lt;- data %&gt;%\n  mutate(\n    length_z = scale(length),\n    weight_z = scale(weight)\n  ) %&gt;% \n  filter(abs(length_z) &gt; 3 | abs(weight_z) &gt; 3)\n\n# Визуализация\nggplot(data, aes(x = length, y = weight)) +\n  geom_point(aes(color = \"Обычные\"), alpha = 0.5) +\n  geom_point(data = outliers, aes(color = \"Аутлаеры\"), size = 3) +\n  scale_color_manual(values = c(\"Обычные\" = \"grey50\", \"Аутлаеры\" = \"red\")) +\n  labs(title = \"Выявление аномальных наблюдений\", color = \"Тип\")\n\n\n\n\nРис. 1.3: Распределение длины карапакса",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Анализ и визуализация данных улова</span>"
    ]
  },
  {
    "objectID": "chapter 1.html#определение-возрастной-структуры-статистические-методы-анализа-размерных-данных",
    "href": "chapter 1.html#определение-возрастной-структуры-статистические-методы-анализа-размерных-данных",
    "title": "2  Анализ и визуализация данных улова",
    "section": "2.5 Определение возрастной структуры: статистические методы анализа размерных данных",
    "text": "2.5 Определение возрастной структуры: статистические методы анализа размерных данных\nЛирическое отступление\nОпределять возрастную структуру по размерным данным — это не попытка «угадать» возраст, а дисциплинированный способ выделить в общей смеси несколько закономерных мод, за которыми почти всегда стоят биологические процессы: вариации в пополнении, различия в темпах роста, селективность промысла. Мы не видим возраст напрямую, зато видим след его накопления в длине, и задача статистики здесь — разложить смешанное распределение на осмысленные компоненты, не выдавая желаемое за действительное. Начинать разумно с простого: гистограмма и сглаженная плотность дают первичную картину, где «пики» — это кандидаты на возрастные группы. Выбор ширины бинов (диапазонов) — не косметика: слишком широкие бины сливают моды, слишком узкие создают шумовые «зубцы». Ядерная плотность полезна как независимая проверка: если пик виден и на гистограмме, и на сглаженной плотности, это хороший знак. Уже на этом шаге важно исключить явные аутлаеры и убедиться, что анализируется однородная по сезону и району выборка: смешение сезонов способно превратить один чёткий пик в два слабых и наоборот.\nK‑means привлекателен скоростью и простотой, но его допущения жестковаты для биологии: он делит по ближайшему центру и фактически предполагает равные дисперсии у групп. Для черновой разметки это приемлемо: задали K, получили кластеры, посмотрели, не распилили ли явный пик пополам и не смешали ли крайние хвосты. Но трактовать эти кластеры как «возрастные классы» без дополнительных проверок нельзя. Минимальный набор проверок — «локальный смысл»: центры кластеров должны быть упорядочены по длине, доли групп не должны выглядеть абсурдно для вашей системы, а границы между кластерами — приходиться на спады между модами гистограммы. Полезно пробежать несколько значений K, посмотреть «локоть» по внутрикластерной дисперсии или силуэт; если модель жадно «доедает» шум, она не помогает задаче.\nДекомпозиция смесью нормалей с EM‑алгоритмом ближе к тому, что нам нужно: каждая компонента имеет свой средний размер, свою дисперсию и свою долю, а принадлежность особи — вероятностная, а не «жёсткая». Это лучше отражает реальность: возрастные группы перекрываются, и жёсткое отнесение на границе избыточно уверенно. Здесь ключевая инженерная мысль — инициализация и выбор числа компонент. Стартовать можно от пиковой структуры гистограммы или от грубых центров k‑means; число компонент выбирать по BIC/AIC и здравому смыслу, помня, что каждая лишняя компонента почти всегда «объясняет» шум. Параметры смеси имеют прозрачную интерпретацию: μ — модальный размер группы, σ — разброс (ростовая гетерогенность плюс измерительная ошибка), λ — доля группы в выборке. Для отчётности полезно ранжировать компоненты по μ, чтобы избежать «перескока меток» между запусками, и дать доверительные интервалы (обычный приём — бутстрэп).\nМетод Бхаттачарии, классика промысловой статистики, по сути делает то же в терминах гистограммы: линейнизует лог‑разности соседних бинов и позволяет визуально «вынуть» наклон, соответствующий компоненте нормального распределения. Он чувствителен к выбору ширины бина и к ровности хвостов, зато нагляден и хорошо работает там, где пики действительно нормальны и отделены. В паре с EM это сильная связка: Бхаттачария помогает выбрать разумное K и старт, EM — уточняет параметры и даёт вероятностные принадлежности. Сопоставление результатов этих двух подходов повышает доверие: если оба «видят» четыре группы с близкими μ, это гораздо лучше, чем красивый рисунок одного метода.\nНа практике полезно придерживаться алгоритма «снизу вверх». Сначала — чистая визуализация: гистограмма, ядерная плотность, по возможности разрезы по полу и возрасту полевой маркировки; это помогает понять, не смешиваем ли мы биологически разные контексты. Затем — черновая кластеризация k‑means для ориентировочных центров и грубого K. Далее — смеси нормалей с EM, выбор K по BIC и проверка стабильности решения от разных стартов. После подгонки — диагностика: наложить компоненты и суммарную смесь на гистограмму, проверить, не «улетели» ли σ, нет ли «дублирующих» компонент с почти одинаковыми μ, сопоставить λ с ожидаемыми долями когорт. И главное — помнить, что «модальные группы по длине» и «возрастные классы» совпадают не автоматом: для перевода мод в возраст нужен ростовой ключ (например, параметры Берталанфи) или независимая информация о когортности. Без этого честнее говорить «модальные размерные группы».\nНаконец, стоит держать в голове пару дисциплинарных напоминаний. Любая смесь будет пытаться объяснить артефакты данных, поэтому контроль качества измерений и фильтрация аутлаеров — не опция, а необходимость. Сезонная выборка и выборочная выловленность сдвигают доли и средние: если орудия ловят неравномерно, λ смеси — это не «структура популяции», а «структура улова». И, как бы прозаично это ни звучало, фиксируйте зерно генератора случайных чисел и документируйте выбор K и стартовые значения: это делает ваш результат воспроизводимым, а спор — предметным. Такой дисциплинированный ход — от картинки к модели, от модели к диагностике, от диагностики к осторожной интерпретации — позволяет извлечь из длины то, что она действительно хранит про возраст, и не больше.\nИ так, возрастная структура популяции — часто важна для расчёта промысловой смертности, оценки репродуктивного потенциала и прогнозирования динамики запасов. Поскольку прямое измерение возраста часто невозможно (например, у беспозвоночных или рыб без четких возрастных меток), используются статистические методы, выделяющие группы в смешанных распределениях размеров.\nОсновные подходы:\n\nМетод k-средних (k-means) — алгоритм кластеризации, группирующий особи в заданное число кластеров (возрастных групп) на основе их размеров.\nМетод Бхаттачарии — статистический подход для разделения смешанных нормальных распределений, часто применяемый для идентификации мод в гистограммах.\nEM-алгоритм — оценка параметров смеси распределений, подходящая для данных с перекрывающимися возрастными группами.\nГауссовы смеси (GMM) — расширение метода Бхаттачарии для многомерного анализа.\nЯдерное сглаживание — непараметрический метод визуализации плотности, помогающий выявить скрытые моды.\n\nРассмотрим метод k-средних (k-means) и метод Бхаттачарии, предварительно построив гистограмму.\n\n# Загрузка библиотек\nlibrary(tidyverse)\nlibrary(mixtools)\n# Гистограмма длины с наложением плотности\nggplot(data, aes(x = length)) +\n  geom_histogram(aes(y = after_stat(density)), fill = \"steelblue\", bins = 20, alpha = 0.7) +\n  geom_density(color = \"#FC4E07\", linewidth = 1) +\n  labs(title = \"Распределение длины карапакса\", \n       subtitle = \"Пики могут соответствовать возрастным группам\",\n       x = \"Длина (мм)\")\n\n\n\n\nРис. 1.3: Распределение длины карапакса\n\n\n\n# Кластеризация по длине (K-means как пример)\nset.seed(123)\nclusters &lt;- kmeans(data$length, centers = 4)  # Предполагаем 4 возрастные группы\ndata$cluster &lt;- factor(clusters$cluster)\n\n# Визуализация кластеров\nggplot(data, aes(x = length, fill = cluster)) +\n  geom_histogram(bins = 25, alpha = 0.7) +\n  labs(title = \"Кластеризация по длине)\", \n       x = \"Длина (мм)\")\n\n\n\n\nРис. 1.4: Кластеризация по длине\n\n\n\n# Установка рабочей директории\nsetwd(\"C:/TEXTBOOK/\")\n\n# Загрузка библиотек\nlibrary(tidyverse)\nlibrary(mixtools)\n\n# Загрузка данных\ndata &lt;- read.csv(\"shrimp_catch.csv\")\n\n# 1. Построение и отображение гистограммы\nhist(data$length, breaks = 20, main = \"Гистограмма распределения длин карапаксов\",\n     xlab = \"Длина карапакса (мм)\", ylab = \"Частота\")\n\n# 2. Инициализация параметров (предположим 4 возрастные группы)\ninit_params &lt;- list(\n  lambda = rep(1/4, 4),\n  mu = c(13, 19, 25, 32),\n  sigma = c(1.5, 1.75, 1.75, 2.5)\n)\n\n# 3. Разделение смеси распределений методом EM\nfit &lt;- normalmixEM(data$length, k = 4, maxit = 1000, epsilon = 1e-3,\n                   lambda = init_params$lambda,\n                   mu = init_params$mu,\n                   sigma = init_params$sigma)\n\n# 4. Визуализация результатов с ggplot2\n# Генерация сетки для построения кривых\nx_grid &lt;- seq(min(data$length), max(data$length), length.out = 500)\n\n# Функция смеси\nmixture_density &lt;- function(x) {\n  fit$lambda[1] * dnorm(x, fit$mu[1], fit$sigma[1]) +\n  fit$lambda[2] * dnorm(x, fit$mu[2], fit$sigma[2]) +\n  fit$lambda[3] * dnorm(x, fit$mu[3], fit$sigma[3]) +\n  fit$lambda[4] * dnorm(x, fit$mu[4], fit$sigma[4])\n}\n\n# График\nggplot(data, aes(x = length)) +\n  # Гистограмма\n  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = \"white\", color = \"black\", alpha = 0.7) +\n  # Исходное распределение (гладкая линия)\n  geom_density(color = \"red\", lwd = 1.2) +\n  # Смесь распределений\n  stat_function(fun = mixture_density, color = \"black\", lwd = 1.5) +\n  # Компоненты смеси\n  stat_function(fun = function(x) fit$lambda[1] * dnorm(x, fit$mu[1], fit$sigma[1]), color = \"blue\", lwd = 1) +\n  stat_function(fun = function(x) fit$lambda[2] * dnorm(x, fit$mu[2], fit$sigma[2]), color = \"green\", lwd = 1) +\n  stat_function(fun = function(x) fit$lambda[3] * dnorm(x, fit$mu[3], fit$sigma[3]), color = \"orange\", lwd = 1) +\n  stat_function(fun = function(x) fit$lambda[4] * dnorm(x, fit$mu[4], fit$sigma[4]), color = \"purple\", lwd = 1) +\n  \n  # Настройка темы и легенды\n  theme_minimal() +\n  labs(\n    x = \"Длина карапакса (мм)\",\n    y = \"Плотность\",\n    title = \"Разделение возрастных групп методом EM\"\n  )\n\n\n\n\nРис. 1.5: Метод Бхаттачарии",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Анализ и визуализация данных улова</span>"
    ]
  },
  {
    "objectID": "chapter 1.html#уравнение-берталанфи",
    "href": "chapter 1.html#уравнение-берталанфи",
    "title": "2  Анализ и визуализация данных улова",
    "section": "2.6 Уравнение Берталанфи",
    "text": "2.6 Уравнение Берталанфи\nУравнение Берталанфи — фундаментальная модель в рыбохозяйственной науке, описывающая асимптотический рост организмов. Оно имеет вид: \\[\nL(t) = L_{\\infty} \\cdot \\left(1 - e^{-k \\cdot (t - t_0)}\\right)\n\\] где L∞— теоретическая максимальная длина особи, k— коэффициент скорости роста, t0— гипотетический возраст при нулевой длине.\nВ приведённом коде модель применяется для анализа роста северной креветки :\n\nПодготовка данных: Удаление аутлаеров (например, строк 10 и 50) повышает точность оценки параметров.\nИнициализация параметров:\n\nL∞ задаётся как максимальная наблюдаемая длина в данных.\nk и t0 подбираются итеративно методом нелинейных наименьших квадратов (nls).\n\nВизуализация: График сопоставляет эмпирические данные (точки) с предсказаниями модели (красная линия), демонстрируя, как рост замедляется с приближением к L∞.\n\nИнтерпретация параметров:\n\nВысокое значение k (&gt;0.3) указывает на быстрый рост молоди.\nt0&lt;0 может отражать ранний метаморфоз личинок.\n\n\n# Загрузка библиотек\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(nlme)\n\n# Загрузка данных\ndata &lt;- read.csv(\"shrimp_catch.csv\")\n\n# Преобразование возраста в числовой формат\ndata$age_num &lt;- as.numeric(data$age)\n\n# Удаление аутлайеров (если необходимо)\ndata_clean &lt;- data %&gt;%\n  filter(!id %in% c(10, 50))  # Пример удаления строк с аномалиями\n\n# Начальные параметры на основе данных\nL_inf_start &lt;- max(data_clean$length, na.rm = TRUE)  # Максимальная длина\nk_start &lt;- 0.3                                        # Средняя скорость роста\nt0_start &lt;- -0.5                                      # Гипотетический возраст\n\n# Подгонка модели с увеличенным числом итераций\nmodel &lt;- nls(\n  length ~ L_inf * (1 - exp(-k * (age_num - t0))),\n  data = data_clean,\n  start = list(L_inf = L_inf_start, k = k_start, t0 = t0_start),\n  control = nls.control(maxiter = 200, warnOnly = TRUE)  # Увеличиваем лимит итераций\n)\n\n# Вывод результатов\nsummary(model)\n\n# Создание последовательности возрастов для предсказания\nage_seq &lt;- seq(min(data_clean$age_num), max(data_clean$age_num), by = 0.1)\n\n# Предсказание значений длины\nlength_pred &lt;- predict(model, newdata = data.frame(age_num = age_seq))\n\n# Построение графика\nggplot(data_clean, aes(x = age_num, y = length)) +\n  geom_point(aes(color = age), alpha = 0.7) +\n  geom_line(data = data.frame(age_num = age_seq, length = length_pred), \n            aes(x = age_num, y = length), color = \"red\", linewidth = 1.2) +\n  labs(\n    title = \"Рост креветок по уравнению Берталанфи\",\n    x = \"Возраст (годы)\",\n    y = \"Длина карапакса (мм)\",\n    color = \"Возрастная группа\"\n  ) +\n  theme_minimal()\n\n# Сохранение графика\nggsave(\"bertalanffy_model.png\", width = 8, height = 6)\n\n\n\n\nРис. 1.6: Рост креветок по уравнению Берталанфи",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Анализ и визуализация данных улова</span>"
    ]
  },
  {
    "objectID": "chapter 1.html#огива-логистическая-кривая-и-50-ное-созревание",
    "href": "chapter 1.html#огива-логистическая-кривая-и-50-ное-созревание",
    "title": "2  Анализ и визуализация данных улова",
    "section": "2.7 Огива, логистическая кривая и 50%-ное созревание",
    "text": "2.7 Огива, логистическая кривая и 50%-ное созревание\nЛогистическая регрессия удобна там, где исход — бинарный: созрел/не созрел, самка/самец. Для протоандрической креветки вероятность быть самкой естественно растёт с длиной, и логистическая кривая описывает это гладким переходом от 0 к 1; её центральная точка даёт L50 = −β0/β1 — длину, при которой половина особей уже самки. Огива — это та же история, но накопительно: как доля самок нарастает по мере увеличения длины; она наглядна для сравнения годов/районов и проверки сдвигов зрелости. Качество модели удобно проверять ROC/AUC: AUC ≈ 0.9+ означает, что длина хорошо ранжирует вероятность женского пола, но не отменяет проверки калибровки. Знак и величина β1 интерпретируются просто: положительный β1 — с каждым миллиметром шансы быть самкой растут, exp(β1) — во сколько раз растут эти шансы на единицу длины. Биологически L50 концентрирует ключевой сигнал: при стабильных условиях он держится в узком интервале (для Pandalus borealis около 25–28 мм), а его снижение обычно маркирует стресс среды или избирательный вылов, «подталкивающий» к более раннему созреванию. В прикладном учёте это даёт два практичных числа — L50 и AUC — и две опоры для интерпретации: насколько резко идёт переход (крутизна кривой) и насколько надёжен прогноз (дискриминация и калибровка).\nЛогистическая кривая — ключевой инструмент для моделирования бинарных процессов, таких как созревание или смена пола у организмов. В случае протоандрических креветок (Pandalus borealis), которые меняют пол с возрастом, зависимость вероятности быть самкой от длины карапакса можно описать логистической функцией:\n\\[\nP(F) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot длина)}}\n\\]\nгде P(F) — вероятность принадлежности к женскому полу, β0 — интерсепт, β1 — коэффициент влияния длины.\nТочка перегиба логистической кривой соответствует длине, при которой вероятность быть самкой равна 50%: \\[\nL_{50} = -\\frac{\\beta_0}{\\beta_1}\n\\]\n\n\n\nРис. 1.7: Логистическая кривая\n\n\nОгива (кумулятивная кривая) показывает накопление вероятности с увеличением длины. Для анализа созревания её можно построить через интеграл логистической функции. Визуально она демонстрирует, как доля самок возрастает с размером.\n\n\n\nРис. 1.8: Огива\n\n\n\n2.7.1 Оценка модели\n\nROC-кривая и AUC:\n\nПлощадь под ROC-кривой (AUC) &gt;0.7 указывает на хорошую предсказательную способность модели.\nЗначение AUC = 0.94(пример из кода) подтверждает сильную связь длины и пола.\n\n\n\n\n\nРис. 1.9: ROC-кривая и AUC\n\n\n\nИнтерпретация коэффициентов:\n\nПоложительный β1 означает: с ростом длины вероятность быть самкой увеличивается.\nНапример, β1=0.25 → увеличение длины на 1 мм повышает шансы в e0.25≈1.28 раза.\n\n\n\n\n2.7.2 Биологический контекст\n\nПротоандрический гермафродитизм: У креветок смена пола с самцов на самок происходит при достижении критического размера (~25-28 мм).\nL50 как индикатор: Снижение L50 в популяции может сигнализировать о стрессовых условиях (перелов, изменение среды), ускоряющих созревание.\n\n\n# Установка рабочей директории\nsetwd(\"C:/TEXTBOOK/\")\n\n# Загрузка библиотек\nlibrary(tidyverse)\nlibrary(pROC)\nlibrary(ggplot2)\n\n# Загрузка данных\ndata &lt;- read_csv(\"shrimp_catch.csv\")\n\n# 1. Предобработка данных -----------------------------------------------------\n# Удаление аутлаеров методом IQR\nQ1 &lt;- quantile(data$length, 0.25)\nQ3 &lt;- quantile(data$length, 0.75)\nIQR &lt;- Q3 - Q1\ndata_clean &lt;- data %&gt;%\n  filter(length &gt;= Q1 - 1.5*IQR & length &lt;= Q3 + 1.5*IQR)\n\n# 2. Логистическая регрессия --------------------------------------------------\n# Преобразование пола в бинарную переменную\ndata_clean$sex_binary &lt;- ifelse(data_clean$sex == \"F\", 1, 0)\n\n# Подгонка модели\nmodel_logit &lt;- glm(sex_binary ~ length, \n                   data = data_clean, \n                   family = binomial(link = \"logit\"))\n\n# Расчет коэффициентов\nbeta0 &lt;- coef(model_logit)[1]\nbeta1 &lt;- coef(model_logit)[2]\n\n# Вычисление L50 (длина 50% созревания)\nL50 &lt;- round(-beta0/beta1, 1)\n\n# 3. Визуализация ------------------------------------------------------------\n# Логистическая кривая\nggplot(data_clean, aes(x = length, y = sex_binary)) +\n  geom_point(aes(color = sex), alpha = 0.6, size = 2) +\n  geom_line(aes(y = predict(model_logit, type = \"response\")), \n            color = \"#D81B60\", linewidth = 1.5) +\n  geom_vline(xintercept = L50, linetype = \"dashed\", color = \"#1E88E5\") +\n  annotate(\"text\", x = L50 + 2, y = 0.2, \n           label = paste(\"L50 =\", L50, \"мм\"), color = \"#1E88E5\") +\n  scale_color_manual(values = c(\"#FFC107\", \"#1976D2\")) +\n  labs(\n    title = \"Зависимость пола от длины карапакса\",\n    subtitle = \"Логистическая регрессия с 50%-ной точкой созревания\",\n    x = \"Длина карапакса (мм)\",\n    y = \"Вероятность быть самкой (P(F))\"\n  ) +\n  theme_minimal(base_size = 12)\n\n# Огива (кумулятивное распределение)\ndata_ogive &lt;- data_clean %&gt;%\n  arrange(length) %&gt;%\n  mutate(\n    cum_females = cumsum(sex_binary),\n    cum_prob = cum_females / max(cum_females)\n  )\n\nggplot(data_ogive, aes(x = length, y = cum_prob)) +\n  geom_line(color = \"#4CAF50\", linewidth = 1.5) +\n  geom_vline(xintercept = L50, linetype = \"dashed\", color = \"#1E88E5\") +\n  geom_hline(yintercept = 0.5, linetype = \"dotted\", color = \"#757575\") +\n  annotate(\"text\", x = L50 + 2, y = 0.55, \n           label = paste(\"50% созревание при\", L50, \"мм\"), color = \"#1E88E5\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    title = \"Огива: Кумулятивное распределение самок\",\n    x = \"Длина карапакса (мм)\",\n    y = \"Накопленная доля самок\"\n  ) +\n  theme_minimal(base_size = 12)\n\n# 4. Оценка модели -----------------------------------------------------------\n# ROC-анализ\nroc_obj &lt;- roc(data_clean$sex_binary, predict(model_logit, type = \"response\"))\nauc_value &lt;- round(auc(roc_obj), 2)\n\n# График ROC-кривой\nplot(roc_obj, col = \"#E53935\", main = paste(\"ROC-кривая (AUC =\", auc_value, \")\"))\n\n# 5. Сохранение результатов --------------------------------------------------\nggsave(\"logistic_curve.png\", width = 8, height = 6, dpi = 300)\nggsave(\"ogive_curve.png\", width = 8, height = 6, dpi = 300)\n\n# Вывод ключевых метрик\ncat(\"Результаты анализа:\\n\")\ncat(\"- Длина 50%-ного созревания (L50):\", L50, \"мм\\n\")\ncat(\"- AUC модели:\", auc_value, \"\\n\")\ncat(\"- Коэффициенты модели:\\n\")\ncat(\"  Intercept (β0):\", round(beta0, 2), \"\\n\")\ncat(\"  Slope (β1):\", round(beta1, 2), \"\\n\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Анализ и визуализация данных улова</span>"
    ]
  },
  {
    "objectID": "chapter 1.html#сравнение-групп-параметров-моделей",
    "href": "chapter 1.html#сравнение-групп-параметров-моделей",
    "title": "2  Анализ и визуализация данных улова",
    "section": "2.8 Сравнение групп, параметров, моделей",
    "text": "2.8 Сравнение групп, параметров, моделей\nСравнивать группы — это не про охоту за маленькими p-value, а про проверяемые ответы на конкретные биологические вопросы. «Самки длиннее самцов?» — переводим в аккуратную статистическую формулировку, начинаем с гигиены данных и только потом подбираем тест. Предобработка банальна, но критична: убираем очевидные аутлаеры по понятному правилу (IQR или заранее согласованный протокол), не «чистим» хвосты до совершенства, сохраняем независимость наблюдений. Разбиваем выборку на подмножества по полу, проверяем типы переменных, смотрим на формы распределений и пропуски. И дальше — не прыжок к t‑тесту, а короткая остановка у предпосылок: нормальность и гомогенность дисперсий — это про остатки и разумность аппроксимации, а не про «магическое число 0.05». При несхожих дисперсиях уместнее Уэлч, при явной ненормальности и неробастности — Манн–Уитни, а при больших n классический t‑тест часто держится благодаря центральной предельной теореме. В любом случае голые p‑значения не заканчивают разговор: эффект размера (Cohen’s d) и доверительные интервалы говорят «на сколько», а не только «есть/нет».\nВизуализация в этом месте — не иллюстрация, а часть доказательства. Boxplot/violin помогают увидеть медианы, разброс и асимметрию; добавленная на график оценка p‑value дисциплинирует интерпретацию, но не подменяет её. Полезно в той же системе координат показать точки, чтобы помнить: каждая точка — отдельная особь, а не абстрактная «генеральная совокупность». И если позволить себе короткую «сапольскину» ремарку: мозг с удовольствием «видит» разницу там, где её нет, поэтому лучше сначала смотреть на график, потом на число, а не наоборот.\nКогда вопрос — уже не «кто крупнее», а «кто растёт быстрее», сравнение средних сменяется сравнением параметров модели. Самый прозрачный путь — объединённая линейная модель с взаимодействием: length ~ age * sex. Значимый коэффициент при взаимодействии — это формализованная фраза «наклоны различаются». Диагностика здесь важнее, чем когда‑либо: линейность, разброс остатков, потенциальные leverage‑точки. Альтернатива — раздельные модели по полу и прямое сравнение наклонов через тест Вальда; он удобен как независимая проверка и часто даёт те же выводы, что и взаимодействие, если структура данных не экзотична. Интерпретация должна оставаться биологической: различающиеся наклоны — это не «магия пола», а потенциальная разница в темпе роста, доступе к корму или сезоне отбора проб.\nДальше мы неизбежно приходим к форме связи «вес–длина». Линейная модель соблазнительно проста, но биологически мир чаще степенной: масса масштабируется примерно как длина в степени 3, с поправками на форму и состояние. Полиномиальная регрессия третьего порядка часто выигрывает в AIC и R², потому что ловит сгибы и плечи; у неё есть и оборотная сторона — склонность к переобучению и слабая интерпретируемость коэффициентов. Степенная модель почти всегда немного проигрывает по «сухим метрикам», зато даёт ясный смысл: параметр b близок к 3 — всё ожидаемо; заметное отклонение — есть предмет для обсуждения физиологии, питания, сезонности. Какой из подходов «лучший»? Тот, у которого остатки ведут себя прилично, AIC не кричит о лишней сложности, а биолог рассказывает связную историю, не пряча глаза. Хорошая практика — сопоставить все три, показать таблицу R²/AIC, приложить графики остатков и проговорить компромисс между точностью и объяснимостью.\nИ в сравнении групп, и в сравнении параметров, и в выборе модели действуют три простых правила. Первое — формулируйте вопрос до теста: это экономит десятки необязательных проверок. Второе — показывайте эффект с интервалами: «на сколько» важнее «насколько значимо». Третье — проверяйте устойчивость: замены теста (t ↔︎ Уэлч ↔︎ Манн–Уитни), альтернативная спецификация модели, бутстрэп интервалов — всё это помогает отличить сигнал от удачного совпадения. И, наконец, не забывайте про контекст отбора проб: если улов по орудиям и глубинам неоднороден, то и выводы про «среднего самца» или «типичную самку» легко превращаются в выводы про «типичный улов». Статистическая аккуратность здесь — это не педантизм, а способ говорить о биологии без самообмана.\n\n2.8.1 Сравнение групп (на примере самцов и самок)\nРассмотрим методы сравнения количественных характеристик (длина, вес) между самцами и самками северной креветки. Анализ включает проверку нормальности распределения, выбор подходящего статистического теста и визуализацию различий.\n\n2.8.1.1 Подготовка данных\nЗагрузим данные и выделим подвыборки для самцов и самок:\n\n# Установка рабочей директории\nsetwd(\"C:/TEXTBOOK/\")\n\n# Загрузка библиотек  \nlibrary(tidyverse)  \nlibrary(ggplot2)  \nlibrary(rstatix)\nlibrary(ggpubr)\n\n# Загрузка данных  \ndata &lt;- read_csv(\"shrimp_catch.csv\") %&gt;%\n  filter(!id %in% c(10, 50))  # Удаление аномальных наблюдений \n\n# Фильтрация данных по полу  \nmales &lt;- data %&gt;% filter(sex == \"M\")  \nfemales &lt;- data %&gt;% filter(sex == \"F\") \n\n\n\n2.8.1.2 Проверка нормальности распределения\nПеред сравнением групп проверим, соответствуют ли данные нормальному распределению (тест Шапиро-Уилка):\n\n# Проверка нормальности для длины самцов  \nshapiro_test(males$length)  \n# Проверка нормальности для длины самок  \nshapiro_test(females$length) \n\nЕсли p-value &gt; 0.05, распределение считается нормальным. В противном случае используем непараметрические методы.\n\n\n2.8.1.3 Сравнение средних значений\nЕсли данные нормальны: t-тест\n\n# T-тест для сравнения длин самцов и самок  \nt_test_result &lt;- t_test(length ~ sex, data = data)  \nt_test_result \n\nЕсли данные не нормальны: U-тест Манна-Уитни\n\n# U-тест для сравнения длин самцов и самок  \nmannwhitney_result &lt;- wilcox_test(length ~ sex, data = data)  \nmannwhitney_result \n\n\n\n2.8.1.4 Эффект размера (коэффициент Коэна)\nДля оценки практической значимости различий рассчитаем коэффициент Коэна:\n\n# Расчет коэффициента Коэна  \ncohens_d_result &lt;- cohens_d(length ~ sex, data = data)  \ncohens_d_result  \n\n\nd &lt; 0.2 : малый эффект,\nd ≈ 0.5 : средний эффект,\nd &gt; 0.8 : большой эффект.\n\n\n\n2.8.1.5 Визуализация различий\nПостроим boxplot для визуального сравнения длин самцов и самок:\n\nggplot(data, aes(x = sex, y = length, fill = sex)) +  \n  geom_boxplot(color = \"black\", alpha = 0.7) +  \n  stat_compare_means(method = \"t.test\") +  # Добавление p-value  \n  labs(title = \"Сравнение длин самцов и самок\",  \n       x = \"Пол\", y = \"Длина карапакса (мм)\") +  \n  theme_minimal() \n\n\n\n\nРис. 1.10: Boxplot сравнения длин самцов и самок\n\n\n\n\n2.8.1.6 Интерпретация результатов\n\nЕсли p-value &lt; 0.05, различия между группами статистически значимы.\nЭффект размера помогает оценить биологическую важность различий. Например, если самки значительно крупнее самцов (d = 1.2), это может указывать на половой диморфизм, связанный с репродуктивной стратегией.\n2.8.1.7 Пример полного анализа для веса\n\n\n# Полный анализ для веса  \nweight_analysis &lt;- data %&gt;%  \n  group_by(sex) %&gt;%  \n  summarise(  \n    mean_weight = mean(weight),  \n    sd_weight = sd(weight),  \n    n = n()  \n  ) %&gt;%  \n  mutate(  \n    t_test = list(t_test(weight ~ sex, data = data)),  \n    cohens_d = list(cohens_d(weight ~ sex, data = data))  \n  )  \n\n# Вывод результатов  \nprint(weight_analysis) \n\n# Распределение веса по полу\nggplot(data, aes(x = factor(sex), y = weight, fill = factor(sex))) +\n  geom_violin(trim = FALSE, alpha = 0.7) +\n  geom_boxplot(width = 0.2, outlier.shape = NA, fill = \"white\") +\n  labs(title = \"Распределение веса по полу\", x = \"Пол\", y = \"Вес (г)\") +\n  theme_minimal()\n\n\n\n\nРис. 1.12: Violin plot для визуализации распределения веса\n\n\n\n\n2.8.1.8 Выводы\n\nИспользуйте t-тест для нормальных данных и U-тест для ненормальных.\nДополните анализ оценкой эффекта размера для биологической интерпретации.\nВизуализируйте различия с помощью boxplot или violin plot.\n\nРекомендации :\n\nДля многомерных данных (например, одновременное сравнение длины, веса и возраста) применяйте MANOVA.\nЕсли группы неоднородны (например, разный возрастной состав), используйте ковариационный анализ (ANCOVA).\n2.8.2 Что делать, если тест на нормальность не пройден для одной из групп?\nПри сравнении количественных характеристик (например, длины карапакса у самцов и самок) важно учитывать, соответствуют ли данные нормальному распределению. Если тест на нормальность (например, Шапиро-Уилка) показывает значимое отклонение от нормальности для одной из групп, это влияет на выбор статистического теста и интерпретацию результатов.\n2.8.2.1 Пример из нашего анализа\nМы провели сравнение длины карапакса между самцами и самками:\n\nДля самцов: shapiro_test(males$length) → p-value = 0.000574 (нормальность отвергнута).\nДля самок: shapiro_test(females$length) → p-value = 0.891 (нормальность подтверждена).\n\nНесмотря на это, мы применили как t-тест , так и U-тест Манна-Уитни :\n\nt-тест : p-value = 1.46e-40 (значимо).\nU-тест : p-value = 1.97e-27 (значимо).\nКоэффициент Коэна: d = 2.14 (большой эффект).\n\n2.8.2.2 Почему это работает?\n\nt-тест устойчив к умеренным отклонениям от нормальности :\n\nПри больших выборках (n &gt; 30) центральная предельная теорема позволяет использовать t-тест даже при слабо выраженной асимметрии.\nВ вашем случае выборка самцов (n = 149) достаточно велика, чтобы компенсировать отклонение от нормальности.\n\nU-тест Манна-Уитни — непараметрическая альтернатива :\n\nЭтот тест не требует нормальности и сравнивает ранги, а не средние значения.\nОн подтверждает значимость различий, что усиливает доверие к выводу.\n\nЭффект размера (коэффициент Кобена) :\n\nd = 2.14 указывает на большой эффект , что важно для биологической интерпретации, даже если p-values значимы.\n\n\n\n\n\n\n2.8.3 Сравнение параметров (линейные модели для оценки межгрупповых различий)\nДля сравнения параметров двух линейных моделей (например, скорости роста самцов и самок) используем следующий подход.\n\n# Установка рабочей директории\nsetwd(\"C:/TEXTBOOK/\")\n\n# Загрузка библиотек\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(knitr)\n\n# Загрузка данных\ndata &lt;- read_csv(\"shrimp_catch.csv\") %&gt;%\n  filter(!id %in% c(10, 50))  # Удаление аномальных наблюдений\n\n# Фильтрация данных по полу\ndata_male &lt;- data %&gt;% filter(sex == \"M\")\ndata_female &lt;- data %&gt;% filter(sex == \"F\")\n\n# Построение моделей\nmodel_male &lt;- lm(length ~ age, data = data_male)\nmodel_female &lt;- lm(length ~ age, data = data_female)\n\nggplot(data, aes(age, length, color = sex)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  scale_color_manual(values = c(\"#E7B800\", \"#00AFBB\")) +\n  labs(x = \"Возраст\", y = \"Длина (мм)\") +\n  theme_minimal()\n\n\n\n\nРис. 1.15: Визуализация моделей\n\n\nМетод 1: Объединенная модель с взаимодействиями\n\n# Установка рабочей директории\njoint_model &lt;- lm(length ~ age * sex, data = data)\nsummary(joint_model) %&gt;% \n  broom::tidy() %&gt;% \n  filter(term == \"age:sexM\") %&gt;% \n  kable(caption = \"Проверка различия наклонов\", digits = 3)\n\n\nTable: Проверка различия наклонов\n\n|term     | estimate| std.error| statistic| p.value|\n|:--------|--------:|---------:|---------:|-------:|\n|age:sexM |     1.86|     0.459|     4.053|       0|\n&gt; \n\nИнтерпретация:\nЗначимый коэффициент взаимодействия age:sexM (p &lt; 0.05) указывает на статистически значимые различия в скорости роста между полами.\nМетод 2: Тест Вальда\n\nlibrary(car)\ndelta_beta &lt;- coef(model_male)[\"age\"] - coef(model_female)[\"age\"]\nse_diff &lt;- sqrt(vcov(model_male)[\"age\",\"age\"] + vcov(model_female)[\"age\",\"age\"])\nz_score &lt;- delta_beta / se_diff\np_value &lt;- 2 * pnorm(-abs(z_score))\n\ncat(\"Разница коэффициентов:\", round(delta_beta, 3), \n    \"\\nZ-статистика:\", round(z_score, 3),\n    \"\\np-value:\", format.pval(p_value, digits = 2))\n\n\ncomparison_table &lt;- data.frame(\n  Параметр = c(\"Скорость роста самцов\", \"Скорость роста самок\", \"Разница\"),\n  Значение = c(\n    round(coef(model_male)[\"age\"], 2),\n    round(coef(model_female)[\"age\"], 2),\n    round(delta_beta, 2)\n  ),\n  `p-value` = c(\n    format.pval(summary(model_male)$coefficients[\"age\",4], digits = 2),\n    format.pval(summary(model_female)$coefficients[\"age\",4], digits = 2),\n    format.pval(p_value, digits = 2)\n  )\n)\nkable(comparison_table, caption = \"Сравнение коэффициентов роста\")\n\nВывод\n\n: Сравнение коэффициентов роста\n\n|Параметр              | Значение|p.value |\n|:---------------------|--------:|:-------|\n|Скорость роста самцов |     5.95|&lt;2e-16  |\n|Скорость роста самок  |     4.09|5.2e-13 |\n|Разница               |     1.86|0.00024 |\n&gt; \n\nИнтерпретация:\nЗначимая разница (p &lt; 0.05) указывает на статистически значимые различия в скорости роста между полами.\n\n\n2.8.4 Сравнение моделей\nОдним из ключевых аспектов анализа биологических данных является определение формы зависимости между переменными. В данном разделе мы рассмотрим основы подбора модели зависимости между длиной и весом креветок. Начиная с простой линейной модели, мы постепенно перейдем к более сложным нелинейным моделям, чтобы продемонстрировать методику выбора наилучшей модели. Cравним три модели — линейную, полиномиальную и степенную — чтобы определить, какая из них наилучшим образом описывает данные. Цель анализа — найти математическую зависимость, которая:\n\nТочно предсказывает вес креветки по её длине.\nИмеет биологическую интерпретацию.\nМинимизирует ошибку предсказания.\n\n\n2.8.4.1 Модели и их параметры\n\nЛинейная: \\(\\text{weight} = \\beta_0 + \\beta_1\\cdot\\text{length}\\)\nПолиномиальная 3-й степени: \\(\\text{weight} = \\beta_0 + \\beta_1\\cdot\\text{length} + \\beta_2\\cdot\\text{length}^2 + \\beta_3\\cdot\\text{length}^3\\)\nСтепенная: \\(\\text{weight} = a\\cdot\\text{length}^b\\)\n\n\n\n2.8.4.2 Метрики\n\nR² - (коэффициент детерминации): чем ближе к 1, тем лучше модель объясняет данные.\nAIC -(информационный критерий Акаике): чем меньше значение, тем лучше модель с учётом её сложности.\n\n\n\n2.8.4.3 Результаты\n\n2.8.4.3.1 1. Линейная модель\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.115      0.085     -24.86   &lt;2e-16 ***\nlength       0.1665     0.0038    43.71    &lt;2e-16 ***\n\n\nR² = 0.894\nAIC = 148.02\n\n\n\n\nРис. 1.5: Линейная модель\n\n\n\n\n2.8.4.3.2 2. Полиномиальная модель\n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \npoly(length,3)1  14.5038    0.2127    68.18   &lt;2e-16 ***\npoly(length,3)2   3.7209    0.2127    17.49   &lt;2e-16 ***\npoly(length,3)3   0.9526    0.2127     4.48  1.2e-05 ***\n\n\nR² = 0.957\nAIC = -52.80\n\n\n\n\nРис. 1.5: Полиномиальная модель\n\n\n\n\n2.8.4.3.3 3. Степенная модель\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \na 0.000157   0.000028    5.60  6.3e-08 ***\nb 2.920160   0.054102   53.98   &lt;2e-16 ***\n\n\nR² = 0.955\nAIC = -48.43 \n\n\n\n\n2.8.4.4 3. Сравнение моделей\n\n\n\nМодель\nR²\nAIC\n\n\n\n\nЛинейная\n0.894\n148.02\n\n\nПолиномиальная\n0.957\n-52.80\n\n\nСтепенная\n0.955\n-48.43\n\n\n\nВыводы:\n\nПолиномиальная модель демонстрирует наилучшие показатели (максимальный R² и минимальный AIC).\nСтепенная модель близка по качеству, но её параметр b≈2.92 близок к биологически ожидаемому значению 3 (вес пропорционален объёму).\nЛинейная модель существенно уступает по точности.\n\n\n\n2.8.4.5 4. Рекомендации\n\nДля прогнозирования используйте полиномиальную модель, так как она минимизирует ошибку.\nДля биологической интерпретации предпочтительна степенная модель: weight∝length2.92.\nИзбегайте переобучения: Полиномиальные модели высокой степени могут терять интерпретируемость.\n\n\n\n2.8.4.6 5. Визуализация остатков\nОстатки степенной модели распределены равномерно, что подтверждает её адекватность: \n\n\n2.8.4.7 Заключение\nДля анализа зависимости веса от длины северной креветки рекомендуется:\n\nПолиномиальная модель — для задач, требующих максимальной точности.\nСтепенная модель — для интерпретации биологических закономерностей.\n\nСкрипт вышеописанных событий:\n\n# Установка рабочей директории\nsetwd(\"C:/TEXTBOOK/\")\n\n# Загрузка библиотек\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Загрузка данных\ndata &lt;- read_csv(\"shrimp_catch.csv\") %&gt;%\n  filter(!id %in% c(10, 50))  # Удаление аномальных наблюдений\n\n# Проверка структуры\nglimpse(data)\n\n# Линейная модель: вес ~ длина\nmodel_linear &lt;- lm(weight ~ length, data = data)\nsummary(model_linear)\n\n# Визуализация\nggplot(data, aes(x = length, y = weight)) +\n  geom_point(color = \"steelblue\", alpha = 0.7) +\n  geom_smooth(method = \"lm\", color = \"#FC4E07\") +\n  labs(title = \"Линейная модель\", x = \"Длина (мм)\", y = \"Вес (г)\")\n\n\n# Полиномиальная модель: вес ~ длина + длина? + длина?\nmodel_poly &lt;- lm(weight ~ poly(length, 3), data = data)\nsummary(model_poly)\n\n# Визуализация\nggplot(data, aes(x = length, y = weight)) +\n  geom_point(color = \"steelblue\", alpha = 0.7) +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), color = \"#E7B800\") +\n  labs(title = \"Полиномиальная модель\", x = \"Длина (мм)\", y = \"Вес (г)\")\n\n\n# Степенная модель: вес ~ длина^k (k подбирается)\nmodel_power &lt;- nls(weight ~ a * length^b, \n                   data = data, \n                   start = list(a = 0.001, b = 3))  # Начальные значения\nsummary(model_power)\n\n# Визуализация\ndata$pred_power &lt;- predict(model_power)\nggplot(data, aes(x = length, y = weight)) +\n  geom_point(color = \"steelblue\", alpha = 0.7) +\n  geom_line(aes(y = pred_power), color = \"#00BA38\", linewidth = 1.2) +\n  labs(title = \"Степенная модель\", x = \"Длина (мм)\", y = \"Вес (г)\")\n\n# Расчет AIC\nAIC(model_linear, model_poly, model_power)\n\n# Расчет R?\nr2_linear &lt;- summary(model_linear)$r.squared\nr2_poly &lt;- summary(model_poly)$r.squared\nr2_power &lt;- 1 - sum(residuals(model_power)^2) / sum((data$weight - mean(data$weight))^2)\n\n# Создание таблицы сравнения моделей\ncomparison_table &lt;- data.frame(\n  Модель = c(\"Линейная\", \"Полиномиальная\", \"Степенная\"),\n  R_square = c(r2_linear, r2_poly, r2_power),\n  AIC = c(AIC(model_linear), AIC(model_poly), AIC(model_power))\n)\n\n# Вывод таблицы\nprint(comparison_table)\n\n# Остатки для степенной модели\ndata$residuals &lt;- residuals(model_power)\n\nggplot(data, aes(x = length, y = residuals)) +\n  geom_point(color = \"#FC4E07\", alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Остатки степенной модели\", x = \"Длина (мм)\", y = \"Ошибка\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Анализ и визуализация данных улова</span>"
    ]
  },
  {
    "objectID": "chapter 2.html",
    "href": "chapter 2.html",
    "title": "3  Нейронные сети в экологии: практическое введение",
    "section": "",
    "text": "3.1 Введение\nЭто практическое занятие — про то, как из разрозненных чисел сделать внятную экологическую историю и как перейти от простых регрессий к нейронным сетям, оставаясь честными перед данными. Мы используем R не из эстетики, а из прагматики: он позволяет прозрачно воспроизводить анализ, контролировать каждую трансформацию и быстро проверять гипотезы. В основе занятия — логика и примеры из статьи Андрея Викторовича Коросова «Нейронные сети в экологии: введение» (Принципы экологии, 2023, №3, 76–96). Там хорошо показан путь от классических линейных моделей к нелинейным конструкциям и дальше — к искусственным нейронным сетям, способным решать задачи классификации и прогнозирования. Мы пойдём тем же маршрутом, но с учебной расстановкой акцентов: сначала поймём, как работает «молоток» (регрессия), прежде чем брать в руки «многофункциональный инструмент» (сеть).\nЗадача занятия двоякая. Во‑первых, усвоить минимально достаточный набор статистических практик, чтобы не путать «эффект» с «удачным совпадением»: проверка предпосылок, визуальная диагностика, простые и понятные метрики качества, раздельные обучающие и тестовые выборки. Во‑вторых, увидеть, как усложнение модели должно быть мотивировано данными и биологией, а не нашей любовью к сложным методам. Если более простая модель объясняет всё, что вам нужно для решения прикладной задачи, смело берите её — мозг склонен влюбляться в красивое, но нам нужна работающая гипотеза.\nСтруктура занятия отражает эволюцию инструментов. Начнём с линейной регрессии на предельно понятном примере: связь массы и длины. Здесь важны не только коэффициенты и p‑значения, но и остатки, проверка линейности, гомоскедастичность, доверительные интервалы. Затем познакомимся с численной оптимизацией: когда аналитического решения нет, мы используем итерационные алгоритмы (nls) и учимся задавать стартовые значения, контролировать сходимость и чувствительность. Далее — множественная регрессия и вопрос интерпретации: что реально добавляет предиктор, а что «ездит зайцем» на коллинеарности. Оттуда естественно перейти к нелинейным зависимостям: аллометрия, линеаризация через логарифмы, сопоставление качества моделей не только по R², но и по AIC, и — что особенно важно — по поведению остатков. Логистическая регрессия вводит нас в мир пороговых процессов и бинарных исходов: S‑кривая, L50, ROC/AUC, калибровка вероятностей — всё это работает одинаково хорошо для токсичности дафний и для созревания по длине.\nКогда базовые кирпичики стоят, делаем шаг к нейронным сетям. Сначала показываем, что сеть без скрытых слоёв фактически воспроизводит линейную модель. Затем добавляем один скрытый нейрон и видим, как появляется возможность описывать нелинейности и пороговые эффекты. Дальше — классификация по нескольким признакам и небольшие архитектуры: оцениваем точность, избегаем утечки информации, фиксируем случайные зерна, обязательно сравниваем с простыми бэйзлайнами, чтобы не путать «мощнее» с «лучше». В финале — пример пространственного моделирования численности по биотопам: разделение на train/test, прогноз на новых условиях, разговор о переносимости моделей и ограничениях, без которых любые «красивые карты» остаются просто эстетикой.\nОрганизация работы предельно проста. Даны три версии скрипта: KOROSOV.R — максимально близко к оригиналу; KOROSOV_updated.R — тот же код с подробными комментариями и пояснениями (основной учебный вариант); KOROSOV_visual.R — дополненный продвинутой визуализацией и небольшой аналитикой качества. Для запуска понадобятся данные vipkar.csv и kihzsdat.csv, корректная рабочая директория в setwd() и набор пакетов (как минимум neuralnet и ggplot2). Мы сознательно держим зависимости минимальными, чтобы главный фокус был на методе и интерпретации, а не на обвязке.\nЧему вы научитесь и на что обращать внимание. Во‑первых, всегда проверять, что модель решает именно ваш вопрос: чёткая формулировка задачи до выбора алгоритма экономит половину времени. Во‑вторых, всегда показывать эффект и неопределённость: коэффициенты с интервалами, калибровка вероятностей, ошибки прогноза на независимых данных. В‑третьих, всегда сравнивать с простым бэйзлайном: если «сеть» не лучше честной регрессии на чистых признаках, значит, проблема не в архитектуре, а в данных или постановке. И да, старайтесь говорить языком биологии: «параметр b близок к 3» — это про объём, «L50 сдвинулся» — про созревание, «AUC высок, но калибровка плывёт» — про надёжность решений на уровне индивидуальных вероятностей.\nНаконец, про дисциплину и воспроизводимость. Фиксируйте seed, документируйте версии пакетов и исходные предположения, храните все промежуточные шаги в скриптах. Это скучно минуту, но экономит дни. И даже когда вы дойдёте до «сетей», помните: сложная модель — это не билет в истину, а всего лишь более гибкий аппроксиматор. Хорошая практика — держать рядом простой, интерпретируемый аналог и объяснять расхождения между ними. Тогда ваши результаты будут не просто «работать», а выдерживать обсуждение с биологами, инженерами и управленцами — то есть приносить пользу за пределами экрана.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Нейронные сети в экологии: практическое введение</span>"
    ]
  },
  {
    "objectID": "chapter 2.html#введение",
    "href": "chapter 2.html#введение",
    "title": "3  Нейронные сети в экологии: практическое введение",
    "section": "",
    "text": "3.1.0.1 Для работы скрипта:\n\nСкачайте файлы данных (vipkar.csv и kihzsdat.csv)\nУстановите рабочую директорию в setwd()\nУстановите необходимые пакеты : install.packages(c(\"neuralnet\", \"ggplot2\"))\n\n\n# ЗАГРУЗКА БИБЛИОТЕК И НАСТРОЙКА СРЕДЫ ================================\nlibrary(neuralnet)   # Для построения нейронных сетей\nlibrary(ggplot2)     # Для продвинутой визуализации (в данном скрипте не используется напрямую)\n\n# Установите свою рабочую директорию (где лежат файлы данных)\n# setwd(\"C:/ВАША_ДИРЕКТОРИЯ/\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Нейронные сети в экологии: практическое введение</span>"
    ]
  },
  {
    "objectID": "chapter 2.html#линейная-регрессия",
    "href": "chapter 2.html#линейная-регрессия",
    "title": "3  Нейронные сети в экологии: практическое введение",
    "section": "3.2 ЛИНЕЙНАЯ РЕГРЕССИЯ",
    "text": "3.2 ЛИНЕЙНАЯ РЕГРЕССИЯ\nВ этом разделе мы изучим основы экологического моделирования на примере зависимости массы тела гадюки от ее длины. Вы построите простую линейную регрессионную модель, визуализируете данные и линию регрессии, а также интерпретируете результаты с помощью функции summary().\nЗагружаем данные\n\n# Данные: масса (w) и длина тела (lt) гадюк (в см и граммах)\nw &lt;- c(85, 90, 85, 95, 95, 135, 165, 135, 140)\nlt &lt;- c(51, 51, 52, 54, 54, 59, 59, 60, 62)\n\nСтроим и запускаем модель \\[\nw_t = a_0 + a_1 \\cdot l_t\n\\]\nгде: - \\(w_t\\) — зависимая переменная, - \\(a_0\\) — свободный член, - \\(a_1\\) — коэффициент регрессии, - \\(l_t\\) — независимая переменная.\n\n# Построение линейной модели: w = a0 + a1*lt\nlreg &lt;- lm(w ~ lt)\n\nВыведем результаты модели\n\n# Просмотр результатов модели:\nsummary(lreg)  # Обратите внимание на коэффициенты и p-значения\n\nНа экране появится:\n\nCall:\nlm(formula = w ~ lt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.452  -7.585  -4.868   1.490  30.623 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -240.766     64.457  -3.735 0.007308 ** \nlt             6.358      1.153   5.516 0.000891 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 13.81 on 7 degrees of freedom\nMultiple R-squared:  0.813,     Adjusted R-squared:  0.7863 \nF-statistic: 30.43 on 1 and 7 DF,  p-value: 0.0008911\n\nМы получили результаты линейной регрессии, где зависимая переменная — масса тела гадюки (w), а независимая переменная — длина тела (lt). Разберем каждый параметр:\n1. **Call (Вызов модели):**\n`lm(formula = w ~ lt)`\nЭто просто напоминание, какая модель была построена. Здесь указано, что мы моделировали зависимость массы (w) от длины тела (lt) с помощью линейной регрессии.\n2. **Residuals (Остатки):**\nОстатки — это разница между наблюдаемыми значениями массы и предсказанными моделью значениями. Они показывают, насколько хорошо модель описывает данные.\n\n`Min`: минимальный остаток = -13.452 (наибольшее недооцененное значение)\n`1Q`: первый квартиль = -7.585 (25% остатков меньше этого значения)\n`Median`: медиана остатков = -4.868 (середина распределения остатков)\n`3Q`: третий квартиль = 1.490 (75% остатков меньше этого значения)\n`Max`: максимальный остаток = 30.623 (наибольшее переоцененное значение)\n\nРаспределение остатков: медиана немного смещена влево (отрицательное значение), а размах между 1Q и 3Q составляет примерно 9 единиц. Это может указывать на легкую асимметрию, но выборка мала.\n3. **Coefficients (Коэффициенты):**\n\n`(Intercept)`: свободный член (a0) = -240.766. Это предсказанное значение массы при длине тела, равной нулю. Биологически это не имеет смысла (длина не может быть нулевой), но это математическая особенность модели.\n`lt`: коэффициент регрессии (a1) = 6.358. Это означает, что при увеличении длины тела на 1 см масса тела увеличивается в среднем на 6.358 г.\n\nДля каждого коэффициента приведены:\n\n`Estimate`: точечная оценка коэффициента.\n`Std. Error`: стандартная ошибка оценки коэффициента. Для intercept = 64.457, для lt = 1.153. Это мера изменчивости оценки.\n`t value`: t-статистика. Рассчитывается как Estimate / Std.Error. Для intercept: -240.766 / 64.457 ≈ -3.735; для lt: 6.358 / 1.153 ≈ 5.516.\n`Pr(&gt;|t|)`: p-значение для проверки гипотезы о равенстве коэффициента нулю.\nДля intercept: p=0.007308 (значим на уровне α=0.01, т.е. intercept статистически значимо отличается от нуля).\nДля lt: p=0.000891 (значим на уровне α=0.001). Это означает, что длина тела значимо влияет на массу.\n\nЗначимость кодов: три звездочки (`***`) означают, что коэффициент значим на уровне 0.001.\n4. **Residual standard error (Стандартная ошибка остатков):** 13.81 на 7 степенях свободы. Это мера разброса остатков. В среднем, предсказания модели отклоняются от реальных значений на ±13.81 г. Степени свободы (df) = n - 2 = 9 - 2 = 7 (n — количество наблюдений).\n5. **Multiple R-squared (Коэффициент детерминации R²):** 0.813. Это означает, что 81.3% вариации массы тела объясняется длиной тела. Остальные 18.7% — это неучтенные факторы и случайная изменчивость.\n6. **Adjusted R-squared (Скорректированный R²):** 0.7863. Этот показатель корректирует R² с учетом числа предикторов. Он полезен при сравнении моделей с разным числом предикторов. Здесь он немного меньше R², так как учитывает, что в модели один предиктор.\n7. **F-statistic (F-статистика):** 30.43 на 1 и 7 степенях свободы. Проверяет гипотезу о том, что все коэффициенты (кроме intercept) равны нулю (т.е. модель не лучше, чем модель только с константой).\n\np-value: 0.0008911 (крайне значимый), что означает, что модель в целом адекватна.\n\n**Выводы:**\n- Уравнение модели: `w = -240.77 + 6.36 * lt`\n- Длина тела значимо влияет на массу (p&lt;0.001).\n- Модель объясняет 81.3% вариации массы.\n- На каждый сантиметр длины тела масса увеличивается примерно на 6.36 г.\n- Остатки модели показывают, что есть несколько точек, которые модель предсказывает с заметной ошибкой (особенно максимальный остаток в 30.6 г). Возможно, для более точного прогноза нужна нелинейная модель или учет дополнительных факторов.\n**Рекомендации:**\n- Проверить допущения линейной регрессии (нормальность остатков, гомоскедастичность) с помощью диагностических графиков.\n- Рассмотреть возможность включения других переменных (например, возраста, пола) в модель.\n- Убедиться, что в данных нет выбросов, которые могут влиять на коэффициенты.\n\n# Визуализация зависимости\nplot(lt, w, \n     main = \"Зависимость массы от длины тела гадюки\", \n     xlab = \"Длина тела (см)\", \n     ylab = \"Масса (г)\", \n     pch = 19,        # Кружки вместо стандартных точек\n     col = \"darkgreen\")\nabline(lreg, col = \"red\", lwd = 2)  # Добавляем линию регрессии\n\n\n\n\nРис. 1.: Пример линейной регрессии",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Нейронные сети в экологии: практическое введение</span>"
    ]
  },
  {
    "objectID": "chapter 2.html#численная-оптимизация",
    "href": "chapter 2.html#численная-оптимизация",
    "title": "3  Нейронные сети в экологии: практическое введение",
    "section": "3.3 ЧИСЛЕННАЯ ОПТИМИЗАЦИЯ",
    "text": "3.3 ЧИСЛЕННАЯ ОПТИМИЗАЦИЯ\nЗдесь вы познакомитесь с численными методами оптимизации параметров моделей, которые применяются, когда аналитическое решение невозможно. На примере той же зависимости массы от длины вы подгоните параметры модели с помощью функции nls() и сравните результаты с аналитическим решением.\nАналитические методы дают точное решение в виде математической формулы, используя алгебраические преобразования и теоремы математического анализа. Они идеальны для простых моделей, где существуют явные решения, обеспечивая прозрачную интерпретацию параметров. В экологии такие методы применимы для базовых зависимостей типа линейной регрессии. Численные методы используются, когда аналитическое решение невозможно, и работают через последовательные приближения, начиная со стартовых значений и итеративно улучшая параметры модели. Они незаменимы для сложных экологических моделей с нелинейными зависимостями, взаимодействиями факторов и “шумными” полевыми данными, позволяя решать задачи, недоступные для аналитических подходов.\n\n# Подгонка параметров через оптимизацию\nnls_model &lt;- nls(w ~ a0 + a1 * lt, start = list(a0 = 1, a1 = 1))\nsummary(nls_model)\n\nНа экране появится:\n\nFormula: w ~ a0 + a1 * lt\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \na0 -240.766     64.457  -3.735 0.007308 ** \na1    6.358      1.153   5.516 0.000891 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 13.81 on 7 degrees of freedom\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 3.247e-08\n\n\n3.3.1 Интерпретация результатов модели\nМы построили линейную модель зависимости массы гадюки (w) от длины её тела (lt) по формуле:\nw = a0 + a1 * lt\nКлючевые параметры модели:\n\na0 (свободный член): -240.8 г\nЭто теоретическая масса при нулевой длине тела. Отрицательное значение указывает, что модель не подходит для очень молодых особей.\na1 (коэффициент при lt): 6.36 г/см\nКаждый дополнительный сантиметр длины тела увеличивает массу в среднем на 6.36 г.\n\nТочность и значимость:\n\nОба коэффициента высоко значимы (p &lt; 0.01), что подтверждает реальность зависимости.\nСтандартная ошибка для a1 составляет 1.15 г/см - это значит, что реальное значение, вероятно, находится между 5.2 и 7.5 г/см.\nМодель хорошо сошлась за 1 шаг (итерацию), что говорит об удачном подборе параметров.\n\nОшибка прогноза:\nСреднее отклонение предсказаний от реальных значений - 13.8 г (стандартная ошибка остатков). Для особи массой 100 г это означает возможную ошибку прогноза около 14%.\n\nБиологический смысл: Модель подтверждает сильную аллометрию - крупные гадюки имеют относительно большую массу тела. Каждый сантиметр длины добавляет около 6.4 г массы. Для особи длиной 55 см прогнозируемая масса составит: -240.8 + 6.36*55 ≈ 109 г.\n\n##МНОЖЕСТВЕННАЯ РЕГРЕССИЯ\nВ этом разделе мы расширим модель, включив несколько факторов. Вы построите множественную регрессию, учитывающую одновременно длину тела и длину хвоста гадюки, и научитесь интерпретировать влияние нескольких предикторов на зависимую переменную.\n\n# Добавляем новый признак - длину хвоста (lc)\nw &lt;- c(40, 156, 105, 85, 80, 50, 75, 48, 75, 67)\nlt &lt;- c(44, 59, 49, 50, 54, 43, 49, 42, 47, 47)\nlc &lt;- c(70, 78, 66, 90, 83, 70, 62, 75, 40, 80)\n\nИспользуя glm-функцию, построим модель с двумя предикторами: \\[\nw = a_0 + a_1 \\cdot l_t + a_2 \\cdot l_c\n\\]\nгде: - \\(w\\) — масса гадюки, - \\(l_t\\) — длина тела гадюки, - \\(l_c\\) — длина хвоста гадюки, - \\(a_0\\) — свободный член (константа), - \\(a_1\\) — коэффициент регрессии при длине тела, - \\(a_2\\) — коэффициент регрессии при длине хвоста.\n\n# Множественная регрессия: w = a0 + a1*lt + a2*lc\nmulti_reg &lt;- glm(w ~ lt + lc)\nsummary(multi_reg)\n\nНа экране появится:\n\nCall:\nglm(formula = w ~ lt + lc)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -191.2982    53.6908  -3.563 0.009183 ** \nlt             6.0308     1.1051   5.457 0.000949 ***\nlc            -0.3150     0.4133  -0.762 0.470913    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n(Dispersion parameter for gaussian family taken to be 270.9752)\n\n    Null deviance: 10132.9  on 9  degrees of freedom\nResidual deviance:  1896.8  on 7  degrees of freedom\nAIC: 88.832\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n3.3.2 Интерпретация результатов множественной регрессии\nМы исследовали зависимость массы гадюки (w) от длины тела (lt) и длины хвоста (lc) с помощью модели:\nw = b0 + b1*lt + b2*lc\nКлючевые выводы модели:\n\nДлина тела (lt) сильно влияет на массу:\n\nКоэффициент: +6.03 г/см\nКаждый сантиметр длины тела увеличивает массу на ~6 г\nВысокая значимость (p = 0.00095)\n\nДлина хвоста (lc) не влияет значимо на массу:\n\nКоэффициент: -0.315 г/см (незначимый)\np-значение 0.47 &gt; 0.05 - статистически недостоверно\nПосле учета длины тела, длина хвоста не добавляет информации\n\nСвободный член (b0): -191.3 г\nОтрицательное значение подтверждает нелинейность роста у молодых особей\n\nКачество модели:\n\nМодель объясняет значительную часть вариации:\nОбщая вариация (Null deviance) = 10132.9\nОстаточная вариация (Residual deviance) = 1896.8 → Объяснено 81% вариации\nAIC = 88.8 (ниже, чем у модели без lc - 92.1, что указывает на лучшее качество)\nМодель быстро сошлась за 2 итерации\n\nБиологическая интерпретация:\n\nМасса тела определяется в основном длиной туловища, а не хвоста\nДля прогноза массы достаточно учитывать только длину тела\nПример прогноза для особи (lt=50 см, lc=70 см):\n-191.3 + 6.03*50 - 0.315*70 ≈ 111 г\n\n\nРекомендация: При изучении массы гадюк можно исключить длину хвоста из модели, так как она не вносит значимого вклада в предсказание. Основным морфометрическим показателем остается длина тела.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Нейронные сети в экологии: практическое введение</span>"
    ]
  },
  {
    "objectID": "chapter 2.html#нелинейные-зависимости",
    "href": "chapter 2.html#нелинейные-зависимости",
    "title": "3  Нейронные сети в экологии: практическое введение",
    "section": "3.4 НЕЛИНЕЙНЫЕ ЗАВИСИМОСТИ",
    "text": "3.4 НЕЛИНЕЙНЫЕ ЗАВИСИМОСТИ\nЭкологические данные часто имеют нелинейный характер. Здесь вы смоделируете степенную зависимость (аллометрию) между массой и длиной тела, используя линеаризацию через логарифмирование, а затем визуализируете кривую модели.\n\n# Часто в экологии связи имеют степенной характер: w = a0 * lt^a1\n# Линеаризация через логарифмирование\nlog_model &lt;- lm(log(w) ~ log(lt))\n\n# Преобразование коэффициентов обратно\na0 &lt;- exp(coef(log_model)[1])  # Переход от логарифмов\na1 &lt;- coef(log_model)[2]       # Показатель степени\n\n# Визуализация степенной зависимости\nplot(lt, w, \n     main = \"Степенная зависимость массы от длины\", \n     xlab = \"Длина тела (см)\", \n     ylab = \"Масса (г)\",\n     pch = 17,\n     col = \"blue\")\ncurve(a0 * x^a1, add = TRUE, col = \"red\", lwd = 2)  # Кривая модели\n\n\n\n\nРис. 2.: Расчет степенной функции",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Нейронные сети в экологии: практическое введение</span>"
    ]
  },
  {
    "objectID": "chapter 2.html#логистическая-регрессия",
    "href": "chapter 2.html#логистическая-регрессия",
    "title": "3  Нейронные сети в экологии: практическое введение",
    "section": "3.5 ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ",
    "text": "3.5 ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ\nВы изучите моделирование пороговых эффектов в экологии на примере смертности дафний в зависимости от концентрации токсиканта. Построив логистическую регрессию, вы получите S-образную кривую, характерную для таких процессов.\n\n# Пример: смертность дафний при разных концентрациях токсиканта\n# Данные:\nK &lt;- c(100, 126, 158, 200, 251, 316, 398, 501, 631, 794, 1000)\np &lt;- c(0, 0, 0, 0, 0, 0.5, 0.5, 1, 1, 1, 1)  # Доля погибших\nd &lt;- data.frame(K, p)\n\n# Построение логистической модели\nlogit_model &lt;- glm(p ~ K, family = binomial(), data = d)\n\n# Визуализация S-образной кривой\nplot(d$K, d$p, \n     xlab = \"Концентрация токсиканта (мг/л)\", \n     ylab = \"Доля погибших\", \n     main = \"Токсическое воздействие на дафний\",\n     pch = 19,\n     col = \"red\")\nlines(d$K, predict(logit_model, type = \"response\"), \n      col = \"blue\", lwd = 2, lty = 1)\n\n\n\n\nРис. 3.: Расчет логистической регрессии гибели дафний в токсиканте",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Нейронные сети в экологии: практическое введение</span>"
    ]
  },
  {
    "objectID": "chapter 2.html#переход-к-сетям",
    "href": "chapter 2.html#переход-к-сетям",
    "title": "3  Нейронные сети в экологии: практическое введение",
    "section": "3.6 ПЕРЕХОД К СЕТЯМ",
    "text": "3.6 ПЕРЕХОД К СЕТЯМ\nСделаем первый шаг к нейронным сетям, построив простейшую сеть без скрытых слоев (аналог линейной регрессии) для модели токсичности. Вы визуализируете структуру сети и убедитесь, что она дает результат, аналогичный линейной модели.\n\n# Простейшая нейросеть (аналог линейной регрессии)\nnn_simple &lt;- neuralnet(p ~ K, data = d, hidden = 0)\n\n# Визуализация структуры сети\nplot(nn_simple, rep = \"best\")\n\n\n\n\nРис. 4.: Схема нейрона",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Нейронные сети в экологии: практическое введение</span>"
    ]
  },
  {
    "objectID": "chapter 2.html#нейроны-как-нелинейные-преобразователи",
    "href": "chapter 2.html#нейроны-как-нелинейные-преобразователи",
    "title": "3  Нейронные сети в экологии: практическое введение",
    "section": "3.7 НЕЙРОНЫ КАК НЕЛИНЕЙНЫЕ ПРЕОБРАЗОВАТЕЛИ",
    "text": "3.7 НЕЙРОНЫ КАК НЕЛИНЕЙНЫЕ ПРЕОБРАЗОВАТЕЛИ\nЗдесь вы добавите в нейронную сеть скрытый слой с одним нейроном, что позволит моделировать нелинейные зависимости. Вы сравните результат работы такой сети с логистической регрессией и увидите, как нейронная сеть имитирует пороговый эффект.\n\n# Сеть с одним скрытым нейроном (имитирует логистическую регрессию)\nnn_1hidden &lt;- neuralnet(p ~ K, data = d, hidden = 1)\n\n# Сравнение с логистической регрессией\nplot(d$K, predict(logit_model, type = \"response\"), \n     type = \"l\", \n     col = \"darkgreen\", \n     lwd = 2,\n     xlab = \"Концентрация\", \n     ylab = \"Смертность\",\n     main = \"Сравнение моделей\")\nlines(d$K, predict(nn_1hidden, d), col = \"blue\", lty = 2, lwd = 2)\nlegend(\"bottomright\", \n       legend = c(\"Логистическая регрессия\", \"Нейронная сеть (1 нейрон)\"),\n       col = c(\"darkgreen\", \"blue\"), \n       lty = 1:2,\n       lwd = 2)\n\n\n\n\nРис. 5.: Сравнение работы",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Нейронные сети в экологии: практическое введение</span>"
    ]
  },
  {
    "objectID": "chapter 2.html#классификация-в-экологии",
    "href": "chapter 2.html#классификация-в-экологии",
    "title": "3  Нейронные сети в экологии: практическое введение",
    "section": "3.8 КЛАССИФИКАЦИЯ В ЭКОЛОГИИ",
    "text": "3.8 КЛАССИФИКАЦИЯ В ЭКОЛОГИИ\nВы примените нейронные сети для решения задачи классификации - определения пола гадюк по морфометрическим признакам. Построив и сравнив несколько архитектур сетей (без скрытых нейронов, с одним и тремя нейронами), вы оцените их точность.\n\n# Загрузка данных по гадюкам (пол, длина тела, длина хвоста, масса)\nv &lt;- read.csv(\"vipkar.csv\")\nhead(v, 3)  # Просмотр первых строк данных\n\nМодель без скрытых нейронов (аналог линейной регрессии)\n\nnv0 &lt;- neuralnet(ns ~ lc, data = v, hidden = 0)\nplot(nv0)  # Визуализация простейшей сети\n\n\n\n\nРис. 6.: Визуализация простейшей сети\n\n\nМодель с одним скрытым нейроном\n\nnv1 &lt;- neuralnet(ns ~ lc, data = v, hidden = 1)\nplot(nv1)  # Схема сети с одним нейроном\n\n\n\n\nРис. 7.: Схема сети с одним нейроном\n\n\nМодель с тремя скрытыми нейронами (полноценная нейросеть)\n\nnv3 &lt;- neuralnet(ns ~ lc + lt + w, data = v, hidden = 3)\nplot(nv3)  # Визуализация сложной сети\n\n\n\n\nРис. 8.: Модель с тремя скрытыми нейронами\n\n\nОценка точности классификации\n\npredictions &lt;- predict(nv3, v)\npredicted_sex &lt;- round(predictions)\naccuracy &lt;- mean(v$ns == predicted_sex)\ncat(\"Точность классификации:\", round(accuracy*100, 1), \"%\\n\")\n\nСравнение разных архитектур нейронных сетей (см. срипт KOROSOV_visual.R)\n\n\n\nРис. 9.: Точность определения пола гадюк",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Нейронные сети в экологии: практическое введение</span>"
    ]
  },
  {
    "objectID": "chapter 2.html#пространственное-моделирование",
    "href": "chapter 2.html#пространственное-моделирование",
    "title": "3  Нейронные сети в экологии: практическое введение",
    "section": "3.9 ПРОСТРАНСТВЕННОЕ МОДЕЛИРОВАНИЕ",
    "text": "3.9 ПРОСТРАНСТВЕННОЕ МОДЕЛИРОВАНИЕ\nВ завершение вы построите нейронную сеть для прогнозирования численности гадюк на островах по характеристикам биотопов. Вы разделите данные на обучающую и тестовую выборки, оцените точность модели и используете ее для прогноза в новых условиях.\n\n# Данные по островам Кижского архипелага\nv &lt;- read.csv(\"kihzsdat.csv\")\nhead(v, 3)  # Структура данных: площадь, биотопы, численность видов\n\n# Случайное разделение данных на обучающую и тестовую выборки\nset.seed(123)  # Для воспроизводимости\ntrain_indices &lt;- sample(1:nrow(v), 12)\ntrain_data &lt;- v[train_indices, ]\ntest_data &lt;- v[-train_indices, ]\n\n# Построение нейросети с 5 нейронами в скрытом слое\nmodel &lt;- neuralnet(vb ~ fo + me + bo, data = train_data, hidden = 5)\n\n# Прогнозирование на обучающей выборке\ntrain_pred &lt;- predict(model, train_data)\ntrain_accuracy &lt;- mean(round(train_pred) == train_data$vb)\ncat(\"Точность на обучающей выборке:\", round(train_accuracy*100, 1), \"%\\n\")\n\n# Прогнозирование на тестовой выборке\ntest_pred &lt;- predict(model, test_data)\ntest_accuracy &lt;- mean(round(test_pred) == test_data$vb)\ncat(\"Точность на тестовой выборке:\", round(test_accuracy*100, 1), \"%\\n\")\n\n# Прогноз для новых условий (пример)\nnew_conditions &lt;- data.frame(\n  fo = c(57.9, 35.3, 83.0),  # Площадь лесов (%)\n  me = c(4.1, 0.0, 7.3),     # Площадь лугов (%)\n  bo = c(3.4, 7.9, 11.5)     # Площадь болот (%)\n)\n\nfuture_pred &lt;- predict(model, new_conditions)\ncat(\"Прогнозируемая численность гадюк:\", round(future_pred), \"\\n\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Нейронные сети в экологии: практическое введение</span>"
    ]
  },
  {
    "objectID": "chapter 3.html",
    "href": "chapter 3.html",
    "title": "4  Основы картографии",
    "section": "",
    "text": "4.1 Введение\nЭто занятие — про то, как превратить координаты и уловы в честные карты, которые помогают думать, а не просто украшать отчёт. Мы будем работать в R, потому что он дисциплинирует: каждая операция видна, воспроизводима и проверяема, а любой красивый результат можно разобрать до строчки кода. В логике курса мы пойдём от простого к сложному: от базовой точечной карты распределения уловов — к картам с береговой линией, к учёту нулевых уловов и разбиению по квартилям, к фасетам для сравнения лет, к локальной автокорреляции (LISA), к промысловым картам и картограммам, и, наконец, к гибридным решениям, где данные съёмок и промысла встречаются на одной карте. В конце добавим «служебные» карты для раздела «Материал и методы» и карты с врезками — тот самый минимум, который ожидают рецензенты. В духе Ноама Хомского напомним: карта — это не территория, а модель наших предположений; чем прозрачнее исходные решения (данные, проекция, шкалы), тем меньше поводов для самообмана.\nВ основе любой качественной карты лежат три вещи: корректная подготовка данных, грамотная картографическая основа и осмысленная визуальная метафора. Сначала убеждаемся, что координаты в единой системе (WGS84), отсутствуют перепутанные долготы и широты, а нулевые уловы размечены как нули, а не пропуски. Затем выбираем основу: береговая линия и полигоны стран из rnaturalearth, при необходимости батиметрия из marmap, корректная проекция для расстояний и площадей (в задачах локального масштаба — UTM). И только после этого — визуальные решения: непрерывные шкалы с воспринимаемо ровной палитрой (viridis), понятная легенда, единицы измерения, подписи, масштабная линейка и, где уместно, стрелка «север». Важный этический момент: размер и цвет несут разные смыслы; не заставляйте читателя угадывать, что из них интенсивность, а что — частота или категория. Нулевые уловы — это не «мусор», а сильный сигнал об отсутствии; показывайте их отдельным слоем и символом, чтобы не переоценивать «горячие точки».\nПо мере усложнения задач мы добавляем структуру. Разбиение по квартилям даёт сопоставимость между годами, фасеты позволяют увидеть межгодовую динамику без наложения, LISA подсвечивает кластеры высокой и низкой интенсивности и аномалии, где значение точки расходится с окружением. Картограммы или сеточная агрегация помогают уйти от шумной точки к устойчивой картинке на уровне промысловых квадратов; гибридные карты честно показывают возможный разрыв между научной съёмкой и промыслом. Здесь важно помнить про «анатомию ошибки»: выбор размера ячейки, числа соседей в LISA, границ квартилей и способа агрегации — это не техническая деталь, а модельное решение; фиксируйте его явно, чтобы завтра вы сами могли воспроизвести сегодняшнюю карту.\nПрактический результат должен быть пригоден для публикации. Все примеры в R можно экспортировать в векторные форматы (PDF, SVG) и растровые (PNG, TIFF) с высоким разрешением, где подписи, легенды и цвета сохраняют читаемость при печати. В скрипте мы покажем, как автоматически подбирать границы области с небольшим буфером — и почему чаще лучше задать их вручную, чтобы карта не «гуляла» между рис. 1 и рис. 2. Мы разберём, как организовать легенды, чтобы они не спорили друг с другом при фасетировании, как синхронизировать цветовые шкалы между годами, чтобы зелёное «вчера» и зелёное «сегодня» значили одно и то же, и как использовать врезку, чтобы читатель понял контекст региона, а не искал его на глобусе.\nНаконец, про дисциплину и воспроизводимость. Данные берём из одного файла (KARTOGRAPHIC.xlsx), зависимости минимальны и явно перечислены, рабочая директория задаётся в начале, все параметры карт — на виду. Такой стиль не только ускоряет работу, но и воспитывает привычку проверять себя: если карта получилась слишком «красивая», вернитесь и взгляните на нули, на шкалы, на проекцию, на подписи. Хорошая карта в рыбохозяйственной и гидробиологической практике — это не “арт‑объект”, а прозрачный инструмент коммуникации: с ней удобно спорить, её можно повторить и на её основе можно принять решение. Если к концу занятия вы без подсказок соберёте три‑четыре типовых карты для результатов и одну аккуратную для «Материалов и методов», задача занятия выполнена.\nДля работы скрипта:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#введение",
    "href": "chapter 3.html#введение",
    "title": "4  Основы картографии",
    "section": "",
    "text": "Скачайте файл данных (KARTOGRAPHIC.xlsx)\nУстановите рабочую директорию в setwd()\nУстановите необходимые пакеты : install.packages(c(\"readxl\", \"tidyverse, \"rnaturalearth\", \"sf\", \"viridis\" )) и др.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#карта-распределения-уловов-в-съемке",
    "href": "chapter 3.html#карта-распределения-уловов-в-съемке",
    "title": "4  Основы картографии",
    "section": "4.2 Карта распределения уловов в съемке",
    "text": "4.2 Карта распределения уловов в съемке\nДанная карта демонстрирует распределение уловов краба в ходе исследовательской съемки. На ней отображены точки наблюдений, где размер и цвет точек соответствуют величине улова.\n\n\n\nРис. 1.: Пример карты распределения уловов в съемке\n\n\nВ скрипте границы карты (лимиты) определяются автоматически с буфером, но чаще их просто устанавливают вручную, например:\n\nxmin &lt;- 37\nxmax &lt;- 49\nymin &lt;- 68.5\nymax &lt;- 70.5\n\nСкрипт карты целиком:\n\n# Очистка памяти и установка рабочей папки\nrm(list = ls())\nsetwd(\"C:/COURSES/KARTOGRAPH/\")\n\n# Загрузка необходимых пакетов\nlibrary(tidyverse)  # Обработка данных и визуализация\nlibrary(readxl)     # Чтение Excel-файлов\n\n# 1. ЗАГРУЗКА ДАННЫХ\nDATA &lt;- read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\") %&gt;% \n  filter(YEAR == 2023, SURV == \"CRAB\")  # Фильтр для 2023 года и съемки CRAB\n\n# 2. АВТОМАТИЧЕСКИЙ РАСЧЕТ ГРАНИЦ С БУФЕРОМ 5%\n# Расчет диапазонов координат\nx_range &lt;- range(DATA$X, na.rm = TRUE)\ny_range &lt;- range(DATA$Y, na.rm = TRUE)\n\n# Расчет 5% буфера\nx_buffer &lt;- 0.05 * diff(x_range)\ny_buffer &lt;- 0.05 * diff(y_range)\n\n# Установка границ с буфером\nxmin &lt;- x_range[1] - x_buffer\nxmax &lt;- x_range[2] + x_buffer\nymin &lt;- y_range[1] - y_buffer\nymax &lt;- y_range[2] + y_buffer\n\n# 3. ВИЗУАЛИЗАЦИЯ ТОЧЕК\nggplot(DATA) +\n  # Точки наблюдений с размером и цветом по величине улова\n  geom_point(aes(x = X, y = Y, size = PROM, color = PROM), alpha = 0.7) +\n  \n  # Цветовая шкала (виридисная палитра)\n  scale_color_viridis_c(option = \"H\", name = \"Улов\") +\n  \n  # Шкала размеров точек\n  scale_size_continuous(name = \"Улов\") +\n  \n  # Настройка границ с автоматически рассчитанными значениями\n  coord_cartesian(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +\n  \n  # Подписи осей\n  labs(x = \"Долгота\", y = \"Широта\", \n       title = \"Распределение уловов краба\", \n       subtitle = \"2023 год, тип съемки: CRAB\") +\n  \n  # Оформление графика\n  theme_bw() +\n  theme(\n    panel.grid = element_line(color = \"grey90\"),\n    legend.position = \"bottom\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#карта-распределения-уловов-в-съемке-с-береговой-линией",
    "href": "chapter 3.html#карта-распределения-уловов-в-съемке-с-береговой-линией",
    "title": "4  Основы картографии",
    "section": "4.3 Карта распределения уловов в съемке с береговой линией",
    "text": "4.3 Карта распределения уловов в съемке с береговой линией\n\n\n\nРис. 2.: Пример карты распределения уловов в съемке с береговой линией\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls())  # Удаление всех объектов из глобального окружения\nsetwd(\"C:/COURSES/KARTOGRAPH/\")  # Установка рабочей директории\n\n# Загрузка необходимых библиотек\nlibrary(rnaturalearth)  # Для получения векторных карт мира\nlibrary(tidyverse)      # Коллекция пакетов для работы с данными\nlibrary(sf)             # Пространственный анализ\n\n####### ЗАГРУЗКА ДАННЫХ И ПОДГОТОВКА ПРОСТРАНСТВЕННЫХ ОБЪЕКТОВ ################\n\n# Чтение и фильтрация данных\nDATA &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\") %&gt;% \n  filter(YEAR == 2023, SURV == \"CRAB\")  # Фильтр данных за 2023 год по типу съемки\n\n# Получение границ России\nrussia &lt;- ne_countries(scale = 10, country = \"Russia\")  # Загрузка векторных границ (масштаб 1:10м)\n\n# Установка границ отображаемой области (долгота/широта)\nxmin=37  # Западная граница\nxmax=49  # Восточная граница\nymin=68.5 # Южная граница\nymax=70.5 # Северная граница\n\n# Построение карты\nggplot() +\n  # Базовая карта России\n  geom_sf(data = russia, fill = \"lightblue\") + \n  # Ограничение области отображения\n  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +\n  # Точки наблюдений с размером и цветом по переменной PROM\n  geom_point(aes(x = X, y = Y, size = PROM, color = PROM),\n             data = DATA, alpha = 0.6) +\n  # Цветовая шкала (viridis, вариант H)\n  scale_color_viridis_c(option = \"H\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#карта-распределения-уловов-включая-нулевые",
    "href": "chapter 3.html#карта-распределения-уловов-включая-нулевые",
    "title": "4  Основы картографии",
    "section": "4.4 Карта распределения уловов, включая нулевые",
    "text": "4.4 Карта распределения уловов, включая нулевые\n\n\n\nРис. 3.: Карта распределения уловов, включая нулевые\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls())  # Удаление всех объектов из глобального окружения\nsetwd(\"C:/COURSES/KARTOGRAPH/\")  # Установка рабочей директории\n\n# Загрузка необходимых библиотек\nlibrary(rnaturalearth)  # Для получения векторных карт мира\nlibrary(tidyverse)      # Коллекция пакетов для работы с данными\nlibrary(sf)             # Пространственный анализ\n\n####### ЗАГРУЗКА ДАННЫХ И ПОДГОТОВКА ПРОСТРАНСТВЕННЫХ ОБЪЕКТОВ ################\n\n# Чтение и фильтрация данных\nDATA &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\") %&gt;% \n  filter(YEAR == 2023, SURV == \"CRAB\")  # Фильтр данных за 2023 год по типу съемки\n\n# Получение границ России\nrussia &lt;- ne_countries(scale = 10, country = \"Russia\")  # Загрузка векторных границ (масштаб 1:10м)\n\n# Установка границ отображаемой области (долгота/широта)\nxmin=37  # Западная граница\nxmax=49  # Восточная граница\nymin=68.5 # Южная граница\nymax=70.5 # Северная граница\n\n# Построение карты\nggplot() +\n  # Базовая карта России\n  geom_sf(data = russia, fill = \"lightblue\") + \n  # Ограничение области отображения\n  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +\n  # Точки наблюдений с размером и цветом по переменной PROM (ненулевые уловы)\n  geom_point(aes(x = X, y = Y, size = PROM, color = PROM),\n             data = filter(DATA, PROM &gt; 0), alpha = 0.6) +\n  # Точки для нулевых уловов (крестики)\n  geom_point(aes(x = X, y = Y),\n             data = filter(DATA, PROM == 0),\n             shape = 4, size = 1, stroke = 1, color = \"black\") +\n  # Цветовая шкала (viridis, вариант H)\n  scale_color_viridis_c(option = \"H\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#карта-распределения-уловов-распределенных-по-квартилям",
    "href": "chapter 3.html#карта-распределения-уловов-распределенных-по-квартилям",
    "title": "4  Основы картографии",
    "section": "4.5 Карта распределения уловов, распределенных по квартилям",
    "text": "4.5 Карта распределения уловов, распределенных по квартилям\n\n\n\nРис. 4.: Карта распределения уловов, распределенных по квартилям\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls())\nsetwd(\"C:/COURSES/KARTOGRAPH/\")\n\n# Загрузка необходимых библиотек\nlibrary(rnaturalearth)\nlibrary(tidyverse)\nlibrary(sf)\n\n####### ЗАГРУЗКА ДАННЫХ И ПОДГОТОВКА ПРОСТРАНСТВЕННЫХ ОБЪЕКТОВ ################\n\n# Чтение и фильтрация данных\nDATA &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\") %&gt;% \n  filter(YEAR == 2023, SURV == \"CRAB\")\n\n# Получение границ России\nrussia &lt;- ne_countries(scale = 10, country = \"Russia\") %&gt;% \n  st_as_sf()\n\n# Установка границ отображаемой области\nxmin=37; xmax=49; ymin=68.5; ymax=70.5\n\n####### ПОДГОТОВКА ДАННЫХ ДЛЯ ВИЗУАЛИЗАЦИИ ################\n# Вычисляем квартили отдельно\nquantiles &lt;- quantile(DATA$PROM[DATA$PROM &gt; 0], probs = seq(0, 1, 0.25))\n\n# Создаем 4 категории с реальными диапазонами значений\nnonzero_data &lt;- DATA %&gt;% \n  filter(PROM &gt; 0) %&gt;%\n  mutate(\n    PROM_cat = cut(\n      PROM,\n      breaks = quantiles,\n      include.lowest = TRUE,\n      labels = c(\n        sprintf(\"%.1f - %.1f\", quantiles[1], quantiles[2]),\n        sprintf(\"%.1f - %.1f\", quantiles[2], quantiles[3]),\n        sprintf(\"%.1f - %.1f\", quantiles[3], quantiles[4]),\n        sprintf(\"%.1f - %.1f\", quantiles[4], quantiles[5])\n      )\n    )\n  )\n\n# Построение карты\nggplot() +\n  # Базовая карта России\n  geom_sf(data = russia, fill = \"lightblue\", color = \"gray40\") + \n  # Ограничение области отображения\n  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +\n  # Точки наблюдений с категориальным размером\n  geom_point(\n    data = nonzero_data,\n    aes(x = X, y = Y, size = PROM_cat, color = PROM),\n    alpha = 0.7\n  ) +\n  # Точки для нулевых уловов (крестики)\n  geom_point(\n    data = filter(DATA, PROM == 0),\n    aes(x = X, y = Y),\n    shape = 4, size = 1.2, stroke = 1, color = \"black\"\n  ) +\n  # Цветовая шкала (непрерывная)\n  scale_color_viridis_c(option = \"H\", name = NULL) +\n  # Ручная настройка размеров для категорий\n  scale_size_manual(\n    name = \"Улов (экз./ч)\",\n    values = c(2, 4, 6, 8),  # Размеры точек для категорий\n    drop = FALSE\n  ) +\n  # Настройки темы\n  theme_bw() +\n  labs(\n    title = \"Распределение уловов краба (2023)\",\n    subtitle = \"Черные крестики - нулевые уловы\",\n    x = \"Долгота\", \n    y = \"Широта\"\n  ) +\n  theme(\n    panel.grid = element_line(color = \"gray90\"),\n    legend.position = \"bottom\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#карта-распределения-уловов-по-фасеткам",
    "href": "chapter 3.html#карта-распределения-уловов-по-фасеткам",
    "title": "4  Основы картографии",
    "section": "4.6 Карта распределения уловов по фасеткам",
    "text": "4.6 Карта распределения уловов по фасеткам\n\n\n\nРис. 5.: Карта распределения уловов по фасеткам\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls())\nsetwd(\"C:/COURSES/KARTOGRAPH/\")\n\n# Установка и подключение библиотек (если не установлено — раскомментируй)\n# install.packages(c(\"rnaturalearth\", \"tidyverse\", \"sf\", \"readxl\", \"viridis\"))\nlibrary(rnaturalearth)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(readxl)\nlibrary(viridis)\n\n####### ЗАГРУЗКА ДАННЫХ И ПОДГОТОВКА ПРОСТРАНСТВЕННЫХ ОБЪЕКТОВ ################\n\n# Чтение и фильтрация данных (убираем фильтр по году, чтобы работать со всеми годами)\nDATA &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\") %&gt;% \n  filter(SURV == \"CRAB\")\n\n# Получение границ России\nrussia &lt;- ne_countries(scale = 10, country = \"Russia\") %&gt;% \n  st_as_sf()\n\n# Установка границ отображаемой области\nxmin &lt;- 37; xmax &lt;- 49\nymin &lt;- 68.5; ymax &lt;- 70.5\n\n# Вычисляем общие квартили для всех лет (чтобы категории были сопоставимыми)\nquantiles &lt;- quantile(DATA$PROM[DATA$PROM &gt; 0], probs = seq(0, 1, 0.25))\n\n# Создаем данные с ненулевыми уловами и категориями\nnonzero_data &lt;- DATA %&gt;%\n  filter(PROM &gt; 0) %&gt;%\n  mutate(\nPROM_cat = cut(\n  PROM,\n  breaks = c(-Inf, quantiles[2:4], Inf),\n  include.lowest = TRUE,\n  labels = c(\n    sprintf(\"%d - %d\", floor(quantiles[1]), floor(quantiles[2])),\n    sprintf(\"%d - %d\", floor(quantiles[2]), floor(quantiles[3])),\n    sprintf(\"%d - %d\", floor(quantiles[3]), floor(quantiles[4])),\n    sprintf(\"%d - %d\", floor(quantiles[4]), floor(max(DATA$PROM)))\n  )\n)\n  )\n\n# Отдельно выделяем точки с нулевым уловом\nzero_data &lt;- DATA %&gt;% filter(PROM == 0)\n\n####### ВИЗУАЛИЗАЦИЯ ################\n\n# Фасеточная карта по годам\nggplot() +\n  # Граница России\n  geom_sf(data = russia, fill = \"lightblue\", color = \"gray40\") +\n  \n  # Ограничение области отображения\n  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +\n  \n  # Точки с уловом\n  geom_point(\n    data = nonzero_data,\n    aes(x = X, y = Y, size = PROM_cat, color = PROM),\n    alpha = 0.7\n  ) +\n  \n  # Нулевые уловы — крестики\n  geom_point(\n    data = zero_data,\n    aes(x = X, y = Y),\n    shape = 4, size = 1.2, stroke = 1, color = \"black\"\n  ) +\n  \n  # Цветовая шкала\n  scale_color_viridis_c(option = \"H\", name = NULL) +\n  \n  # Настройка размеров точек по категориям\n  scale_size_manual(\n    name = \"Улов (экз./ч)\",\n    values = c(1, 2,4, 6),\n    drop = FALSE\n  ) +\n  \n  # Фасет по годам\n  facet_wrap(~ YEAR, ncol = 2, labeller = label_value) +\n  \n  # Тема и заголовок\n  theme_bw() +\n  labs(\n    title = \"Распределение уловов краба по годам\",\n    subtitle = NULL,\n    x = \"Долгота\", \n    y = \"Широта\"\n  ) +\n  theme(\n    panel.grid = element_line(color = \"gray90\"),\n    legend.position = \"bottom\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#карта-распределения-уловов-с-автокорреляцией-lisa",
    "href": "chapter 3.html#карта-распределения-уловов-с-автокорреляцией-lisa",
    "title": "4  Основы картографии",
    "section": "4.7 Карта распределения уловов с автокорреляцией LISA",
    "text": "4.7 Карта распределения уловов с автокорреляцией LISA\nАлгоритм LISA (Local Indicators of Spatial Association) представляет собой инструмент выявления пространственных закономерностей на уровне отдельных объектов. В отличие от глобальных показателей, которые дают обобщенную оценку автокорреляции для всего региона, LISA позволяет идентифицировать конкретные кластеры и аномалии, определяя, какие именно участки вносят основной вклад в пространственную структуру данных. В контексте анализа промысловых данных краба за 2023 год, этот метод позволяет выявить зоны концентрации уловов и территории с аномальными показателями.\nСуть кластеризации по методу LISA заключается в сравнении значения каждого конкретного объекта (точки съемки) со значениями его соседей. Алгоритм последовательно выполняет несколько ключевых шагов: сначала создается матрица пространственных весов, где для каждой точки определяются k ближайших соседей (в данном случае k=4). Затем для каждой точки рассчитывается локальный индекс Морана (Ii), который количественно оценивает степень сходства между значением в точке и ее окружением. Статистическая значимость кластеризации проверяется через p-значение, полученное методом Монте-Карло.\nБиологическая интерпретация выявленных кластеров основана на их классификации:\n\nHigh-High (красные точки): зоны высокой концентрации уловов, окруженные такими же продуктивными участками — потенциальные “горячие точки” скопления краба\nLow-Low (синие точки): территории с устойчиво низкими уловами, окруженные аналогичными участками — возможные акватории с неблагоприятными условиями\nHigh-Low (розовые точки): аномалии высоких уловов на фоне низкопродуктивного окружения — требуют проверки на ошибки данных или изучения уникальных локальных факторов\nLow-High (голубые точки): участки неожиданно низких уловов в окружении продуктивных зон — возможные признаки перелова или деградации среды\n\nВизуализация результатов (рис. 6) сочетает картографическую основу с семантикой цвета и размера: размер точки пропорционален величине улова (PROM), а цвет отражает тип кластера. Серые точки обозначают территории без статистически значимой автокорреляции. Ограничение области исследования координатами 37-49° в.д. и 68.5-70.5° с.ш. фокусирует анализ на ключевом промысловом районе, а преобразование в проекцию UTM (32638) обеспечивает точность расчетов расстояний.\nПрактическая ценность анализа заключается в возможности целевого управления промыслом: выявленные кластеры High-High могут стать объектами особого мониторинга для предотвращения перелова, тогда как зоны Low-Low требуют изучения причин низкой продуктивности (например, исследования донных сообществ или океанографических условий). Аномальные точки (High-Low/Low-High) служат индикаторами для выборочного контроля достоверности данных. Такой подход переводит сырые данные съемки в пространственно-стратифицированную основу для принятия управленческих решений, позволяя оптимизировать промысловое усилие и минимизировать воздействие на уязвимые участки донных экосистем.\n\n\n\nРис. 6.: Карта распределения уловов с автокорреляцией LISA\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls())\nsetwd(\"C:/COURSES/KARTOGRAPH/\")\n\n# Загрузка библиотек\nlibrary(rnaturalearth)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(spdep)\nlibrary(ggspatial)\nlibrary(readxl)\n\n# 1. ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ\nDATA &lt;- read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\") %&gt;% \n  filter(YEAR == 2023, SURV == \"CRAB\")\n\n# Проверка названий колонок\nprint(names(DATA))  # Убедитесь, что координаты названы правильно\n\n# Преобразование в пространственные данные (замените X/Y на ваши названия)\npoints_sf &lt;- st_as_sf(DATA, coords = c(\"X\", \"Y\"), crs = 4326)\n\n# 2. ПОЛУЧЕНИЕ КАРТЫ РОССИИ\n# Задаем границы области\nxmin &lt;- 37\nxmax &lt;- 49\nymin &lt;- 68.5\nymax &lt;- 70.5\n\n# Создаём ограничивающий прямоугольник\nbbox &lt;- st_bbox(c(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), crs = 4326)\nbbox_poly &lt;- st_as_sfc(bbox)\n\n# Карта России\nrussia &lt;- ne_countries(country = \"Russia\", scale = 10) %&gt;% \n  st_as_sf() %&gt;% \n  st_crop(bbox)  # Обрезка без st_intersection\n\n# 3. ПОДГОТОВКА ТОЧЕК\n# Удаление дубликатов по координатам\ncoords &lt;- st_coordinates(points_sf)\npoints_sf &lt;- points_sf[!duplicated(coords), , drop = FALSE]\n\n# Перевод в UTM\npoints_utm &lt;- st_transform(points_sf, crs = 32638)\n\n# 4. АНАЛИЗ LISA\n# Матрица весов\nknn &lt;- knearneigh(points_utm, k = 4)\nnb &lt;- knn2nb(knn)\nlistw &lt;- nb2listw(nb, style = \"W\")\n\n# Локальный Моран\nlocal_moran &lt;- localmoran(points_utm$PROM, listw)\n\n# Добавляем кластеры\npoints_utm &lt;- points_utm %&gt;%\n  mutate(\n    Local_I = local_moran[, \"Ii\"],\n    P_value = local_moran[, \"Pr(z != E(Ii))\"],\n    Mean_PROM = mean(PROM, na.rm = TRUE),  # Добавляем среднее значение\n    Cluster = case_when(\n      Local_I &gt; 0 & PROM &gt; Mean_PROM ~ \"High-High\",\n      Local_I &gt; 0 & PROM &lt;= Mean_PROM ~ \"Low-Low\",  # Включаем PROM == 0\n      Local_I &lt; 0 & PROM &gt; Mean_PROM ~ \"High-Low\",\n      Local_I &lt; 0 & PROM &lt;= Mean_PROM ~ \"Low-High\",  # PROM == 0 попадает сюда\n      TRUE ~ \"Not significant\"\n    )\n  )\n\n# Обратно в WGS84\npoints_result &lt;- st_transform(points_utm, crs = 4326)\n\n# 5. ВИЗУАЛИЗАЦИЯ\ncluster_colors &lt;- c(\n  \"High-High\" = \"red\",\n  \"Low-Low\" = \"blue\",\n  \"High-Low\" = \"pink\",\n  \"Low-High\" = \"lightblue\",\n  \"Not significant\" = \"gray\"\n)\n\nggplot() +\n  # Карта России\n  geom_sf(data = russia, fill = \"lightblue\", color = \"black\") +\n  \n  # Все точки (включая PROM == 0) — в одном слое\n  geom_sf(\n    data = points_result,\n    aes(color = Cluster, size = PROM),\n    alpha = 0.8\n  ) +\n  \n  # Настройки координат и масштаба\n  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax), expand = FALSE) +\n  annotation_scale(location = \"tl\", width_hint = 0.3) +\n  \n  # Цвет и размер\n  scale_color_manual(values = cluster_colors) +\n  scale_size_continuous(range = c(1, 8), name = \"Величина улова\") +\n  \n  # Заголовки и тема\n  labs(\n    title = \"Пространственная автокорреляция уловов краба (LISA)\",\n    subtitle = \"2023 год, тип съемки: CRAB\",\n    color = \"Тип кластера\"\n  ) +\n  \ntheme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"right\",\n    panel.border = element_rect(colour = \"black\", size = 1, fill = NA)  # Рамка вокруг карты\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#карта-распределения-уловов-с-автокорреляцией-lisa-по-фасеткам",
    "href": "chapter 3.html#карта-распределения-уловов-с-автокорреляцией-lisa-по-фасеткам",
    "title": "4  Основы картографии",
    "section": "4.8 Карта распределения уловов с автокорреляцией LISA по фасеткам",
    "text": "4.8 Карта распределения уловов с автокорреляцией LISA по фасеткам\n\n\n\nРис. 7.: Карта распределения уловов с автокорреляцией LISA по фасеткам\n\n\n\n# Очистка памяти и установка рабочей папки\nrm(list = ls())\nsetwd(\"C:/COURSES/KARTOGRAPH/\")\n\n# Загрузка необходимых пакетов\nlibrary(rnaturalearth)  # Географические карты\nlibrary(tidyverse)      # Обработка данных и визуализация\nlibrary(sf)             # Пространственные данные\nlibrary(spdep)          # Пространственная статистика\nlibrary(ggspatial)      # Дополнения для карт в ggplot\nlibrary(readxl)         # Чтение Excel-файлов\n\n# 1. ЗАГРУЗКА И ПРЕОБРАЗОВАНИЕ ДАННЫХ\n# - Чтение данных из Excel\n# - Фильтрация только данных по крабу\nDATA &lt;- read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\") %&gt;% \n  filter(SURV == \"CRAB\")\n\n# Преобразование в пространственный объект с координатами\npoints_sf &lt;- st_as_sf(DATA, coords = c(\"X\", \"Y\"), crs = 4326)\n\n# 2. ПОДГОТОВКА КАРТОГРАФИЧЕСКОЙ ОСНОВЫ\n# - Определение границ области исследования\nxmin &lt;- 37; xmax &lt;- 49; ymin &lt;- 68.5; ymax &lt;- 70.7\n\n# - Создание ограничивающего прямоугольника\nbbox &lt;- st_bbox(c(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), crs = 4326)\n\n# - Загрузка и обрезка карты России по заданным границам\nrussia &lt;- ne_countries(country = \"Russia\", scale = 10) %&gt;% \n  st_as_sf() %&gt;% \n  st_crop(bbox)\n\n# 3. ФУНКЦИЯ ДЛЯ ПРОСТРАНСТВЕННОГО АНАЛИЗА ПО ГОДАМ\nanalyze_year &lt;- function(data_year) {\n  # Удаление дубликатов координат\n  coords &lt;- st_coordinates(data_year)\n  data_year &lt;- data_year[!duplicated(coords), , drop = FALSE]\n  \n  # Перепроецирование в UTM для точных расчетов\n  points_utm &lt;- st_transform(data_year, crs = 32638)\n  \n  # Построение матрицы пространственных весов (4 ближайших соседа)\n  knn &lt;- knearneigh(points_utm, k = 4)\n  nb &lt;- knn2nb(knn)\n  listw &lt;- nb2listw(nb, style = \"W\")  # Стандартизованная матрица\n  \n  # Расчет локальной пространственной автокорреляции (LISA)\n  local_moran &lt;- localmoran(points_utm$PROM, listw)\n  \n  # Классификация кластеров на основе результатов\n  points_utm &lt;- points_utm %&gt;%\n    mutate(\n      Local_I = local_moran[, \"Ii\"],\n      P_value = local_moran[, \"Pr(z != E(Ii))\"],\n      Mean_PROM = mean(PROM, na.rm = TRUE),\n      Cluster = case_when(\n        Local_I &gt; 0 & PROM &gt; Mean_PROM ~ \"High-High\",     # Горячая точка\n        Local_I &gt; 0 & PROM &lt;= Mean_PROM ~ \"Low-Low\",      # Холодная точка\n        Local_I &lt; 0 & PROM &gt; Mean_PROM ~ \"High-Low\",      # Выброс (высокий среди низких)\n        Local_I &lt; 0 & PROM &lt;= Mean_PROM ~ \"Low-High\",     # Выброс (низкий среди высоких)\n        TRUE ~ \"Not significant\"                          # Незначимые\n      )\n    )\n  \n  # Возврат в географические координаты\n  st_transform(points_utm, crs = 4326)\n}\n\n# 4. ОБРАБОТКА ДАННЫХ ПО ГОДАМ\n# - Разделение данных по годам\n# - Применение анализа для каждого года\n# - Объединение результатов\nresults_list &lt;- DATA %&gt;%\n  group_split(YEAR) %&gt;% \n  lapply(function(group) {\n    analyze_year(st_as_sf(group, coords = c(\"X\", \"Y\"), crs = 4326))\n  }) %&gt;%\n  bind_rows()\n\n# 5. КАТЕГОРИЗАЦИЯ УЛОВОВ\n# - Расчет квантилей для всего набора данных\nPROM_breaks &lt;- quantile(results_list$PROM, \n                         probs = c(0, 0.25, 0.5, 0.75, 1), \n                         na.rm = TRUE) %&gt;% \n  round(1)  # Округление значений\n\n# - Создание меток с реальными диапазонами\nPROM_labels &lt;- sprintf(\"%.1f - %.1f\", PROM_breaks[1:4], PROM_breaks[2:5])\n\n# - Добавление категорий уловов в данные\nresults_list &lt;- results_list %&gt;%\n  mutate(\n    PROM_category = cut(\n      PROM, \n      breaks = PROM_breaks, \n      labels = PROM_labels,\n      include.lowest = TRUE\n    )\n  )\n\n# 6. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ\n# Цветовая схема для типов кластеров\ncluster_colors &lt;- c(\n  \"High-High\" = \"red\",       # Горячие точки\n  \"Low-Low\" = \"blue\",        # Холодные точки\n  \"High-Low\" = \"pink\",       # Выбросы высокие\n  \"Low-High\" = \"lightblue\",  # Выбросы низкие\n  \"Not significant\" = \"gray\" # Незначимые\n)\n\n# Построение карты\nggplot(data = results_list) +\n  # Базовая карта России\n  geom_sf(data = russia, fill = \"#E8E5D6\", color = \"black\", inherit.aes = FALSE) +\n  \n  # Точки наблюдений с цветом по кластерам и размером по уловам\n  geom_sf(aes(color = Cluster, size = PROM_category), alpha = 0.8) +\n  \n  # Разделение на панели по годам\n  facet_wrap(~ YEAR, ncol = 2) +\n  \n  # Установка границ карты\n  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax), expand = FALSE) +\n  \n  # Настройка легенды для кластеров\n  scale_color_manual(\n    values = cluster_colors,\n    name = \"Тип кластера\",\n    guide = guide_legend(nrow = 2)\n  ) +\n  \n  # Настройка легенды для уловов (реальные диапазоны)\n  scale_size_manual(\n    name = \"Величина улова\",\n    values = c(1, 2, 3, 5),  # Размеры точек для 4-х категорий\n    breaks = levels(results_list$PROM_category),\n    guide = guide_legend(nrow = 2)\n  ) +\n  \n  # Заголовки и подписи\n  labs(\n    title = \"Пространственная автокорреляция уловов краба (LISA)\",\n    subtitle = \"Тип съемки: CRAB\"\n  ) +\n  \n  # Оформление графика\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(size = 9, margin = margin(t = 5)),\n    axis.text.y = element_text(size = 9, angle = 90, hjust = 0.5, margin = margin(r = 5)),\n    panel.background = element_rect(fill = \"#F0F8FF\", color = NA),  # Фон океана\n    panel.grid.major = element_line(color = \"grey90\", linetype = \"dotted\"),\n    legend.position = \"bottom\",           # Легенда внизу\n    legend.box = \"horizontal\",            # Горизонтальное расположение\n    panel.border = element_rect(fill = NA, color = \"black\", size = 0.7),\n    strip.background = element_rect(fill = \"white\", color = \"black\", size = 0.7),  # Заголовки панелей\n    strip.text = element_text(size = 11, face = \"bold\")\n  ) +\n  \n  # Разметка осей (долгота с шагом 2°, широта с шагом 1°)\n  scale_x_continuous(\n    breaks = seq(floor(xmin), ceiling(xmax), by = 2),\n    labels = function(x) paste0(x, \"°E\")\n  ) +\n  scale_y_continuous(\n    breaks = seq(floor(ymin), ceiling(ymax), by = 1),  \n    labels = function(y) paste0(y, \"°N\")\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#промысловые-карты-с-квартильным-распределением-уловов",
    "href": "chapter 3.html#промысловые-карты-с-квартильным-распределением-уловов",
    "title": "4  Основы картографии",
    "section": "4.9 Промысловые карты с квартильным распределением уловов",
    "text": "4.9 Промысловые карты с квартильным распределением уловов\n\n\n\nРис. 8.: Промысловые карты с квартильным распределением уловов\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls())\nsetwd(\"C:/COURSES/KARTOGRAPH/\")\n\n# Загрузка необходимых библиотек\nlibrary(rnaturalearth)\nlibrary(tidyverse)\nlibrary(sf)\n\n####### ЗАГРУЗКА ДАННЫХ И ПОДГОТОВКА ПРОСТРАНСТВЕННЫХ ОБЪЕКТОВ ################\n\n# Чтение и фильтрация данных\nDATA &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"FISHERY\") %&gt;% \n  filter(YEAR == 2023)\n\n# Получение границ России\nrussia &lt;- ne_countries(scale = 10, country = \"Russia\") %&gt;% \n  st_as_sf()\n\n# Установка границ отображаемой области\nxmin=37; xmax=48; ymin=68.6; ymax=71\n\n####### ПОДГОТОВКА ДАННЫХ ДЛЯ ВИЗУАЛИЗАЦИИ ################\n# Вычисляем квартили отдельно\nquantiles &lt;- quantile(DATA$CPUE[DATA$CPUE &gt; 0], probs = seq(0, 1, 0.25))\n\n# Создаем 4 категории с реальными диапазонами значений\nnonzero_data &lt;- DATA %&gt;% \n  filter(CPUE &gt; 0) %&gt;%\n  mutate(\n    CPUE_cat = cut(\n      CPUE,\n      breaks = quantiles,\n      include.lowest = TRUE,\n      labels = c(\n        sprintf(\"%.1f - %.1f\", quantiles[1], quantiles[2]),\n        sprintf(\"%.1f - %.1f\", quantiles[2], quantiles[3]),\n        sprintf(\"%.1f - %.1f\", quantiles[3], quantiles[4]),\n        sprintf(\"%.1f - %.1f\", quantiles[4], quantiles[5])\n      )\n    )\n  )\n\n# Построение карты\nggplot() +\n  # Базовая карта России\n  geom_sf(data = russia, fill = \"lightblue\", color = \"gray40\") + \n  # Ограничение области отображения\n  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +\n  # Точки наблюдений с категориальным размером\n  geom_point(\n    data = nonzero_data,\n    aes(x = X, y = Y, size = CPUE_cat, color = CPUE),\n    alpha = 0.7\n  ) +\n  # Точки для нулевых уловов (крестики)\n  geom_point(\n    data = filter(DATA, CPUE == 0),\n    aes(x = X, y = Y),\n    shape = 4, size = 1.2, stroke = 1, color = \"black\"\n  ) +\n  # Цветовая шкала (непрерывная)\n  scale_color_viridis_c(option = \"H\", name = NULL) +\n  # Ручная настройка размеров для категорий\n  scale_size_manual(\n    name = \"CPUE\",\n    values = c(2, 4, 6, 8),  # Размеры точек для категорий\n    drop = FALSE\n  ) +\n  # Настройки темы\n  theme_bw() +\n  labs(\n    title = \"Распределение CPUE краба (2023)\",\n    subtitle = NULL,\n    x = \"Долгота\", \n    y = \"Широта\"\n  ) +\n  theme(\n    panel.grid = element_line(color = \"gray90\"),\n    legend.position = \"bottom\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#промысловые-карты-с-агрегацией-в-центрах-полигонов-промквадратов",
    "href": "chapter 3.html#промысловые-карты-с-агрегацией-в-центрах-полигонов-промквадратов",
    "title": "4  Основы картографии",
    "section": "4.10 Промысловые карты с агрегацией в центрах полигонов (промквадратов)",
    "text": "4.10 Промысловые карты с агрегацией в центрах полигонов (промквадратов)\n\n\n\nРис. 9.: Промысловые карты с агрегацией в центрах полигонов (промквадратов)\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls())\nsetwd(\"C:/COURSES/KARTOGRAPH/\")\n\n# Загрузка необходимых библиотек\nlibrary(rnaturalearth)\nlibrary(tidyverse)\nlibrary(sf)\n\n####### ЗАГРУЗКА ДАННЫХ И ПОДГОТОВКА ПРОСТРАНСТВЕННЫХ ОБЪЕКТОВ ################\n\n# Чтение и фильтрация данных\nDATA &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"FISHERY\") %&gt;% \n  filter(YEAR == 2023)\n\n# Преобразуем CPUE в пространственные точки\nspec_points &lt;- st_as_sf(DATA, coords = c(\"X\", \"Y\"), crs = 4326)\n\n# Карта России\nrussia &lt;- ne_countries(scale = 10, country = \"Russia\") \n\n# Параметры карты и сетки\nxmin &lt;- 32; xmax &lt;- 48; ymin &lt;- 68; ymax &lt;- 72\nxcs &lt;- 1; ycs &lt;- 0.25\n\n\n# Создание основного датафрейма и пространственных объектов\npoints_sf &lt;- st_as_sf(DATA, coords = c(\"X\", \"Y\"), crs = 4326)\n\n# Создание сетки\ngrid_sf &lt;- st_make_grid(points_sf, \n                        cellsize = c(xcs, ycs),\n                        n = c(2 + (xmax - xmin)/xcs, 2 + (ymax - ymin)/ycs),\n                        offset = c(xmin - xcs, ymin - ycs)) %&gt;% \n  st_sf() %&gt;% \n  mutate(cell_id = row_number())\n\n# Присоединяем точки Catch к сетке и агрегируем по ячейкам и годам\nshares_df_catch &lt;- st_join(points_sf, grid_sf) %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(cell_id, YEAR) %&gt;% \n  summarise(\n    Count = n(),\n    CATCH = mean(CPUE, na.rm = TRUE)\n  ) %&gt;% \n  ungroup()\n\n# Присоединяем статистику Catch к сетке\ngird_shares_catch &lt;- right_join(grid_sf, shares_df_catch, by = \"cell_id\")\n\n\n\n# Центроиды сетки по W\nCENTROIDS_W &lt;- gird_shares_catch %&gt;% \n  st_centroid()\n\n#################### ВИЗУАЛИЗАЦИЯ #########################################\n\nggplot() +\n  # 1. Сетка без заливки\n  geom_sf(data = grid_sf, fill = NA, color = \"grey80\", linewidth = 0.3) +\n  \n  # 2. Границы России\n  geom_sf(data = russia, fill = \"grey95\") +\n  \n  # 3. Центроиды ячеек с CATCH (цвет и размер по значению)\n  geom_sf(\n    data = CENTROIDS_W, \n    aes(size = CATCH, color = CATCH),\n    shape = 16, \n    alpha = 0.7\n  ) +\n  \n  # 4. Цветовая шкала (viridis как в первом скрипте)\n  scale_color_viridis_c(\n    option = \"H\", \n    name = NULL,\n    limits = c(0, max(gird_shares_catch$CATCH, na.rm = TRUE))\n  ) +\n  \n  # 5. Шкала размера центроидов\n  scale_size_continuous(\n    range = c(1, 10), \n    name = \"CPUE\"\n  ) +\n  \n  # 6. Обрезаем область отображения\n  coord_sf(\n    xlim = c(xmin, xmax), \n    ylim = c(ymin, ymax),\n    expand = FALSE  # Точное соответствие границ\n  ) +\n  \n  # 7. Шкалы для осей координат\n  scale_x_continuous(\n    breaks = seq(xmin, xmax, by = 2),  # Метки каждые 2 градуса\n    name = \"Долгота\"\n  ) +\n  scale_y_continuous(\n    breaks = seq(ymin, ymax, by = 0.5),  # Метки каждые 0.5 градуса\n    name = \"Широта\"\n  ) +\n  \n  # 8. Тема оформления\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"bottom\",\n    panel.border = element_rect(fill = NA, color = \"black\", size = 0.5),\n    # Добавляем сетку для осей координат\n    panel.grid.major = element_line(color = \"gray90\", linewidth = 0.2)\n  ) +\n  \n  # 9. Явное указание названий осей (дублируем для надежности)\n  labs(x = \"Долгота\", y = \"Широта\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#промысловые-карты---картограммы",
    "href": "chapter 3.html#промысловые-карты---картограммы",
    "title": "4  Основы картографии",
    "section": "4.11 Промысловые карты - картограммы",
    "text": "4.11 Промысловые карты - картограммы\n\n\n\nРис. 10.: Промысловые карты - картограммы\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls())\nsetwd(\"C:/COURSES/KARTOGRAPH/\")\n\n# Загрузка необходимых библиотек\nlibrary(rnaturalearth)\nlibrary(tidyverse)\nlibrary(sf)\n\n####### ЗАГРУЗКА ДАННЫХ И ПОДГОТОВКА ПРОСТРАНСТВЕННЫХ ОБЪЕКТОВ ################\n\n# Чтение и фильтрация данных\nDATA &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"FISHERY\") %&gt;% \n  filter(YEAR == 2023)\n\n# Преобразуем CPUE в пространственные точки\nspec_points &lt;- st_as_sf(DATA, coords = c(\"X\", \"Y\"), crs = 4326)\n\n# Карта России\nrussia &lt;- ne_countries(scale = 10, country = \"Russia\") \n\n# Параметры карты и сетки\nxmin &lt;- 32; xmax &lt;- 48; ymin &lt;- 68; ymax &lt;- 72\nxcs &lt;- 1; ycs &lt;- 0.25\n\n\n# Создание основного датафрейма и пространственных объектов\npoints_sf &lt;- st_as_sf(DATA, coords = c(\"X\", \"Y\"), crs = 4326)\n\n# Создание сетки\ngrid_sf &lt;- st_make_grid(points_sf, \n                        cellsize = c(xcs, ycs),\n                        n = c(2 + (xmax - xmin)/xcs, 2 + (ymax - ymin)/ycs),\n                        offset = c(xmin - xcs, ymin - ycs)) %&gt;% \n  st_sf() %&gt;% \n  mutate(cell_id = row_number())\n\n# Присоединяем точки Catch к сетке и агрегируем по ячейкам и годам\nshares_df_catch &lt;- st_join(points_sf, grid_sf) %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(cell_id, YEAR) %&gt;% \n  summarise(\n    Count = n(),\n    CATCH = mean(CPUE, na.rm = TRUE)\n  ) %&gt;% \n  ungroup()\n\n# Присоединяем статистику Catch к сетке\ngird_shares_catch &lt;- right_join(grid_sf, shares_df_catch, by = \"cell_id\")\n\n\n\n# Центроиды сетки по W\nCENTROIDS_W &lt;- gird_shares_catch %&gt;% \n  st_centroid()\n\n#################### ВИЗУАЛИЗАЦИЯ #########################################\n\nggplot() +\n  # 1. Сетка без заливки\n  geom_sf(data = grid_sf, fill = NA, color = \"grey80\", linewidth = 0.3) +\n  \n  # 2. Границы России\n  geom_sf(data = russia, fill = \"grey95\") +\n  \n  # 3. Заливка по улову с палитрой viridis option \"H\"\n  geom_sf(data = gird_shares_catch, aes(fill = CATCH), color = NA) +\n  \n  # 4. Цветовая шкала viridis option \"H\" для заливки\n  scale_fill_viridis_c(\n    option = \"H\", \n    name = \"CPUE\",\n    limits = c(0, max(gird_shares_catch$CATCH, na.rm = TRUE)),\n    na.value = \"transparent\"\n  ) +\n  \n  # 5. Обрезаем область отображения\n  coord_sf(\n    xlim = c(xmin, xmax), \n    ylim = c(ymin, ymax),\n    expand = FALSE\n  ) +\n  \n  # 6. Шкалы для осей координат\n  scale_x_continuous(\n    breaks = seq(xmin, xmax, by = 2),\n    name = \"Долгота\"\n  ) +\n  scale_y_continuous(\n    breaks = seq(ymin, ymax, by = 0.5),\n    name = \"Широта\"\n  ) +\n  \n  # 7. Тема оформления\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"bottom\",\n    panel.border = element_rect(fill = NA, color = \"black\", size = 0.5),\n    panel.grid.major = element_line(color = \"gray90\", size = 0.2)\n  ) +\n  labs(x = \"Долгота\", y = \"Широта\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#промысловые-карты---картограммы-по-фасеткам",
    "href": "chapter 3.html#промысловые-карты---картограммы-по-фасеткам",
    "title": "4  Основы картографии",
    "section": "4.12 Промысловые карты - картограммы по фасеткам",
    "text": "4.12 Промысловые карты - картограммы по фасеткам\n\n\n\nРис. 11.: Промысловые карты - картограммы по фасеткам\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls())\nsetwd(\"C:/COURSES/KARTOGRAPH/\")\n\nlibrary(rnaturalearth)\nlibrary(tidyverse)\nlibrary(ggspatial)\nlibrary(sf)\n\n####### READ DATA AND PREPARE SPATIAL OBJECTS ############################\n\n# Чтение и фильтрация данных\nDATA &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"FISHERY\") %&gt;% \n  filter(YEAR &gt; 2020 & YEAR &lt; 2025)\n\n# Карта России\nrussia &lt;- ne_countries(scale = 10, country = \"Russia\") \n\n# Параметры карты и сетки\nxmin &lt;- 32; xmax &lt;- 48; ymin &lt;- 68; ymax &lt;- 72\nxcs &lt;- 2; ycs &lt;- 0.5\n\n# Преобразование в пространственные объекты\npoints_sf &lt;- st_as_sf(DATA, coords = c(\"X\", \"Y\"), crs = 4326)\n\n# Создание сетки\ngrid_sf &lt;- st_make_grid(\n  points_sf,\n  cellsize = c(xcs, ycs),\n  n = c(2 + (xmax - xmin)/xcs, 2 + (ymax - ymin)/ycs),\n  offset = c(xmin - xcs, ymin - ycs)\n) %&gt;% \n  st_sf() %&gt;% \n  mutate(cell_id = row_number())\n\n# Агрегация данных по сетке и годам\nshares_df_catch &lt;- points_sf %&gt;% \n  st_join(grid_sf) %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(cell_id, YEAR) %&gt;% \n  summarise(CATCH = mean(CPUE, na.rm = TRUE), .groups = 'drop')\n\n# Присоединение статистики к сетке\ngird_shares_catch &lt;- grid_sf %&gt;% \n  right_join(shares_df_catch, by = \"cell_id\")\n\n#################### ВИЗУАЛИЗАЦИЯ #########################################\n\n# Определяем общий максимум CPUE для единой шкалы цветов\ncatch_max &lt;- max(gird_shares_catch$CATCH, na.rm = TRUE)\n\n# Рассчитываем шаг для подписей (в 2 раза реже исходной сетки)\nx_breaks &lt;- seq(xmin, xmax, by = xcs * 2)  # 4 градуса\ny_breaks &lt;- seq(ymin, ymax, by = ycs * 2)  # 1 градус\n\n# Функция для форматирования подписей: пропускаем первую подпись\nformat_labels &lt;- function(breaks) {\n  labels &lt;- paste0(breaks, \"°\")\n  labels[1] &lt;- \"\"  # Пропускаем первую подпись\n  return(labels)\n}\n\nggplot() +\n  # Контуры сетки\n  geom_sf(data = grid_sf, fill = NA, color = \"grey80\", linewidth = 0.3) +\n  \n  # Заливка по улову с цветовой схемой viridis\n  geom_sf(data = gird_shares_catch, aes(fill = CATCH), color = NA) +\n  \n  # Границы России\n  geom_sf(data = russia, fill = \"#E8E5D6\") +\n  \n  # Фасетирование по годам\n  facet_wrap(~ YEAR, nrow = 2) +\n  \n  # Цветовая шкала\n  scale_fill_viridis_c(\n    option = \"H\", \n    name = \"CPUE\",\n    limits = c(0, catch_max),\n    na.value = \"transparent\"\n  ) +\n  \n  # Область отображения\n  coord_sf(\n    xlim = c(xmin, xmax), \n    ylim = c(ymin, ymax),\n    expand = FALSE\n  ) +\n  \n  # Управление подписями осей с символом градуса (пропускаем первую подпись)\n  scale_x_continuous(\n    breaks = x_breaks,\n    labels = format_labels\n  ) +\n  scale_y_continuous(\n    breaks = y_breaks,\n    labels = format_labels\n  ) +\n  \n  # Оформление с тиками на осях\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    legend.position = \"bottom\",\n    legend.key.width = unit(2.5, \"cm\"),\n    legend.title = element_text(vjust = 0.8, size = 12),\n    panel.border = element_rect(fill = NA, color = \"black\", size = 0.7),\n    panel.grid.major = element_line(color = \"grey90\", size = 0.2),\n    strip.background = element_rect(fill = \"#E8E5D6\", color = \"black\"),\n    strip.text = element_text(face = \"bold\", size = 12),\n    axis.text.x = element_text(size = 9, angle = 0, margin = margin(t = 5)),\n    axis.text.y = element_text(size = 9, angle = 90, hjust = 0.5, margin = margin(r = 5)),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    \n    # Тики (засечки) на оси\n    axis.ticks = element_line(color = \"black\", size = 0.5),\n    axis.ticks.length = unit(0.2, \"cm\"),\n    axis.ticks.x = element_line(color = \"black\", size = 0.5),\n    axis.ticks.y = element_line(color = \"black\", size = 0.5)\n  ) +\n  \n  # Настройка легенды\n  guides(fill = guide_colorbar(\n    title.position = \"top\",\n    title.hjust = 0.5,\n    barwidth = 15,\n    frame.colour = \"black\",\n    ticks.colour = \"black\"\n  ))\n\n# Сохранение результата\nggsave(\"KARTOGRAPH11.jpg\", \n       device = \"jpeg\", \n       dpi = 300,\n       width = 7,\n       height = 5,\n       units = \"in\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#гибридные-карты---картограммы-и-точки-съемка-и-промысловые-данные",
    "href": "chapter 3.html#гибридные-карты---картограммы-и-точки-съемка-и-промысловые-данные",
    "title": "4  Основы картографии",
    "section": "4.13 Гибридные карты - картограммы и точки (съемка и промысловые данные)",
    "text": "4.13 Гибридные карты - картограммы и точки (съемка и промысловые данные)\n\n\n\nРис. 12.: Гибридные карты - картограммы и точки (съемка и промысловые данные)\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls())\nsetwd(\"C:/COURSES/KARTOGRAPH/\")\n\nlibrary(rnaturalearth)\nlibrary(tidyverse)\nlibrary(ggspatial)\nlibrary(sf)\n\n####### READ DATA AND PREPARE SPATIAL OBJECTS ############################\n\n# Чтение и фильтрация данных\nDATA &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"FISHERY\") %&gt;% \n  filter(YEAR &gt; 2020 & YEAR &lt; 2025)\n\nSURVEY &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\") %&gt;% \n  filter(YEAR &gt; 2020 & YEAR &lt; 2025, SURV == \"CRAB\")\n\n# Создание 4 категорий для переменной PROM\nbreaks &lt;- quantile(SURVEY$PROM, \n                   probs = c(0, 0.25, 0.5, 0.75, 1), \n                   na.rm = TRUE)\nSURVEY$PROM_cat &lt;- cut(SURVEY$PROM,\n                       breaks = breaks,\n                       include.lowest = TRUE,\n                       labels = c(\"Q1 (Low)\", \"Q2\", \"Q3\", \"Q4 (High)\"))\n\n# Карта России\nrussia &lt;- ne_countries(scale = 10, country = \"Russia\") \n\n# Параметры карты и сетки\nxmin &lt;- 32; xmax &lt;- 48; ymin &lt;- 68; ymax &lt;- 72\nxcs &lt;- 2; ycs &lt;- 0.5\n\n# Преобразование в пространственные объекты\npoints_sf &lt;- st_as_sf(DATA, coords = c(\"X\", \"Y\"), crs = 4326)\nsurvey_sf &lt;- st_as_sf(SURVEY, coords = c(\"X\", \"Y\"), crs = 4326)\n\n# Создание сетки\ngrid_sf &lt;- st_make_grid(\n  points_sf,\n  cellsize = c(xcs, ycs),\n  n = c(2 + (xmax - xmin)/xcs, 2 + (ymax - ymin)/ycs),\n  offset = c(xmin - xcs, ymin - ycs)\n) %&gt;% \n  st_sf() %&gt;% \n  mutate(cell_id = row_number())\n\n# Агрегация данных по сетке и годам\nshares_df_catch &lt;- points_sf %&gt;% \n  st_join(grid_sf) %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(cell_id, YEAR) %&gt;% \n  summarise(CATCH = mean(CPUE, na.rm = TRUE), .groups = 'drop')\n\n# Присоединение статистики к сетке\ngird_shares_catch &lt;- grid_sf %&gt;% \n  right_join(shares_df_catch, by = \"cell_id\")\n\n################### ВИЗУАЛИЗАЦИЯ #########################################\nggplot() +\n  # Контуры сетки\n  geom_sf(data = grid_sf, fill = NA, color = \"grey80\", linewidth = 0.3) +\n  \n  # Заливка по улову (средний CPUE)\n  geom_sf(data = gird_shares_catch, aes(fill = CATCH)) +\n  \n  # Границы России\n  geom_sf(data = russia, fill = \"#E8E5D6\") +\n  \n  # Точки SURVEY: голубые с черной окантовкой\n  geom_sf(data = survey_sf, \n          aes(size = PROM_cat),\n          fill = \"lightblue\",    # Голубая заливка\n          color = \"black\",        # Черная окантовка\n          alpha = 0.7,\n          shape = 21,             # Круг с обводкой\n          stroke = 0.5,           # Толщина окантовки\n          show.legend = \"point\") +\n  \n  # Фасетирование по годам\n  facet_wrap(~ YEAR, nrow = 2) +\n  \n  # Цветовая шкала для заливки\n  scale_fill_gradient(\n    low = \"white\", \n    high = \"red\",\n    na.value = NA,\n    limits = c(0, max(gird_shares_catch$CATCH, na.rm = TRUE)),\n    name = \"Catch (CPUE)\"\n  ) +\n  \n  # Шкала размеров для точек\n  scale_size_manual(\n    name = \"PROM Category\",\n    values = c(1.5, 2.5, 3.5, 4.5)  # Размеры точек для 4 категорий\n  ) +\n  \n  ### ОСИ С ГЕОГРАФИЧЕСКИМИ КООРДИНАТАМИ ###\n  scale_x_continuous(\n    breaks = c(32, 38, 44, 48),                    \n    labels = c(\"32°E\", \"38°E\", \"44°E\", \"48°E\"),    \n    name = NULL\n  ) +\n  scale_y_continuous(\n    breaks = c(68.5, 69.5, 70.5, 71.5),          \n    labels = c(\"68.5°N\", \"69.5°N\", \"70.5°N\", \"71.5°N\"),\n    name = NULL\n  ) +\n  \n  # Область отображения\n  coord_sf(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +\n  \n  # Оформление\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(size = 8),\n    axis.text.y = element_text(size = 8, angle = 90, hjust = 0.5),\n    panel.grid = element_line(color = \"grey90\"),\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\",  # Размещение легенд в одну строку\n    panel.border = element_rect(fill = NA, color = \"black\", size = 0.5),\n    strip.background = element_rect(fill = \"white\", color = \"black\")\n  ) +\n  # Управление легендами\n  guides(\n    fill = guide_colorbar(title.position = \"top\", title.hjust = 0.5),\n    size = guide_legend(title.position = \"top\", title.hjust = 0.5)\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#карты-для-главы-материал-и-методы",
    "href": "chapter 3.html#карты-для-главы-материал-и-методы",
    "title": "4  Основы картографии",
    "section": "4.14 Карты для “главы Материал и методы”",
    "text": "4.14 Карты для “главы Материал и методы”\n\n\n\nРис. 13.: Карты для “главы Материал и методы”\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls()) # Удаление всех объектов из глобального окружения\nsetwd(\"C:/COURSES/KARTOGRAPH/\") # Установка рабочей директории\n\n# -----------------\n# ЗАГРУЗКА ПАКЕТОВ\n# -----------------\nlibrary(sf)          # Пространственные операции с векторными данными\nlibrary(marmap)      # Работа с батиметрическими данными (карты глубин)\nlibrary(tidyverse)   # Коллекция пакетов для обработки данных (dplyr, ggplot2 и др.)\nlibrary(rnaturalearth) # Векторные картографические данные (границы, береговые линии)\nlibrary(ggspatial)   # Инструменты для пространственной визуализации в ggplot\nlibrary(readxl)      # Импорт данных из Excel-файлов\n\n# -----------------\n# ЗАГРУЗКА ДАННЫХ\n# -----------------\n# Чтение данных из Excel-листа\nDATA &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\")\n\n# Фильтрация данных:\nSUMMER &lt;- DATA[DATA$SURV == \"SUM\" & DATA$YEAR == 2024, ] # Летние исследования 2024\nCRAB &lt;- DATA[DATA$SURV == \"CRAB\" & DATA$YEAR == 2024, ]   # Крабовые исследования 2024\n\n# -----------------\n# ПОДГОТОВКА КАРТОГРАФИЧЕСКОЙ ОСНОВЫ\n# -----------------\n# Загрузка векторных границ России\nrussia_map &lt;- ne_states(country = \"russia\", returnclass = \"sf\")\n\n# Загрузка береговой линии мирового океана\ncoast &lt;- ne_coastline(scale = 10, returnclass = \"sf\")\n\n# Создание сетки для навигации (5° по долготе, 1° по широте)\nga_grid &lt;- russia_map %&gt;% \n  st_make_grid(cellsize = c(5, 1), offset = c(30, 67))\n\n# Установка границ региона интереса\nxmin &lt;- 30; xmax &lt;- 58\nymin &lt;- 67; ymax &lt;- 72.5\n\n# -----------------\n# БАТИМЕТРИЧЕСКИЕ ДАННЫЕ\n# -----------------\n# Загрузка данных о глубинах из базы NOAA\nbat &lt;- getNOAA.bathy(\n  lon1 = xmin, lon2 = xmax,\n  lat1 = ymin, lat2 = ymax,\n  resolution = 1,   # Разрешение данных (1 минута дуги)\n  keep = TRUE       # Сохранить кэш на диске\n)\n\n# Преобразование в таблицу XYZ (долгота, широта, глубина)\nbat_xyz &lt;- as.xyz(bat)\n\n# Создание цветовой схемы для глубин:\nbreaks &lt;- c(-10000, -7000, -6000, -5000, -4000, -3000, -2000, -1000, \n            -500, -200, -50, -1, 5, 50, 100, 150, 200, 300, 400, 500, 1000, 3000)\ncols &lt;- c(\n  \"#5e99d6\", \"#669cd4\", \"#6c9fd4\", \"#96bce3\", \"#AEC8E3\", \"#a6c4e3\",\n  \"#AEC8E3\", \"#BBD0EB\", \"#C7DCF1\", \"#DAECFA\", \"#D2E5F6\", \"#e1f2d8\",\n  \"#B8D3AA\", \"#b3b387\", \"#9EC187\", \"#C7D097\", \"#DADBAF\", \"#F3F0C7\",\n  \"#E6DBA8\", \"#DACFA1\", \"#D1BF81\", \"#C69D45\"\n)\n\n# Категоризация глубин для визуализации\nbat_xyz$V4 &lt;- cut(bat_xyz$V3, breaks = breaks)\nniveles &lt;- levels(bat_xyz$V4)  # Сохранение уровней для легенды\n\n# -----------------\n# ПОСТРОЕНИЕ БАЗОВОЙ КАРТЫ\n# -----------------\nmap &lt;- ggplot() +\n  # Векторные границы России\n  geom_sf(data = russia_map) +\n  # Батиметрическая подложка (цвет = глубина)\n  geom_tile(data = bat_xyz, aes(x = V1, y = V2, fill = V4), show.legend = FALSE) +\n  # Цветовая схема для глубин\n  scale_fill_manual(name = \"Глубина\", values = cols, breaks = niveles) +\n  # Наложение сетки\n  geom_sf(data = ga_grid, alpha = 0.01, linetype = 3) +\n  # Береговая линия\n  geom_sf(data = coast, linewidth = 0.2, fill = NA) +\n  # Ограничение области карты\n  coord_sf(xlim = c(32, 56), ylim = c(68.5, 72.3)) +\n  # Масштабная линейка (top-left)\n  annotation_scale(location = \"tl\", width_hint = 0.2) +\n  # Оформление\n  labs(x = NULL, y = NULL, fill = \"Глубина (м)\") +\n  theme(panel.border = element_rect(colour = \"black\", fill = NA, linewidth = 1))\n\n# -----------------\n# ДОБАВЛЕНИЕ АННОТАЦИЙ\n# -----------------\nmap &lt;- map +\n  annotate(\"text\", x = 40, y = 72.1, size = 5, \n           label = \"Баренцево море\", fontface = \"bold\") +\n  annotate(\"text\", x = 52.2, y = 69.1, size = 4, \n           label = \"о. Колгуев\", fontface = \"bold\") +\n  annotate(\"text\", x = 33, y = 68.9, size = 4, \n           label = \"Кольский\", fontface = \"bold\") +\n  annotate(\"text\", x = 33, y = 68.6, size = 4, \n           label = \"п-ов\", fontface = \"bold\")\n\n# -----------------\n# ДОБАВЛЕНИЕ ТОЧЕК НАБЛЮДЕНИЙ\n# -----------------\nmap &lt;- map +\n  # Точки исследований краба (синие)\n  geom_point(\n    data = CRAB, \n    aes(x = X + 0.2, y = Y), # Смещение для визуального разделения\n    size = 3, color = \"black\", fill = \"#1E90FF\", \n    shape = 21, alpha = 1\n  ) +\n  # Точки летних исследований (оранжевые)\n  geom_point(\n    data = SUMMER, \n    aes(x = X, y = Y), \n    size = 3, color = \"black\", fill = \"#FFA500\", \n    shape = 21, alpha = 1\n  )\n\n# Вывод финальной карты\nprint(map)\n\n# -----------------\n# СОХРАНЕНИЕ РЕЗУЛЬТАТА\n# -----------------\nggsave(\"DATA_MAP.jpg\", \n       plot = map,          # Используем явное указание объекта\n       device = \"jpeg\", \n       dpi = 600,           # Высокое разрешение\n       width = 7,           # Ширина в дюймах\n       height = 5,          # Высота в дюймах\n       units = \"in\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 3.html#карты-с-картой-врезкой-и-маршрутом",
    "href": "chapter 3.html#карты-с-картой-врезкой-и-маршрутом",
    "title": "4  Основы картографии",
    "section": "4.15 Карты с картой-врезкой и маршрутом",
    "text": "4.15 Карты с картой-врезкой и маршрутом\n\n\n\nРис. 14.: Карты с картой-врезкой и маршрутом\n\n\n\n# Очистка окружения и установка рабочей директории\nrm(list = ls()) # Удаление всех объектов из глобального окружения\nsetwd(\"C:/COURSES/KARTOGRAPH/\") # Установка рабочей директории\n\n# -----------------\n# ЗАГРУЗКА ПАКЕТОВ\n# -----------------\nlibrary(sf)          # Пространственные операции с векторными данными\nlibrary(marmap)      # Работа с батиметрическими данными (карты глубин)\nlibrary(tidyverse)   # Коллекция пакетов для обработки данных\nlibrary(rnaturalearth) # Векторные картографические данные\nlibrary(ggspatial)   # Инструменты для пространственной визуализации\nlibrary(readxl)      # Импорт данных из Excel\nlibrary(ggOceanMaps) # Специализированные карты океанов\nlibrary(cowplot)     # Компоновка графиков и добавление элементов\n\n# -----------------\n# ЗАГРУЗКА ДАННЫХ\n# -----------------\n# Чтение данных из Excel\nDATA &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\")\n\n# Фильтрация данных (крабовые исследования 2022)\nDATA &lt;- DATA[DATA$SURV == \"CRAB\" & DATA$YEAR == 2022, ]\n\n# Загрузка векторных границ России\nrussia_map &lt;- ne_states(country = \"russia\", returnclass = \"sf\")\n\n# Установка границ региона интереса\nxmin &lt;- 35; xmax &lt;- 50\nymin &lt;- 67.2; ymax &lt;- 71\n\n# -----------------\n# БАТИМЕТРИЧЕСКИЕ ДАННЫЕ\n# -----------------\n# Загрузка данных о глубинах\nbat &lt;- getNOAA.bathy(xmin, xmax, ymin, ymax, resolution = 1, keep = TRUE)\nbat_xyz &lt;- as.xyz(bat)\n\n# Определение цветовых уровней для глубин\nbreaks &lt;- c(-10000, -7000, -6000, -5000, -4000, -3000, -2000, -1000, \n            -500, -200, -50, -1, 5, 50, 100, 150, 200, 300, 400, 500, 1000, 3000)\ncols &lt;- c(\n  \"#5e99d6\", \"#669cd4\", \"#6c9fd4\", \"#96bce3\", \"#AEC8E3\", \"#a6c4e3\",\n  \"#AEC8E3\", \"#BBD0EB\", \"#C7DCF1\", \"#DAECFA\", \"#D2E5F6\", \"#e1f2d8\",\n  \"#B8D3AA\", \"#b3b387\", \"#9EC187\", \"#C7D097\", \"#DADBAF\", \"#F3F0C7\",\n  \"#E6DBA8\", \"#DACFA1\", \"#D1BF81\", \"#C69D45\"\n)\n\n# Категоризация глубин\nbat_xyz$V4 &lt;- cut(bat_xyz$V3, breaks = breaks)\nniveles &lt;- levels(bat_xyz$V4)\n\n# Создание координатной сетки\nga_grid &lt;- russia_map %&gt;% \n  st_make_grid(cellsize = c(2, 0.5), offset = c(34, 67))\n\n# -----------------\n# ПОСТРОЕНИЕ ОСНОВНОЙ КАРТЫ\n# -----------------\nmap &lt;- ggplot() +\n  # Векторные границы России\n  geom_sf(data = russia_map) +\n  # Батиметрическая подложка\n  geom_tile(data = bat_xyz, aes(x = V1, y = V2, fill = V4), show.legend = FALSE) +\n  scale_fill_manual(values = cols, breaks = niveles) +\n  # Контур нулевой глубины (береговая линия)\n  geom_contour(data = bat_xyz, aes(x = V1, y = V2, z = V3), \n               breaks = 0, color = \"black\", linewidth = 0.5) + \n  # Координатная сетка\n  geom_sf(data = ga_grid, alpha = 0.01, linetype = 3) +\n  # Ограничение области карты\n  coord_sf(xlim = c(36, 49), ylim = c(67.4, 70.8)) + \n  # Масштабная линейка\n  annotation_scale(location = \"tr\", width_hint = 0.5) +\n  labs(x = NULL, y = NULL) +\n  # Географические подписи\n  annotate(\"text\", x = 47, y = 70.7, size = 5, \n           label = \"Баренцево море\", fontface = \"bold\") +\n  annotate(\"text\", x = 48.4, y = 68.62, size = 4,\n           label = \"о. Колгуев\", fontface = \"bold\") +\n  annotate(\"text\", x = 37.5, y = 67.7, size = 5,\n           label = \"Кольский п-ов\", fontface = \"bold\") +\n  # Маршрут и точки исследований\n  geom_path(data = DATA, aes(x = X, y = Y), color = \"black\") +\n  geom_point(data = DATA, aes(x = X, y = Y), \n             size = 3, color = \"black\", fill = \"white\", \n             shape = 21, alpha = 0.8) +\n  # ДОБАВЛЕНИЕ РАМКИ - ключевое изменение\n  theme(panel.border = element_rect(colour = \"black\", fill = NA, linewidth = 1.5))\n\n# -----------------\n# СОЗДАНИЕ ВСТАВКИ-ЛОКАЦИИ\n# -----------------\n# Область для вставки\nins &lt;- data.frame(lon = c(10, 10, 70, 70), lat = c(67, 80, 80, 67))\n\n# Получение данных для вставки\nmar_bathy &lt;- getNOAA.bathy(9, 71, 66.5, 83, res = 4, keep = TRUE)\nbathy &lt;- raster_bathymetry(stars::st_as_stars(marmap::as.raster(mar_bathy)), \n                           depths = NULL, verbose = FALSE)\n\n# Построение вставки\ninsetmap &lt;- basemap(ins, shapefiles = list(land = dd_land, bathy = bathy), \n                   bathy.style = \"rub\", legends = FALSE) +\n  # Прямоугольник, обозначающий область основной карты\n  geom_rect(aes(xmin = 35, xmax = 51, ymin = 67.5, ymax = 71), \n            fill = \"black\", color = \"black\", alpha = 0.2) +\n  labs(y = NULL, x = NULL) +\n  # Упрощение оформления\n  theme(axis.text.x = element_blank(), \n        axis.text.y = element_blank(),\n        # Рамка для вставки\n        panel.border = element_rect(colour = \"black\", fill = NA, linewidth = 1))\n\n# -----------------\n# ФИНАЛЬНАЯ КОМПОНОВКА С РАМКОЙ\n# -----------------\nMAP &lt;- ggdraw() +\n  # Основная карта\n  draw_plot(map) +\n  # Вставка с позиционированием\n  draw_plot(insetmap,\n            height = 0.3,\n            x = -0.26,\n            y = 0.55) \n\n# Вывод финальной карты\nprint(MAP)\n\n# -----------------\n# СОХРАНЕНИЕ РЕЗУЛЬТАТА\n# -----------------\nggsave(\"DATA_MAP_FRAMED.jpg\", \n       plot = MAP,\n       device = \"jpeg\", \n       dpi = 600,\n       width = 7,\n       height = 6,\n       units = \"in\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Основы картографии</span>"
    ]
  },
  {
    "objectID": "chapter 4.html",
    "href": "chapter 4.html",
    "title": "5  sdmTMB - оценка и визуализация индекса обилия по съемке",
    "section": "",
    "text": "5.1 Введение\nЭто практическое занятие — про то, как превратить данные съёмок в строгую, воспроизводимую оценку индекса запаса, например, камчатского краба во времени и пространстве. Мы используем sdmTMB как рабочую лошадку: это современная реализация пространственно‑временных смешанных моделей (GAMM/GLMM) с гауссовским марковским полем (SPDE) и распределением Твиди для «рыбных» данных с нулями и передисперсией. В духе Нассима Талеба начнём с честного признания: карта — не территория, а модель — не «истина», а аккуратно сформулированная гипотеза, которую мы обязаны проверять. В духе Даниэля Канемана будем сознательно притормаживать «Систему 1» — желание скорее получить красивый график — и переводить себя в «Систему 2»: чёткие допущения, диагностика, чувствительность к альтернативам.\nМы ставим две цели. Первая — стандартизировать индексы обилия краба по годам, разложив наблюдаемую вариацию на «сигнал» (год, экология) и «шум» (пространственная автокорреляция, структура съёмок, нули). Вторая — показать полный воспроизводимый конвейер: от проекции координат до карт плотности и временного индекса с 50% и 95% доверительными интервалами. По дороге мы дисциплинируем себя против типичных когнитивных ловушек: WYSIATI («то, что видим — и есть всё») — когда пара удачных карт заставляет игнорировать нули; подтверждающая предвзятость — когда заранее «знаем», что глубже «хуже»; эффект красивой истории — когда гладкая лента доверия соблазняет не проверять остатки. Антидот — ясные процедуры и проверки, описанные заранее и выполненные последовательно.\nМы начнём с данных. Координаты переводим в метры/километры (UTM), потому что расстояния и сетка «mesh» живут в евклидовом пространстве, а не в градусах. Уборка артефактов и выпуклая оболочка (convex hull) — не косметика: мы явно ограничиваем область предсказаний тем местом, где у нас есть информация. Сетку прогнозирования строим равномерно (шаг 10 км для индексов, более плотный — для карт) и размножаем по годам; если в данных есть переменные усилия (длительность, ширина трала, скорость), используем плотность или offset(log(покрытой площади)) — это превращает «уловы» в сопоставимые «на единицу усилия». Здесь полезно напоминание: если модель «слышит» усилие как сигнал обилия, мы сами подменили биологию методикой отбора.\nДальше — пространство и время. Мы строим треугольную сетку (mesh) c разумным cutoff (например, 10 км) и запасом до границы полигона (чтобы край не «ломал» ковариацию). Пространственный член — стационарный Матерн; пространственно-временной — сначала iid по годам (как минимум), а затем, при необходимости, AR1, если данные поддерживают «инерцию» в годах. Распределение Твиди с лог‑ссылкой — стандарт для траловых съёмок: допускает нули как «структурные» и «стохастические», контролирует передисперсию. Это не догма: дельта‑подход (биномиальная вероятность + гамма интенсивность) иногда лучше, но здесь мы остаёмся в Твиди как в хорошо зарекомендовавшем себя компромиссе.\nФормула модели консервативна и объяснима: фиксированные эффекты включают год (как индикатор индекса) и, при необходимости, тип съёмки (SURV), чтобы стандартизовать методические различия. Экологические ковариаты — глубина, температура, дистанция до берега — добавляются в виде сглаживаний s(DEPTH), s(TEMP), s(DIST) или как монотонные трансформации; каждая ковариата — это гипотеза, а не украшение. Для каждого добавления задаём вопрос: «какую конкретную ошибку я уменьшаю этой переменной?» Если ответ «никакую», это кандидат в удаление. Слишком «гладкая» глубина, которая «улучшает» AIC на единицы и не меняет биологического смысла, вероятно, эксплуатирует шум.\nОценивание — через ML/REML; критерии — AIC/ΔAIC, но не только. Мы смотрим на sanity‑проверки (сходимость, положительно определённая Гессиан, разумный диапазон Матерна, адекватные сигмы), на остатки (гистограмма, QQ‑плот), на семивариограмму резервов, на карты предсказаний и стандартных ошибок. Индекс рассчитываем функцией get_index с bias_correct: на лог‑шкале с случайными эффектами геометрическое среднее систематически смещается — коррекция устраняет этот эффект. Лента неопределённости — 50% и 95% — полезна для управленческих разговоров: медианная динамика и «широкая страховочная» область.\nВизуализация — не финальный штрих, а проверка гипотез. Карты плотности по годам (facet) показывают миграции «горячих пятен» и «дырок» покрытия; точки наблюдений поверх — предохранитель от «галлюцинаций» модели там, где данных не было. Нули — крестиками, чтобы не исчезали в непрерывной шкале. Эффекты ковариат мы рисуем на шкале плотности (после экспоненты), с лентами доверия, фиксируя год и тип съёмки — и спрашиваем себя: «могу ли я объяснить это процессом, а не только формой сглаживания?». Индекс обилия — линия с лентами, с подписью единиц и масштаба (млн экз.), без «спрятанных» преобразований оси.\nВ плане дисциплины мы будем систематичны. Сначала — базовая модель времени (год), чтобы увидеть «скелет» динамики. Затем — добавление SURV (стандартизация методики). После — по одной экологической ковариате, с сопоставлением моделей (AIC, стабильность, биологический смысл, поведение остаточного пространства). Любое улучшение проверяется на чувствительность к настройкам mesh (cutoff, расширение границы), к выбору семейства (дельта vs Твиди), к структуре времени (iid vs AR1). Если модель с глубиной уменьшает AIC и делает диапазон Матерна менее расплывчатым — это аргумент в её пользу; если добавление температуры почти не меняет карт и даёт большой SE сглаживания — это сигнал о слабом или неоднородном эффекте.\nЧтобы минимизировать «быстрые ошибки» — утечки информации и переобучение — мы делаем простые защитные шаги. Мы не подстраиваем сетку по виденному индексу; мы не выбираем шаг прогнозной сетки постфактум «чтобы было красиво»; мы отделяем данные для индекса от «демо‑карт» других лет, если демонстрируем экстраполяции. И мы держим под рукой «план Б»: если в данных явная и сильная избытка нулей, тестируем дельта‑подход; если глубина и температура «ходят» вместе, проверяем их по отдельности или используем частичную зависимость.\nЧто вы получите в итоге. Полный воспроизводимый конвейер: проекция координат в UTM (км), выпуклая оболочка и прогнозная сетка, треугольная mesh, базовая и расширенные модели sdmTMB, карты плотности по годам с точками съёмок, графики эффекта глубины (или других ковариат) с доверительными лентами, и — ключевое — индекс обилия с 50% и 95% ДИ, рассчитанный с коррекцией смещения. Вместе с этим — набор диагностик, который формирует «иммунитет» к излишней уверенности: sanity‑чек модели, остатки, вариограммы, сравнение AIC, проверка масштабов (диапазон Матерна, SD компонентов), и минимальный блок чувствительности к архитектуре mesh и структуре времени.\nИ несколько практичных замечаний напоследок. 1) Проекция и единицы — это не бухгалтерия: неправильные километры превращают «корреляцию на 140 км» в географическую фантазию. 2) Выпуклая оболочка экономит силы модели и удерживает нас от прогнозов «за краем карты»; слишком агрессивная оболочка может «отрезать» настоящее пространство — сравнивайте два уровня жёсткости. 3) Твиди — не панацея: если нули доминируют, дельта‑модель и/или hurdle может дать более прозрачную интерпретацию. 4) Индекс — управленческий инструмент: показывайте медиану и ленты, обозначайте годы со сменой методик, не скрывайте ширину неопределённости. 5) Воспроизводимость — ваш лучший адвокат: фиксируйте seed, версии пакетов, все ключевые параметры сетки и модели прямо в скрипте.\nЭтот курс — про ремесло. Мы не «ловим» красивый график, а выстраиваем цепочку решений, на каждом шаге задавая себе вопрос номер один: «почему это должно быть правдой?» и номер два: вопрос «где здесь могу ошибиться из‑за собственной уверенности?». Если к концу занятия у вас получится модель, чьи выводы понятны, а неопределённость — показана, вы сделали ровно то, что нужно прикладной экологии: превратили разрозненные наблюдения в аккуратное знание, которое можно проверить, воспроизвести и использовать.\nЦель:\nПродемонстрировать применение современных методов SDM (Species Distribution Modeling) и GAMM (Generalized Additive Mixed Models) для стандартизации оценки запасов промысловых видов на примере камчатского краба.\nКлючевые аспекты:\nДля работы скрипта:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>sdmTMB - оценка и визуализация индекса обилия по съемке</span>"
    ]
  },
  {
    "objectID": "chapter 4.html#введение",
    "href": "chapter 4.html#введение",
    "title": "5  sdmTMB - оценка и визуализация индекса обилия по съемке",
    "section": "",
    "text": "Подготовка данных:\n\nПреобразование координат в проекцию UTM (км)\nФильтрация данных через выпуклую оболочку (convex hull)\nСоздание прогнозной сетки с шагом 10 км (2 км)\n\nМоделирование:\n\nПостроение треугольной сетки (mesh) для учета пространственной автокорреляции\nПодбор модели sdmTMB с пространственно-временными случайными эффектами\nУчет ключевых факторов: температура, глубина, тип съемки\n\nВизуализация:\n\nКарты распределения плотности с наложением данных съемок\nДинамика индекса обилия с 50% и 95% доверительными интервалами\n\n\n\n\nСкачайте файл данных (KARTOGRAPHIC.xlsx)\nУстановите рабочую директорию в setwd()\nУстановите необходимые пакеты (см. начало скрипта).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>sdmTMB - оценка и визуализация индекса обилия по съемке</span>"
    ]
  },
  {
    "objectID": "chapter 4.html#базовая-оценка",
    "href": "chapter 4.html#базовая-оценка",
    "title": "5  sdmTMB - оценка и визуализация индекса обилия по съемке",
    "section": "5.2 Базовая оценка",
    "text": "5.2 Базовая оценка\n\n# ---------------------------\n# 1. ПОДГОТОВКА СРЕДЫ И ДАННЫХ\n# ---------------------------\n\n# Очистка рабочей среды\nrm(list = ls())\n\n# Установка рабочей директории (замените на свою)\nsetwd(\"C:/COMBINE/\")\n\n# Загрузка необходимых пакетов\nlibrary(readxl)       # Для чтения Excel-файлов\nlibrary(ggplot2)      # Визуализация данных\nlibrary(dplyr)        # Обработка данных\nlibrary(PBSmapping)   # Для работы с пространственными данными\nlibrary(sdmTMB)       # Пространственно-временное моделирование\nlibrary(INLA)         # Продвинутые пространственные модели\nlibrary(sp)           # Классы для пространственных данных\nlibrary(sf)           # Пространственные данные (современный формат)\nlibrary(rnaturalearth) # Загрузка картографических данных\n\n# Загрузка данных из Excel-файла\ndata &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\")\n\n# Просмотр структуры данных\nstr(data)\n\nДолжно выглядеть так:\n\n&gt; str(data)\ntibble [1,126 x 20] (S3: tbl_df/tbl/data.frame)\n $ NUM    : num [1:1126] 1 2 3 4 5 6 7 8 9 10 ...\n $ CALL   : chr [1:1126] \"UFJN\" \"UFJN\" \"UFJN\" \"UFJN\" ...\n $ CRUSE  : num [1:1126] 112 112 112 112 112 112 112 112 112 112 ...\n $ SURV   : chr [1:1126] \"SUM\" \"SUM\" \"SUM\" \"SUM\" ...\n $ TRAL   : num [1:1126] 2 3 5 7 9 11 13 15 17 19 ...\n $ DATE   : POSIXct[1:1126], format: \"2019-08-16\" \"2019-08-16\" ...\n $ MONTH  : num [1:1126] 8 8 8 8 8 8 8 8 8 8 ...\n $ YEAR   : num [1:1126] 2019 2019 2019 2019 2019 ...\n $ TIME   : chr [1:1126] \"9:43\" \"14:19\" \"19:33\" \"2:47\" ...\n $ DECMIN : num [1:1126] 1.04 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 ...\n $ DUR    : num [1:1126] 1.73 0.25 0.25 0.25 0.25 ...\n $ DEPTH  : num [1:1126] 200 198 196 132 128 131 64 73 91 62 ...\n $ SPEED  : num [1:1126] 3 3 3 3 3 3 3 3 3 3 ...\n $ CATCH  : num [1:1126] 12.9 365.3 253 163.9 55.7 ...\n $ Y      : num [1:1126] 69.5 69.5 69.4 68.8 69.4 ...\n $ X      : num [1:1126] 35.8 35.9 37.4 38.6 39 ...\n $ PROM   : num [1:1126] 2 0 0 3 0 6 6 34 22 9 ...\n $ Density: num [1:1126] 30 0 0 45 0 ...\n $ DIST   : num [1:1126] 28.7 28.7 49.9 37.3 90.8 ...\n $ TEMP   : num [1:1126] 5.57 5.49 4.99 4.8 4.4 ...\n&gt; \n\nДалее:\n\n# --------------------------------------------------\n# 2. ПРЕОБРАЗОВАНИЕ КООРДИНАТ В ПРОЕКЦИЮ UTM (в км)\n# --------------------------------------------------\n\n# Создание пространственного объекта из данных\ndata_sf &lt;- st_as_sf(\n  data, \n  coords = c(\"X\", \"Y\"), # Указание столбцов с координатами\n  crs = 4326            # Система координат WGS84 (широта/долгота)\n) \n\n# Преобразование в UTM зону 37N (метры)\ndata_utm &lt;- st_transform(data_sf, crs = 32637) \n\n# Извлечение координат и перевод в километры\nutm_coords &lt;- st_coordinates(data_utm)\ndata$xkm &lt;- utm_coords[, 1] / 1000  # X в км\ndata$ykm &lt;- utm_coords[, 2] / 1000  # Y в км\n\n# Очистка временных объектов\nrm(data_sf, data_utm, utm_coords)\n\n# -----------------------------------------\n# 3. ОПРЕДЕЛЕНИЕ ГРАНИЦ ИССЛЕДОВАНИЯ\n# -----------------------------------------\n\n# Вычисление границ исследовательского полигона\nxl &lt;- c(min(data$xkm), max(data$xkm))  # Границы по X\nyl &lt;- c(min(data$ykm), max(data$ykm))  # Границы по Y\n\n# ----------------------------------------\n# 4. СОЗДАНИЕ РАСТРОВОЙ СЕТКИ ДЛЯ МОДЕЛИ\n# ----------------------------------------\n\n# Создание равномерной сетки с шагом 10 км \nGRID &lt;- makeGrid(\n  x = seq(xl[1], xl[2], 10), \n  y = seq(yl[1], yl[2], 10),\n  byrow = FALSE,\n  projection = \"UTM\", \n  zone = 37\n)\n\n# Расчет центроидов ячеек сетки\nGRID &lt;- calcCentroid(GRID, rollup = 3)\n\n# -----------------------------------------------------------\n# 5. ПОСТРОЕНИЕ ВЫПУКЛОЙ ОБОЛОЧКИ (CONVEX HULL) ДЛЯ ДАННЫХ\n# -----------------------------------------------------------\n\n# Создание выпуклой оболочки вокруг точек данных\nHull &lt;- inla.nonconvex.hull(cbind(data$xkm, data$ykm), convex = -0.03)\n\n# Визуализация оболочки \n plot(Hull)\n\n\n\n\nРис. 1.: Визуализация оболочки съемок\n\n\n\n# Визуализация оболочки и точек съемок 2019-2024\npoints(data$xkm, data$ykm, pch=1, cex=0.55,col=\"black\")\n\n\n\n\nРис. 2.: Визуализация оболочки и точек съемок\n\n\n\n# Фильтрация сетки: оставляем только точки внутри оболочки\nline &lt;- Hull$loc[, 1:2] %&gt;% as.data.frame()\ncolnames(line) &lt;- c(\"X\", \"Y\")\nGRID$AREA &lt;- point.in.polygon(GRID$X, GRID$Y, line$X, line$Y)\nGRID &lt;- GRID[GRID$AREA &gt; 0.1, c(\"X\", \"Y\")]  # Только внутренние точки\n\n# -------------------------------------------------\n# 6. ПОДГОТОВКА СЕТКИ ДЛЯ ПРОГНОЗИРОВАНИЯ\n# -------------------------------------------------\n\n# Создание временной сетки (для каждого года)\ngrid &lt;- replicate_df(GRID, \"YEAR\", unique(data$YEAR))\ncolnames(grid) &lt;- c(\"xkm\", \"ykm\", \"YEAR\")\ngrid$SURV &lt;- \"CRAB\"  # Добавляем информацию о типе съемки\n\n# Визуализация оболочки и сетки для прогнозирования (grid)\n plot(Hull)\n points(grid$xkm, grid$ykm, pch=1, cex=0.55,col=\"black\")\n\n\n\n\nРис. 3.: Визуализация оболочки и сетки для прогнозирования (grid)\n\n\n\n# ---------------------------------------------------\n# 7. ПОСТРОЕНИЕ ПРОСТРАНСТВЕННОЙ СЕТКИ (MESH)\n# ---------------------------------------------------\n\n# Создание треугольной сетки для пространственного моделирования\nmesh_sdm &lt;- make_mesh(\n  data, \n  c(\"xkm\", \"ykm\"),  # Координаты\n  cutoff = 10        # Минимальное расстояние между узлами (км)\n)\n\n# Визуализация сетки \n plot(mesh_sdm)\n\n\n\n\nРис. 4.: Визуализация сетки (mesh)\n\n\n\n# ---------------------------------------------------\n# 8. ПОСТРОЕНИЕ ПРОСТРАНСТВЕННО-ВРЕМЕННОЙ МОДЕЛИ\n# ---------------------------------------------------\n\nm &lt;- sdmTMB(\n  data = data, \n  formula = Density ~ 0 + as.factor(YEAR),  # Формула: плотность зависит от года\n  time = \"YEAR\",         # Временная переменная\n  mesh = mesh_sdm,       # Пространственная сетка\n  family = tweedie(link = \"log\"),  # Статистическое распределение\n  spatial = \"on\",        # Включение пространственных эффектов\n  spatiotemporal = \"iid\" # Пространственно-временные эффекты\n)\n\n# Вывод результатов модели\nsummary(m)\nAIC(m)  # Критерий Акаике\nsanity(m)  # Проверка корректности модели\n\nПолучили результаты:\n\n&gt; # Вывод результатов модели\n&gt; summary(m)\nSpatiotemporal model fit by ML ['sdmTMB']\nFormula: Density ~ 0 + as.factor(YEAR)\nMesh: mesh_sdm (isotropic covariance)\nTime column: YEAR\nData: data\nFamily: tweedie(link = 'log')\n \nConditional model:\n                    coef.est coef.se\nas.factor(YEAR)2019     2.15    0.72\nas.factor(YEAR)2020     1.49    0.76\nas.factor(YEAR)2021     1.74    0.76\nas.factor(YEAR)2022     1.62    0.73\nas.factor(YEAR)2023     1.50    0.74\nas.factor(YEAR)2024     1.56    0.72\n\nDispersion parameter: 19.71\nTweedie p: 1.50\nMatern range: 142.65\nSpatial SD: 2.01\nSpatiotemporal IID SD: 0.95\nML criterion at convergence: 5984.224\n\nSee ?tidy.sdmTMB to extract these values as a data frame.\n&gt; AIC(m)  # Критерий Акаике\n[1] 11990.45\n&gt; sanity(m)  # Проверка корректности модели\nv Non-linear minimizer suggests successful convergence\nv Hessian matrix is positive definite\nv No extreme or very small eigenvalues detected\nv No gradients with respect to fixed effects are &gt;= 0.001\nv No fixed-effect standard errors are NA\nv No standard errors look unreasonably large\nv No sigma parameters are &lt; 0.01\nv No sigma parameters are &gt; 100\nv Range parameter doesn't look unreasonably large\n\n\n5.2.0.1 Годовые эффекты:\n2019: 2.15 ± 0.72 → exp(2.15) ≈ 8.58 экз./км²\n2020: 1.49 ± 0.76 → exp(1.49) ≈ 4.44 экз./км²\n2024: 1.56 ± 0.72 → exp(1.56) ≈ 4.76 экз./км²\n\n2019 год - пик запаса (8.58 экз./км²)\n2020 год - резкое снижение (-52% к 2019)\n2021-2024 - стабилизация на уровне ~4.5-5.0 экз./км²\nСтандартные ошибки ~0.75:\n\nПриемлемая точность для данных такого объема\nВсе годовые оценки статистически значимы\n\nМодель пространственно-временного распределения плотности камчатского краба успешно прошла все диагностические проверки, демонстрируя отличную сходимость и статистическую надежность. Параметр распределения Твиди (p=1.50) оптимально соответствует данным траловых съемок, учитывая характерную для уловов передисперсию и избыток нулевых значений.\nГодовые оценки показывают выраженную динамику запаса: в 2019 году зафиксирован пик плотности (8.58 экз./км²), после чего в 2020 году произошло резкое снижение до 4.44 экз./км². В последующие годы (2021-2024) плотность стабилизировалась на уровне 4.5-5.0 экз./км², что составляет примерно 55% от максимальных значений 2019 года. Стандартные ошибки годовых коэффициентов (0.72-0.76) свидетельствуют о хорошей точности оценок при текущем объеме данных.\nПространственная структура распределения характеризуется крупномасштабными скоплениями с диапазоном корреляции 143 км (Matern range: 142.65), что согласуется с известными особенностями миграций камчатского краба. Высокое значение пространственной изменчивости (SD=2.01) отражает типичную для вида мозаичность распределения, где участки высокой плотности соседствуют с зонами отсутствия особей. Умеренная пространственно-временная изменчивость (IID SD=0.95) указывает на относительную стабильность пространственной структуры запаса между годами.\nПараметр дисперсии (19.71) подтверждает ожидаемо высокую вариабельность данных, характерную для траловых съемок морских гидробионтов. Полученные результаты надежно фиксируют значительное сокращение запаса после 2019 года с последующей стабилизацией на пониженном уровне.\n5.2.1 Пояснение результатов sanity(m) для начинающих гидробиологов (от DeepSeek):\n1. v Non-linear minimizer suggests successful convergence\n(Нелинейный оптимизатор успешно сошелся)\nПояснение: Алгоритм поиска параметров модели корректно завершил работу. Это значит, что модель “научилась” описывать ваши данные и не застряла в промежуточных вычислениях. Как если бы вы успешно завершили лабораторный анализ без технических сбоев.\n2. v Hessian matrix is positive definite\n(Матрица Гессе положительно определена)\nПояснение: Математическое подтверждение, что найденные параметры модели действительно оптимальны. Аналогично тому, как в микроскопии вы видите четкий фокус - здесь модель “четко видит” закономерности в данных.\n3. v No extreme or very small eigenvalues detected\n(Не обнаружено экстремальных или очень маленьких собственных значений)\nПояснение: Модель статистически стабильна. Представьте, что вы измеряете длину рыб - если бы ваш штангенциркуль иногда показывал 0 или 1000 мм, это было бы проблемой. Здесь аналогично - вычисления надежны.\n4. v No gradients with respect to fixed effects are &gt;= 0.001\n(Градиенты для фиксированных эффектов &lt; 0.001)\nПояснение: Все ключевые параметры модели (например, влияние года на плотность) рассчитаны точно. Это как убедиться, что все измерения в вашем эксперименте выполнены с требуемой точностью (±0.1 мг, ±1 см и т.д.).\n5. v No fixed-effect standard errors are NA\n(Стандартные ошибки для фиксированных эффектов не отсутствуют)\nПояснение: Для каждого рассчитанного параметра (например, годовых оценок) указана погрешность. Важно как в химическом анализе - если для концентрации вещества нет погрешности, результат ненадежен.\n6. v No standard errors look unreasonably large\n(Стандартные ошибки выглядят разумными)\nПояснение: Погрешности оценок адекватны. Например, если плотность краба 5±1 экз./км² - это нормально, но 5±100 экз./км² было бы бессмысленным.\n7. v No sigma parameters are &lt; 0.01\n(Параметры сигма не меньше 0.01)\nПояснение: Модель не игнорирует важные источники изменчивости. Аналогично тому, что в пробе воды вы не упустили бы важный показатель, сказав “он слишком мал”.\n8. v No sigma parameters are &gt; 100\n(Параметры сигма не превышают 100)\nПояснение: Модель не преувеличивает случайные вариации. Как если бы вы не приписали естественные колебания температуры воды катастрофическому изменению климата.\n9. v Range parameter doesn't look unreasonably large\n(Параметр диапазона не выглядит чрезмерно большим)\nПояснение: Пространственная автокорреляция имеет биологически осмысленный масштаб. Например, если модель показала бы, что скопления краба одинаковы на расстоянии 1000 км - это было бы нереалистично.\n\n\n# ---------------------------------------------------\n# 9. ДИАГНОСТИКА МОДЕЛИ\n# ---------------------------------------------------\n\n# Расчет остатков модели\ndata$resids &lt;- residuals(m) \n\n# Гистограмма остатков\nhist(data$resids)\n\n# График квантиль-квантиль\nqqnorm(data$resids)\nabline(a = 0, b = 1)\n\n\n\n\nРис. 5.: Гистограмма остатков\n\n\n\n\n\nРис. 6.: График квантиль-квантиль\n\n\n\n# ---------------------------------------------------\n# 10. ПРОГНОЗИРОВАНИЕ НА СЕТКЕ\n# ---------------------------------------------------\n\n# Прогноз значений плотности на сетке\npredictions &lt;- predict(m, newdata = grid, return_tmb_object = TRUE)\nRASP &lt;- predictions$data\n\n# Преобразование координат обратно в широту/долготу\nRASP$xkm_m &lt;- RASP$xkm * 1000  # Обратно в метры\nRASP$ykm_m &lt;- RASP$ykm * 1000\n\n# Создание пространственного объекта в UTM\nutm_proj &lt;- CRS(\"+proj=utm +zone=37 +datum=WGS84 +units=m +no_defs\")\ncoords &lt;- cbind(RASP$xkm_m, RASP$ykm_m)\nsp_points &lt;- SpatialPoints(coords, proj4string = utm_proj)\n\n# Преобразование в WGS84 (широта/долгота)\nwgs84_proj &lt;- CRS(\"+proj=longlat +datum=WGS84\")\nsp_points_latlon &lt;- spTransform(sp_points, wgs84_proj)\n\n# Добавление координат в основной датафрейм\nRASP$X &lt;- coordinates(sp_points_latlon)[, 1]  # Долгота\nRASP$Y &lt;- coordinates(sp_points_latlon)[, 2]  # Широта\n\n# Удаление временных столбцов\nRASP$xkm_m &lt;- NULL\nRASP$ykm_m &lt;- NULL\n\n# Проверка структуры результата\nstr(RASP)\n\n# ---------------------------------------------\n# 11. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ (КАРТА)\n# ---------------------------------------------\n\n# Загрузка картографических данных\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Определение региона интереса (Арктика России)\narctic_bbox &lt;- st_bbox(c(xmin = 25, xmax = 70, ymin = 65, ymax = 80), crs = 4326)\narctic &lt;- st_crop(world, arctic_bbox)\n\n# Кастомные разрывы для цветовой шкалы\nmy_breaks &lt;- c(0.001, 0.1, 1, 200, 10000)\n\n# Создание основной визуализации\nggplot() +\n  # Теплокарта плотности\n  geom_point(\n    data = RASP, \n    aes(x = X, y = Y, color = exp(est)), \n    size = 0.8, \n    alpha = 0.7\n  ) + \n  # Наблюдаемые точки данных\n  geom_point(\n    data = data, \n    aes(x = X, y = Y, size = PROM), # Размер по плотности\n    color = \"black\", \n    fill = NA, \n    alpha = 0.6,\n    shape = 21 # Кружки с обводкой\n  ) +\n  # Картографическая подложка\n  geom_sf(data = arctic, fill = \"lightgrey\", color = \"darkgrey\") +\n  # Цветовая шкала (логарифмическая)\n  scale_color_viridis_c(\n    name = \"\",\n    option = \"H\", \n    trans = \"log\", \n    breaks = my_breaks, \n    labels = my_breaks\n  ) +\n  # Разделение по годам\n  facet_wrap(~ YEAR, ncol = 2) +\n  # Настройка области просмотра\n  coord_sf(\n    xlim = c(min(RASP$X)-1, max(RASP$X)+1),\n    ylim = c(min(RASP$Y)-0.5, max(RASP$Y)+0.5),\n    crs = 4326\n  ) +\n  # Тема оформления\n  theme_bw(base_size = 12) +\n  labs(x = \"Долгота\", y = \"Широта\", title = \"Пространственное распределение плотности\") +\n  theme(\n    panel.grid = element_line(color = \"grey90\"),\n    legend.position = \"bottom\",\n    legend.key.width = unit(1.2, \"cm\"),\n    strip.background = element_rect(fill = \"white\")\n  )\n\n# Сохранение графика (раскомментируйте)\n# ggsave(\"sdmTMBmap10.jpg\", width = 8, height = 8, dpi = 300)\n\n\n\n\nРис. 7.: Визуализация результатов (КАРТА)\n\n\n\n# ---------------------------------------------------\n# 12. РАСЧЕТ ИНДЕКСОВ ОБИЛИЯ\n# ---------------------------------------------------\n\n# Расчет индексов с разными доверительными интервалами\nindex &lt;- get_index(predictions, area = 4, level = 0.95, bias_correct = TRUE)\nindex2 &lt;- get_index(predictions, area = 4, level = 0.5, bias_correct = TRUE)\n\n# Формирование сводной таблицы результатов\ntotal &lt;- data.frame(\n  YEAR = index$YEAR,\n  lwr_95 = index$lwr,\n  lwr_50 = index2$lwr,\n  estimate = index$est,\n  upr_50 = index2$upr,\n  upr_95 = index$upr,\n  se = index$se,\n  cv = sqrt(exp(index$se^2) - 1) # Коэффициент вариации\n)\n\n# Визуализация индексов обилия\nggplot(total, aes(x = YEAR, y = estimate/1000000)) + \n  # Основная линия оценки\n  geom_line(linewidth = 1, color = \"steelblue\") +\n  \n  # 95% доверительный интервал (более широкий и прозрачный)\n  geom_ribbon(\n    aes(ymin = lwr_95/1000000, ymax = upr_95/1000000),\n    alpha = 0.2,  # Полупрозрачность\n    fill = \"steelblue\",\n    color = NA     # Без контура\n  ) +\n  \n  # 50% доверительный интервал (менее прозрачный)\n  geom_ribbon(\n    aes(ymin = lwr_50/1000000, ymax = upr_50/1000000),\n    alpha = 0.4,  # Меньшая прозрачность\n    fill = \"steelblue\",\n    color = NA\n  ) +\n  \n  # Настройки осей и заголовков\n  ylab('Промысловый запас, млн. экз') +\n  xlab('Год') +\n  \n  # Вертикальные линии для годов\n  geom_vline(\n    xintercept = total$YEAR, \n    linetype = \"dotted\", \n    color = \"grey60\", \n    alpha = 0.6\n  ) +\n  \n  # Точки с значениями оценок\n  geom_point(\n    size = 3,\n    color = \"navyblue\",\n    fill = \"white\",\n    shape = 21\n  ) +\n  \n  # Настройка темы\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"grey90\"),\n    axis.line = element_line(color = \"grey30\"),\n    legend.position = \"none\"\n  )\n\n\n\n\nРис. 8.: Визуализация индексов обилия\n\n\n\n# Форматированный вывод результатов\ntotal %&gt;% \n  mutate(cv_percent = 100 * cv) %&gt;% \n  select(\n    YEAR, \n    estimate, \n    lwr_50,  # Нижняя граница 50% ДИ\n    upr_50,  # Верхняя граница 50% ДИ\n    lwr_95,  # Нижняя граница 95% ДИ\n    upr_95,  # Верхняя граница 95% ДИ\n    cv_percent\n  ) %&gt;%\n  knitr::kable(\n    format = \"pandoc\", \n    digits = c(0, 0, 0, 0, 0, 0, 1),\n    col.names = c(\n      \"Год\", \n      \"Оценка\", \n      \"Нижняя 50%\", \n      \"Верхняя 50%\", \n      \"Нижняя 95%\", \n      \"Верхняя 95%\", \n      \"CV%\"\n    )\n  )\n\n\n  Год    Оценка   Нижняя 50%   Верхняя 50%   Нижняя 95%   Верхняя 95%    CV%\n-----  --------  -----------  ------------  -----------  ------------  -----\n 2019   2381774      2177448       2605274      1835312       3090946   13.4\n 2020   1634549      1539111       1735906      1372377       1946805    8.9\n 2021   1920507      1794122       2055795      1575823       2340584   10.1\n 2022   1036673       959251       1120344       827345       1298963   11.5\n 2023   1147685      1068401       1232853       932147       1413062   10.6\n 2024   1055733       985624       1130829       864640       1289060   10.2\n&gt;",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>sdmTMB - оценка и визуализация индекса обилия по съемке</span>"
    ]
  },
  {
    "objectID": "chapter 4.html#базовая-оценка-предикторы",
    "href": "chapter 4.html#базовая-оценка-предикторы",
    "title": "5  sdmTMB - оценка и визуализация индекса обилия по съемке",
    "section": "5.3 Базовая оценка + предикторы",
    "text": "5.3 Базовая оценка + предикторы\nСравнение пространственно-временных моделей sdmTMB с учетом типа съемки (SURV) и года (YEAR)\nРассмотрим 4 пространственно-временные модели, оценивая их по:\nКачеству подгонки (AIC) Стабильности оценок (sanity check) Значимости ковариат Биологическому смыслу\n4 модели: базовая модель, модель с глубиной (DEPTH),модель с температурой (TEMP), модель с расстоянием до берега (DIST)\n\n&gt; # 8. ПОСТРОЕНИЕ ПРОСТРАНСТВЕННО-ВРЕМЕННОЙ МОДЕЛИ\n&gt; # ---------------------------------------------------\n&gt; \n&gt; m &lt;- sdmTMB(\n+   data = data, \n+   formula = Density ~ 0+ as.factor(SURV) + as.factor(YEAR),  # Формула: плотность зависит от года\n+   time = \"YEAR\",         # Временная переменная\n+   mesh = mesh_sdm,       # Пространственная сетка\n+   family = tweedie(link = \"log\"),  # Статистическое распределение\n+   spatial = \"on\",        # Включение пространственных эффектов\n+   spatiotemporal = \"iid\" # Пространственно-временные эффекты\n+ )\n&gt; \n&gt; \n&gt; # Вывод результатов модели\n&gt; summary(m)\nSpatiotemporal model fit by ML ['sdmTMB']\nFormula: Density ~ 0 + as.factor(SURV) + as.factor(YEAR)\nMesh: mesh_sdm (isotropic covariance)\nTime column: YEAR\nData: data\nFamily: tweedie(link = 'log')\n \nConditional model:\n                    coef.est coef.se\nas.factor(SURV)CRAB     4.75    0.44\nas.factor(SURV)SUM      2.54    0.37\nas.factor(YEAR)2020    -0.57    0.36\nas.factor(YEAR)2021    -0.20    0.36\nas.factor(YEAR)2022    -0.61    0.36\nas.factor(YEAR)2023    -0.59    0.36\nas.factor(YEAR)2024    -0.85    0.36\n\nDispersion parameter: 23.16\nTweedie p: 1.41\nMatern range: 63.53\nSpatial SD: 1.22\nSpatiotemporal IID SD: 0.97\nML criterion at convergence: 5914.655\n\nSee ?tidy.sdmTMB to extract these values as a data frame.\n&gt; AIC(m)  # Критерий Акаике\n[1] 11853.31\n&gt; sanity(m)  # Проверка корректности модели\nv Non-linear minimizer suggests successful convergence\nv Hessian matrix is positive definite\nv No extreme or very small eigenvalues detected\nv No gradients with respect to fixed effects are &gt;= 0.001\nv No fixed-effect standard errors are NA\nv No standard errors look unreasonably large\nv No sigma parameters are &lt; 0.01\nv No sigma parameters are &gt; 100\nv Range parameter doesn't look unreasonably large\n&gt; \n&gt; md &lt;- sdmTMB(\n+   data = data, \n+   formula = Density ~ 0+ as.factor(SURV) + as.factor(YEAR)+s(DEPTH),  # Формула: плотность зависит от года\n+   time = \"YEAR\",         # Временная переменная\n+   mesh = mesh_sdm,       # Пространственная сетка\n+   family = tweedie(link = \"log\"),  # Статистическое распределение\n+   spatial = \"on\",        # Включение пространственных эффектов\n+   spatiotemporal = \"iid\" # Пространственно-временные эффекты\n+ )\n&gt; \n&gt; \n&gt; # Вывод результатов модели\n&gt; summary(md)\nSpatiotemporal model fit by ML ['sdmTMB']\nFormula: Density ~ 0 + as.factor(SURV) + as.factor(YEAR) + s(DEPTH)\nMesh: mesh_sdm (isotropic covariance)\nTime column: YEAR\nData: data\nFamily: tweedie(link = 'log')\n \nConditional model:\n                    coef.est coef.se\nas.factor(SURV)CRAB     5.42    0.32\nas.factor(SURV)SUM      3.09    0.28\nas.factor(YEAR)2020    -0.52    0.27\nas.factor(YEAR)2021    -0.15    0.27\nas.factor(YEAR)2022    -0.67    0.27\nas.factor(YEAR)2023    -0.63    0.27\nas.factor(YEAR)2024    -0.93    0.27\nsDEPTH                 -0.60    0.42\n\nSmooth terms:\n           Std. Dev.\nsds(DEPTH)      1.71\n\nDispersion parameter: 24.43\nTweedie p: 1.39\nMatern range: 40.20\nSpatial SD: 0.97\nSpatiotemporal IID SD: 0.94\nML criterion at convergence: 5907.365\n\nSee ?tidy.sdmTMB to extract these values as a data frame.\n&gt; AIC(md)  # Критерий Акаике\n[1] 11842.73\n&gt; sanity(md)  # Проверка корректности модели\nv Non-linear minimizer suggests successful convergence\nv Hessian matrix is positive definite\nv No extreme or very small eigenvalues detected\nv No gradients with respect to fixed effects are &gt;= 0.001\nv No fixed-effect standard errors are NA\nv No standard errors look unreasonably large\nv No sigma parameters are &lt; 0.01\nv No sigma parameters are &gt; 100\nv Range parameter doesn't look unreasonably large\n&gt; \n&gt; \n&gt; mt &lt;- sdmTMB(\n+   data = data, \n+   formula = Density ~ 0+ as.factor(SURV) + as.factor(YEAR)+s(TEMP),  # Формула: плотность зависит от года\n+   time = \"YEAR\",         # Временная переменная\n+   mesh = mesh_sdm,       # Пространственная сетка\n+   family = tweedie(link = \"log\"),  # Статистическое распределение\n+   spatial = \"on\",        # Включение пространственных эффектов\n+   spatiotemporal = \"iid\" # Пространственно-временные эффекты\n+ )\n&gt; \n&gt; \n&gt; # Вывод результатов модели\n&gt; summary(mt)\nSpatiotemporal model fit by ML ['sdmTMB']\nFormula: Density ~ 0 + as.factor(SURV) + as.factor(YEAR) + s(TEMP)\nMesh: mesh_sdm (isotropic covariance)\nTime column: YEAR\nData: data\nFamily: tweedie(link = 'log')\n \nConditional model:\n                    coef.est coef.se\nas.factor(SURV)CRAB     4.95    0.40\nas.factor(SURV)SUM      2.69    0.34\nas.factor(YEAR)2020    -0.14    0.43\nas.factor(YEAR)2021    -0.19    0.33\nas.factor(YEAR)2022    -0.77    0.42\nas.factor(YEAR)2023    -0.62    0.33\nas.factor(YEAR)2024    -0.91    0.34\nsTEMP                   0.80    0.83\n\nSmooth terms:\n          Std. Dev.\nsds(TEMP)      3.15\n\nDispersion parameter: 23.42\nTweedie p: 1.40\nMatern range: 55.04\nSpatial SD: 1.12\nSpatiotemporal IID SD: 0.96\nML criterion at convergence: 5912.795\n\nSee ?tidy.sdmTMB to extract these values as a data frame.\n&gt; AIC(mt)  # Критерий Акаике\n[1] 11853.59\n&gt; sanity(mt)  # Проверка корректности модели\nv Non-linear minimizer suggests successful convergence\nv Hessian matrix is positive definite\nv No extreme or very small eigenvalues detected\nv No gradients with respect to fixed effects are &gt;= 0.001\nv No fixed-effect standard errors are NA\nv No standard errors look unreasonably large\nv No sigma parameters are &lt; 0.01\nv No sigma parameters are &gt; 100\nv Range parameter doesn't look unreasonably large\n&gt; \n&gt; mdist &lt;- sdmTMB(\n+   data = data, \n+   formula = Density ~ 0+ as.factor(SURV) + as.factor(YEAR)+s(DIST),  # Формула: плотность зависит от года\n+   time = \"YEAR\",         # Временная переменная\n+   mesh = mesh_sdm,       # Пространственная сетка\n+   family = tweedie(link = \"log\"),  # Статистическое распределение\n+   spatial = \"on\",        # Включение пространственных эффектов\n+   spatiotemporal = \"iid\" # Пространственно-временные эффекты\n+ )\n&gt; \n&gt; \n&gt; # Вывод результатов модели\n&gt; summary(mdist)\nSpatiotemporal model fit by ML ['sdmTMB']\nFormula: Density ~ 0 + as.factor(SURV) + as.factor(YEAR) + s(DIST)\nMesh: mesh_sdm (isotropic covariance)\nTime column: YEAR\nData: data\nFamily: tweedie(link = 'log')\n \nConditional model:\n                    coef.est coef.se\nas.factor(SURV)CRAB     4.74    0.44\nas.factor(SURV)SUM      2.55    0.37\nas.factor(YEAR)2020    -0.57    0.36\nas.factor(YEAR)2021    -0.20    0.36\nas.factor(YEAR)2022    -0.61    0.36\nas.factor(YEAR)2023    -0.60    0.36\nas.factor(YEAR)2024    -0.85    0.36\nsDIST                  -0.06    0.16\n\nSmooth terms:\n          Std. Dev.\nsds(DIST)         0\n\nDispersion parameter: 23.11\nTweedie p: 1.41\nMatern range: 63.83\nSpatial SD: 1.22\nSpatiotemporal IID SD: 0.97\nML criterion at convergence: 5914.594\n\nSee ?tidy.sdmTMB to extract these values as a data frame.\n\n**Possible issues detected! Check output of sanity().**\n&gt; AIC(mdist)  # Критерий Акаике\n[1] 11857.19\n&gt; sanity(mdist)  # Проверка корректности модели\nv Non-linear minimizer suggests successful convergence\nv Hessian matrix is positive definite\nv No extreme or very small eigenvalues detected\nv No gradients with respect to fixed effects are &gt;= 0.001\nv No fixed-effect standard errors are NA\nx `ln_smooth_sigma` standard error may be large\ni Try simplifying the model, adjusting the mesh, or adding priors\n\nv No sigma parameters are &lt; 0.01\nv No sigma parameters are &gt; 100\nv Range parameter doesn't look unreasonably large\n&gt; \n\n\n5.3.0.1 1. Базовая модель (SURV + YEAR)\nDensity ~ 0 + as.factor(SURV) + as.factor(YEAR)\n\nAIC: 11853.31\nПроверка стабильности: Все параметры стабильны\nКлючевые эффекты:\n\nВысокая плотность в съемках CRAB (коэф. 4.75)\nСнижение плотности во всех годах относительно базового уровня (2020-2024: -0.57 до -0.85)\n\nПространственные параметры:\n\nДиапазон Матерна: 63.53 км\nПространственная SD: 1.22\n\n\n\n\n5.3.0.2 2. Модель с глубиной (DEPTH)\nDensity ~ 0 + as.factor(SURV) + as.factor(YEAR) + s(DEPTH)\n\nAIC: 11842.73 (наилучший)\nПроверка стабильности: Все параметры стабильны\nКлючевые эффекты:\n\nСильное отрицательное влияние глубины (коэф. -0.60, SE=0.42)\nУсиление контраста между съемками CRAB/SUM (CRAB: 5.42 vs SUM: 3.09)\n\nУлучшения:\n\nСнижение AIC на 10.58 пунктов\nУменьшение пространственного диапазона (40.20 км)\n\nИнтерпретация: Глубина — значимый экологический фактор распределения\n\n\n\n5.3.0.3 3. Модель с температурой (TEMP)\nDensity ~ 0 + as.factor(SURV) + as.factor(YEAR) + s(TEMP)\n\nAIC: 11853.59 (хуже базовой)\nПроверка стабильности: Стабильна, но высокий SE сглаживания\nКлючевые эффекты:\n\nСлабый положительный эффект температуры (коэф. 0.80, SE=0.83)\nНезначительное изменение годовых эффектов\n\nПроблемы: Минимальное улучшение модели, высокая неопределенность эффекта температуры\n\n\n\n5.3.0.4 4. Модель с расстоянием (DIST)\nDensity ~ 0 + as.factor(SURV) + as.factor(YEAR) + s(DIST)\n\nAIC: 11857.19 (наихудший)\nПроверка стабильности: Проблемы со сглаживанием\nКлючевые эффекты:\n\nНезначительный эффект расстояния (коэф. -0.06, SE=0.16)\nПрактически идентична базовой модели\n\nПроблемы: Наихудший AIC, предупреждения о нестабильности\n\n\n\n5.3.1 Сводка сравнения моделей\n\n\n\n\n\n\n\n\n\n\n\nМодель\nAIC\nΔAIC\nСтабильность\nКлючевой предиктор\nЭффект ковариаты\n\n\n\n\nDEPTH\n11842.73\n-\n✓✓✓\nГлубина\nСильный (-0.60)\n\n\nБазовая\n11853.31\n+10.6\n✓✓✓\n-\n-\n\n\nTEMP\n11853.59\n+10.9\n✓✓\nТемпература\nСлабый (+0.80)\n\n\nDIST\n11857.19\n+14.5\n✗\nРасстояние\nНезначительный (-0.06)\n\n\n\n\n\n5.3.2 Рекомендации\n\nЛучшая модель: С глубиной (DEPTH)\n\nЗначительное улучшение AIC (-10.58)\nБиологически интерпретируемый эффект (глубина — ключевой фактор распределения краба)\nСтабильные оценки параметров\n\nПрактическое значение:\n\nГлубина объясняет ~12% пространственной вариабельности (судя по изменению пространственной SD)\nМодель адекватно отражает экологические предпочтения вида\n\n\n\nВывод: Включение глубины как ковариаты существенно улучшает модель, тогда как температура и расстояние не дают значимых улучшений.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>sdmTMB - оценка и визуализация индекса обилия по съемке</span>"
    ]
  },
  {
    "objectID": "chapter 4.html#визуализация-эффектов",
    "href": "chapter 4.html#визуализация-эффектов",
    "title": "5  sdmTMB - оценка и визуализация индекса обилия по съемке",
    "section": "5.4 Визуализация эффектов",
    "text": "5.4 Визуализация эффектов\nМодель с глубиной - md (см. передыдущий скрипт)\n\n# ---------------------------------------------------\n# 8.1. ВИЗУАЛИЗАЦИЯ ЭФФЕКТА ГЛУБИНЫ\n# ---------------------------------------------------\n\n# Создаем новый датафрейм для предсказаний\nnewdata &lt;- expand.grid(\n  DEPTH = seq(50, 400, by = 2),\n  YEAR = 2020,\n  SURV = \"CRAB\",\n  xkm = mean(data$xkm),\n  ykm = mean(data$ykm)\n)\n\n# Делаем предсказания с расчетом стандартных ошибок\npred &lt;- predict(md, newdata = newdata, re_formula = NA, se_fit = TRUE)\n\n\n# Визуализируем эффект глубины\nggplot(pred, aes(x = DEPTH, y = exp(est))) +\n  geom_line(linewidth = 1.2, color = \"blue4\") +\n  geom_ribbon(\n    aes(\n      ymin = exp(est - 1.96 * est_se), \n      ymax = exp(est + 1.96 * est_se)  # Исправлено на se.fit\n    ),\n    alpha = 0.3, \n    fill = \"steelblue\"\n  ) +\n  labs(\n    title = \"Эффект глубины на плотность краба\",\n    subtitle = \"Год: 2020, Тип съемки: CRAB\",\n    x = \"Глубина (м)\",\n    y = \"Предсказанная плотность (особей/км²)\"\n  ) +\n  theme_bw(base_size = 14) +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\nРис. 9.: Визуализация эффекта глубины на плотность краба",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>sdmTMB - оценка и визуализация индекса обилия по съемке</span>"
    ]
  },
  {
    "objectID": "chapter 4.html#карта-с-акцентом-на-нулевые-уловы",
    "href": "chapter 4.html#карта-с-акцентом-на-нулевые-уловы",
    "title": "5  sdmTMB - оценка и визуализация индекса обилия по съемке",
    "section": "5.5 Карта с акцентом на нулевые уловы",
    "text": "5.5 Карта с акцентом на нулевые уловы\nПовторяем базовую оценку, но меняем в карте нулевые уловы на крестики\n\n\n\nРис. 10.: Визуализация результатов (КАРТА)\n\n\nСкрипт для карты с базовой оценкой\n\n# ---------------------------\n# 1. ПОДГОТОВКА СРЕДЫ И ДАННЫХ\n# ---------------------------\n\n# Очистка рабочей среды\nrm(list = ls())\n\n# Установка рабочей директории (замените на свою)\nsetwd(\"C:/COMBINE/\")\n\n# Загрузка необходимых пакетов\nlibrary(readxl)       # Для чтения Excel-файлов\nlibrary(ggplot2)      # Визуализация данных\nlibrary(dplyr)        # Обработка данных\nlibrary(PBSmapping)   # Для работы с пространственными данными\nlibrary(sdmTMB)       # Пространственно-временное моделирование\nlibrary(INLA)         # Продвинутые пространственные модели\nlibrary(sp)           # Классы для пространственных данных\nlibrary(sf)           # Пространственные данные (современный формат)\nlibrary(rnaturalearth) # Загрузка картографических данных\n\n# Загрузка данных из Excel-файла\ndata &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\")\n\n# Просмотр структуры данных\nstr(data)\n\n\n# --------------------------------------------------\n# 2. ПРЕОБРАЗОВАНИЕ КООРДИНАТ В ПРОЕКЦИЮ UTM (в км)\n# --------------------------------------------------\n\n# Создание пространственного объекта из данных\ndata_sf &lt;- st_as_sf(\n  data, \n  coords = c(\"X\", \"Y\"), # Указание столбцов с координатами\n  crs = 4326            # Система координат WGS84 (широта/долгота)\n) \n\n# Преобразование в UTM зону 37N (метры)\ndata_utm &lt;- st_transform(data_sf, crs = 32637) \n\n# Извлечение координат и перевод в километры\nutm_coords &lt;- st_coordinates(data_utm)\ndata$xkm &lt;- utm_coords[, 1] / 1000  # X в км\ndata$ykm &lt;- utm_coords[, 2] / 1000  # Y в км\n\n# Очистка временных объектов\nrm(data_sf, data_utm, utm_coords)\n\n# -----------------------------------------\n# 3. ОПРЕДЕЛЕНИЕ ГРАНИЦ ИССЛЕДОВАНИЯ\n# -----------------------------------------\n\n# Вычисление границ исследовательского полигона\nxl &lt;- c(min(data$xkm), max(data$xkm))  # Границы по X\nyl &lt;- c(min(data$ykm), max(data$ykm))  # Границы по Y\n\n# ----------------------------------------\n# 4. СОЗДАНИЕ РАСТРОВОЙ СЕТКИ ДЛЯ МОДЕЛИ\n# ----------------------------------------\n\n# Создание равномерной сетки с шагом 10 км (для визуализации карты использовался шаг 2 км\nGRID &lt;- makeGrid(\n  x = seq(xl[1], xl[2], 10), \n  y = seq(yl[1], yl[2], 10),\n  byrow = FALSE,\n  projection = \"UTM\", \n  zone = 37\n)\n\n# Расчет центроидов ячеек сетки\nGRID &lt;- calcCentroid(GRID, rollup = 3)\n\n# -----------------------------------------------------------\n# 5. ПОСТРОЕНИЕ ВЫПУКЛОЙ ОБОЛОЧКИ (CONVEX HULL) ДЛЯ ДАННЫХ\n# -----------------------------------------------------------\n\n# Создание выпуклой оболочки вокруг точек данных\nHull &lt;- inla.nonconvex.hull(cbind(data$xkm, data$ykm), convex = -0.03)\n\n# Визуализация оболочки \n plot(Hull)\n\n# Визуализация оболочки и точек съемок 2019-2024\npoints(data$xkm, data$ykm, pch=1, cex=0.55,col=\"black\")\n\n\n# Фильтрация сетки: оставляем только точки внутри оболочки\nline &lt;- Hull$loc[, 1:2] %&gt;% as.data.frame()\ncolnames(line) &lt;- c(\"X\", \"Y\")\nGRID$AREA &lt;- point.in.polygon(GRID$X, GRID$Y, line$X, line$Y)\nGRID &lt;- GRID[GRID$AREA &gt; 0.1, c(\"X\", \"Y\")]  # Только внутренние точки\n\n# -------------------------------------------------\n# 6. ПОДГОТОВКА СЕТКИ ДЛЯ ПРОГНОЗИРОВАНИЯ\n# -------------------------------------------------\n\n# Создание временной сетки (для каждого года)\ngrid &lt;- replicate_df(GRID, \"YEAR\", unique(data$YEAR))\ncolnames(grid) &lt;- c(\"xkm\", \"ykm\", \"YEAR\")\ngrid$SURV &lt;- \"CRAB\"  # Добавляем информацию о типе съемки\n\n# Визуализация оболочки и сетки для прогнозирования (grid}\n plot(Hull)\n points(grid$xkm, grid$ykm, pch=1, cex=0.55,col=\"black\")\n\n# ---------------------------------------------------\n# 7. ПОСТРОЕНИЕ ПРОСТРАНСТВЕННОЙ СЕТКИ (MESH)\n# ---------------------------------------------------\n\n# Создание треугольной сетки для пространственного моделирования\nmesh_sdm &lt;- make_mesh(\n  data, \n  c(\"xkm\", \"ykm\"),  # Координаты\n  cutoff = 10        # Минимальное расстояние между узлами (км)\n)\n\n# Визуализация сетки (раскомментируйте)\n plot(mesh_sdm)\n\n# ---------------------------------------------------\n# 8. ПОСТРОЕНИЕ ПРОСТРАНСТВЕННО-ВРЕМЕННОЙ МОДЕЛИ\n# ---------------------------------------------------\n\nm &lt;- sdmTMB(\n  data = data, \n  formula = Density ~ 0 + as.factor(YEAR),  # Формула: плотность зависит от года\n  time = \"YEAR\",         # Временная переменная\n  mesh = mesh_sdm,       # Пространственная сетка\n  family = tweedie(link = \"log\"),  # Статистическое распределение\n  spatial = \"on\",        # Включение пространственных эффектов\n  spatiotemporal = \"iid\" # Пространственно-временные эффекты\n)\n\n\n# Вывод результатов модели\nsummary(m)\nAIC(m)  # Критерий Акаике\nsanity(m)  # Проверка корректности модели\n\n# ---------------------------------------------------\n# 9. ДИАГНОСТИКА МОДЕЛИ\n# ---------------------------------------------------\n\n# Расчет остатков модели\ndata$resids &lt;- residuals(m) \n\n# Гистограмма остатков\nhist(data$resids)\n\n# График квантиль-квантиль\nqqnorm(data$resids)\nabline(a = 0, b = 1)\n\n# ---------------------------------------------------\n# 10. ПРОГНОЗИРОВАНИЕ НА СЕТКЕ\n# ---------------------------------------------------\n\n# Прогноз значений плотности на сетке\npredictions &lt;- predict(m, newdata = grid, return_tmb_object = TRUE)\nRASP &lt;- predictions$data\n\n# Преобразование координат обратно в широту/долготу\nRASP$xkm_m &lt;- RASP$xkm * 1000  # Обратно в метры\nRASP$ykm_m &lt;- RASP$ykm * 1000\n\n# Создание пространственного объекта в UTM\nutm_proj &lt;- CRS(\"+proj=utm +zone=37 +datum=WGS84 +units=m +no_defs\")\ncoords &lt;- cbind(RASP$xkm_m, RASP$ykm_m)\nsp_points &lt;- SpatialPoints(coords, proj4string = utm_proj)\n\n# Преобразование в WGS84 (широта/долгота)\nwgs84_proj &lt;- CRS(\"+proj=longlat +datum=WGS84\")\nsp_points_latlon &lt;- spTransform(sp_points, wgs84_proj)\n\n# Добавление координат в основной датафрейм\nRASP$X &lt;- coordinates(sp_points_latlon)[, 1]  # Долгота\nRASP$Y &lt;- coordinates(sp_points_latlon)[, 2]  # Широта\n\n# Удаление временных столбцов\nRASP$xkm_m &lt;- NULL\nRASP$ykm_m &lt;- NULL\n\n# Проверка структуры результата\nstr(RASP)\n\n# ---------------------------------------------\n# 11. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ (КАРТА)\n# ---------------------------------------------\n\n# Загрузка картографических данных\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Определение региона интереса (Арктика России)\narctic_bbox &lt;- st_bbox(c(xmin = 25, xmax = 70, ymin = 65, ymax = 80), crs = 4326)\narctic &lt;- st_crop(world, arctic_bbox)\n\n# Определяем кастомные breaks для шкалы\nmy_breaks &lt;- c(0.001,0.1,1,  200, 10000)\n\n# Создаем категории для PROM\ndata &lt;- data %&gt;%\n  mutate(\n    PROM_cat = case_when(\n      PROM == 0 ~ \"0\",\n      PROM &gt;= 1 & PROM &lt; 10 ~ \"1-9\",\n      PROM &gt;= 10 & PROM &lt; 100 ~ \"10-99\",\n      PROM &gt;= 100 ~ \"100+\"\n    ),\n    PROM_cat = factor(PROM_cat, levels = c(\"0\", \"1-9\", \"10-99\", \"100+\")),\n    shape_cat = ifelse(PROM_cat == \"0\", \"zero\", \"non_zero\")\n  )\n\n# Обновляем график\nggplot() +\n  geom_point(\n    data = RASP, \n    aes(x = X, y = Y, color = exp(est)), \n    size = 0.8, \n    alpha = 0.7\n  ) + \n  geom_point(\n    data = data, \n    aes(x = X, y = Y, size = PROM_cat, shape = shape_cat),\n    color = \"black\", \n    fill = NA, \n    alpha = 0.6\n  ) +\n  scale_size_manual(\n    name = NULL,\n    values = c(\"0\" = 1, \"1-9\" = 2, \"10-99\" = 3),\n    labels = c(\"0\", \"10\", \"100\")\n  ) +\n  scale_shape_manual(\n    values = c(\"zero\" = 4, \"non_zero\" = 21),\n    guide = \"none\" # Скрываем легенду для формы\n  ) +\n  guides(\n    size = guide_legend(\n      override.aes = list(shape = c(4, 21, 21)) # Крестик только для первого элемента\n    )\n  ) +\n  geom_sf(data = arctic, fill = \"lightgrey\", color = \"darkgrey\") +\n  scale_color_viridis_c(\n    name = NULL,\n    option = \"H\", \n    trans = \"log\", \n    breaks = my_breaks, \n    labels = my_breaks, \n    limits = range(my_breaks),\n    guide = guide_colorbar(\n      barwidth = unit(5, \"cm\"),\n      title.position = \"top\",\n      direction = \"horizontal\"\n    )\n  ) +\n  facet_wrap(~ YEAR, ncol = 2) +\n  coord_sf(\n    xlim = c(min(RASP$X)-1, max(RASP$X)+1),\n    ylim = c(min(RASP$Y)-0.5, max(RASP$Y)+0.5),\n    crs = 4326\n  ) +\n  theme_bw(base_size = 12) +\n  labs(x = NULL, y = NULL) +\n  theme(\n    panel.grid = element_line(color = \"grey90\"),\n    legend.position = \"bottom\",\n    legend.key.width = unit(1.2, \"cm\"),\n    strip.background = element_rect(fill = \"white\")\n  )\n\n\n# Сохранение графика (раскомментируйте)\n# ggsave(\"sdmTMBmapZero.jpg\", width = 8, height = 8, dpi = 300)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>sdmTMB - оценка и визуализация индекса обилия по съемке</span>"
    ]
  },
  {
    "objectID": "chapter 4.html#определение-площади-съемки",
    "href": "chapter 4.html#определение-площади-съемки",
    "title": "5  sdmTMB - оценка и визуализация индекса обилия по съемке",
    "section": "5.6 Определение площади съемки",
    "text": "5.6 Определение площади съемки\n\n# ---------------------------\n# 1. ПОДГОТОВКА СРЕДЫ И ДАННЫХ\n# ---------------------------\n\n# Очистка рабочей среды\nrm(list = ls())\n\n# Установка рабочей директории (замените на свою)\nsetwd(\"C:/COMBINE/\")\n\n# Загрузка необходимых пакетов\nlibrary(readxl)       # Для чтения Excel-файлов\nlibrary(PBSmapping)   # Для работы с пространственными данными\nlibrary(sdmTMB)       # Пространственно-временное моделирование\nlibrary(INLA)         # Продвинутые пространственные модели\n\n# Загрузка данных из Excel-файла\ndata &lt;- readxl::read_excel(\"KARTOGRAPHIC.xlsx\", sheet = \"SURVEY\")\n\n# Просмотр структуры данных\nstr(data)\n\n\n# --------------------------------------------------\n# 2. ПРЕОБРАЗОВАНИЕ КООРДИНАТ В ПРОЕКЦИЮ UTM (в км)\n# --------------------------------------------------\n\n# Создание пространственного объекта из данных\ndata_sf &lt;- st_as_sf(\n  data, \n  coords = c(\"X\", \"Y\"), # Указание столбцов с координатами\n  crs = 4326            # Система координат WGS84 (широта/долгота)\n) \n\n# Преобразование в UTM зону 37N (метры)\ndata_utm &lt;- st_transform(data_sf, crs = 32637) \n\n# Извлечение координат и перевод в километры\nutm_coords &lt;- st_coordinates(data_utm)\ndata$xkm &lt;- utm_coords[, 1] / 1000  # X в км\ndata$ykm &lt;- utm_coords[, 2] / 1000  # Y в км\n\n# Очистка временных объектов\nrm(data_sf, data_utm, utm_coords)\n\n# -----------------------------------------\n# 3. ОПРЕДЕЛЕНИЕ ГРАНИЦ ИССЛЕДОВАНИЯ\n# -----------------------------------------\n\n# Вычисление границ исследовательского полигона\nxl &lt;- c(min(data$xkm), max(data$xkm))  # Границы по X\nyl &lt;- c(min(data$ykm), max(data$ykm))  # Границы по Y\n\n# ----------------------------------------\n# 4. СОЗДАНИЕ РАСТРОВОЙ СЕТКИ ДЛЯ МОДЕЛИ\n# ----------------------------------------\n\n# Создание равномерной сетки с шагом 10 км (для визуализации карты использовался шаг 2 км\nGRID &lt;- makeGrid(\n  x = seq(xl[1], xl[2], 10), \n  y = seq(yl[1], yl[2], 10),\n  byrow = FALSE,\n  projection = \"UTM\", \n  zone = 37\n)\n\n# Расчет центроидов ячеек сетки\nGRID &lt;- calcCentroid(GRID, rollup = 3)\n\n# -----------------------------------------------------------\n# 5. ПОСТРОЕНИЕ ВЫПУКЛОЙ ОБОЛОЧКИ (CONVEX HULL) ДЛЯ ДАННЫХ\n# -----------------------------------------------------------\n\n# Создание выпуклой оболочки вокруг точек данных\nHull &lt;- inla.nonconvex.hull(cbind(data$xkm, data$ykm), convex = -0.03)\n\n# Визуализация оболочки \n plot(Hull)\n\n# Преобразование Hull в объект PolySet:\npolys &lt;- data.frame(\n  PID = rep(1, nrow(Hull$loc)), # ID полигона\n  POS = 1:nrow(Hull$loc),       # Порядок точек\n  X = Hull$loc[, 1],            # Координата X (в км)\n  Y = Hull$loc[, 2]             # Координата Y (в км)\n)\npolys &lt;- PBSmapping::as.PolySet(polys, projection = \"UTM\", zone = 37)\n\n# Расчет площади:\narea &lt;- PBSmapping::calcArea(polys)\nprint(paste(\"Площадь Hull:\", round(area$area, 2), \"км кв.\"))\n\n[1] “Площадь Hull: 283947.5 км кв.”",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>sdmTMB - оценка и визуализация индекса обилия по съемке</span>"
    ]
  },
  {
    "objectID": "chapter 5.html",
    "href": "chapter 5.html",
    "title": "6  Продукционная модель SPiCT",
    "section": "",
    "text": "6.1 Введение\nЭто практическое занятие — приглашение работать с неопределённостью честно и профессионально. Мы будем оценивать запас промыслового вида с помощью SPiCT — стохастической продукционной модели в байесовской постановке. В терминах Роберта Сапольского: модель — это способ дисциплинировать интуицию, а не подменить реальность. В терминах Даниэля Канемана: мы осознанно переводим себя из «быстрой» Системы 1 (интуиции и красивых историй) в «медленную» Систему 2 (проверяемые допущения, приоры, диагностика, сценарии). И в терминах Нассима Талеба: мы работаем в мире «толстых хвостов» и чёрных лебедей; поэтому модели должны быть не только точными «в среднем», но и устойчивыми к сюрпризам.\nЗачем SPiCT и почему именно такой подход. SPiCT реализует стохастическую продукционную динамику (обобщение Шефера/Пеллы–Томлинсона) и оценивает параметры через TMB в байесовской парадигме. Это важно по трём причинам. Во‑первых, реальный улов и индексы шумные, с нулями, с передисперсией; стохастическая модель учитывает процессные и наблюдательные ошибки, а не прячет их в остатки. Во‑вторых, байесовские прайеры (или приоры) — не «псевдонаучные пожелания», а протокол того, что мы готовы считать правдоподобным до данных: n≈2 (Шефер), разумный диапазон K, начальная доля B/K. Прайеры делают оценку стабильной при слабых данных и позволяют явно «рассказать» модели, что мы знаем из биологии. В‑третьих, пакет построен вокруг воспроизводимой диагностики: OSA‑остатки, автокорреляция, сравнение априор/апостериор, ретроспектива (коэффициент Мона), сценарии управления — это инструменты, уменьшающие риск когнитивных ловушек.\nПро честные допущения и минимализм. «Простые» объяснения предпочтительнее при прочих равных, пока они работают. Мы не перегружаем модель лишними степенями свободы: фиксируем n=2, задаём информативные прайеры на K и начальную долю B/K, аккуратно масштабируем неопределённость последних лет (чтобы не делать «свежие» данные богами), используем малый шаг интегрирования dteuler для корректной динамики. Ричард Докинз сказал бы: эволюция — это про ограничения и компромиссы; продукционная модель — тоже. Она не объясняет всё, но хорошо решает задачу «сколько можем брать и оставаться в зелёной зоне» при ограниченной информации.\nКак держать под контролем наши ошибки мышления. Харари метко писал о силе нарративов: мозг любит «истории с концом», даже если данных мало. Мы противопоставляем этому протокол. Сначала — валидируем входные ряды (catch и индексы, лаги и кросс‑корреляции), затем — формируем единый вход SPiCT с явным календарём измерений (timeI и obsI), задаём прайеры и повышаем неопределённость там, где это честно (последние годы). Дальше — диагностика: OSA‑остатки без смещения и избыточной автокорреляции, нормальность на QQ‑плотах, сравнение априора и апостериора (данные «говорят», или всё держится на прайере?), корреляции параметров (типичная антагония K и q — не баг, а свойство задачи), ретроспектива Мона (устойчивость оценок к добавлению новых лет). Это «канемановская» дисциплина: мы строим защитные барьеры от своей уверенности.\nПро риск и «толстые хвосты». Талеб напомнил бы, что средние — коварны. Даже если B/BMSY&gt;1 и F/FMSY&lt;1, управленческие правила должны учитывать ширину доверительных интервалов, а не только точку. Поэтому мы интерпретируем не одну кривую, а пучок сценариев: «держать текущий вылов», «держать текущий F», «ловить на FMSY», «снизить/увеличить F», «хоккейная клюшка», «ICES‑правило», фиксированные квоты. И для каждого — не только прогноз B и F, но и риск превышения FMSY и ухода ниже BMSY. Хорошая рекомендация — это баланс «надёжности» и «пользы»: консервативный вылов, сохраняющий B/BMSY&gt;1.2 при низкой вероятности перелова, часто выигрывает у агрессивных схем, которые «в среднем» немного выгоднее, но делают систему хрупкой.\nКак читать результаты и не обмануться. Сходимость (convergence=0) и финитные стандартные ошибки — допуск к интерпретации. OSA‑диагностика без смещения и лишней автокорреляции — индикатор адекватности структуры ошибок. Разумный K (с учётом прайера), r в биологическом диапазоне, q1–q2 сопоставимые для CPUE/BESS — признак «реалистичности». Если апостериор на K почти совпадает с прайером — не беда, это честный сигнал: данных мало, рекомендацию стоит «страховать» широкой лентой и консервативным сценарием. Если ковариации параметров велики — не прячем, а подчёркиваем в выводах. Если ретроспектива показывает |ρ| близко к нулю — модель устойчива; если нет — упрощаем, усиливаем приоры, перепроверяем данные.\nПро «матчасть» и воспроизводимость. SPiCT — это не только fit.spict(), а экосистема справок (?check.inp, ?fit.spict), кратких обзоров и живого «технического» руководства. Мы сохраняем код, версии пакетов, начальные значения и приоры внутри скрипта — так, чтобы завтра любой исследователь смог повторить наши оценки. Это и есть антихрупкость : система, которая выигрывает от проверок и критики. И это способ строить доверие в сообществе — не за счёт красноречия, а за счёт прозрачности.\nЧто вы освоите по итогам занятия. 1) Подготовку входов: единая шкала времени, лаги индексов, кросс‑корреляции. 2) Настройку прайеров и их роль при ограниченных данных. 3) Запуск и диагностику модели: от конвергенции до OSA и ретроспективы. 4) Чтение ключевых ориентиров (MSY, BMSY, FMSY, B/BMSY, F/FMSY) и их неопределённости. 5) Формирование и интерпретацию сценариев управления с учётом риска, а не только «лучшей точки». 6) Коммуникацию результатов для управленцев: одна картинка «Kobe plot», одна таблица сценариев, одна короткая формулировка рекомендации с оговорками и предпосылками.\nИ главное — стиль мышления. Мы будем вспоминать Сапольского, когда захочется превратить «красивую» картинку в факт; Канемана — когда рука потянется «довернуть» модель до нужного ответа; Талеба — когда нужно выбирать между «чуть больше сейчас» и «устойчиво много лет»; Хокинга — когда стоит убрать лишнюю сложность; Докинза — когда интерпретируем параметры через процессы; Харари — когда формируем честный и открытый нарратив о том, что модель знает и чего не знает. Такой образ действий превращает SPiCT из «чёрного ящика» в дисциплину: воспроизводимый, устойчивый и полезный для принятия решений анализ.\nИ так, библиотека SPiCT https://github.com/DTUAqua/spict - оценка запаса с помощью стохастической версии продукционной модели и байесовского подхода. Доступен краткий обзор пакета здесь, который служит для ознакомления с пакетом и его функционалом. В обзоре также содержится описание более продвинутых функций пакета.\nДокумент с техническими рекомендациями по использованию SPiCT доступен здесь . Это постоянно обновляемый документ.\nПакет также содержит достаточно подробную документацию в виде справочных текстов, связанных с каждой функцией (некоторые из них могут быть не полностью актуальны). Доступ к ним можно получить обычным для R способом, набрав, например ?check.inp, . Для начала (помимо изучения краткого обзора) рекомендуется прочитать ?check.inpи ?fit.spict.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#установка-пакетов-и-загрузка-данных",
    "href": "chapter 5.html#установка-пакетов-и-загрузка-данных",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.2 Установка пакетов и Загрузка данных",
    "text": "6.2 Установка пакетов и Загрузка данных\n\n# ------------------------- 1. ПОДГОТОВКА СРЕДЫ ---------------------------\n\n## 1.1 Установка пакетов (выполнить один раз)\n# install.packages(\"tidyverse\")\n#remotes::install_github(\"DTUAqua/spict/spict\") # Установка SPiCT\n#install.packages(\"TMB\", type=\"source\")\n\n## 1.2 Загрузка библиотек\nlibrary(spict)   # Основной пакет для моделирования\nlibrary(tidyverse) # Для обработки данных и визуализации\n\n\n## 1.3 Установка рабочей директории\nsetwd(\"C:/SPICT\") # Укажите вашу рабочую папку\n\n\n# ------------------------- 2. ЗАГРУЗКА ДАННЫХ ---------------------------\n\n## 2.1 Вектор лет наблюдений\nYear &lt;- 2005:2024\n\n## 2.2 Данные по вылову (тыс. тонн)\nCatch &lt;- c(5,  7,  6, 10, 14, 25, 28, 30, 32, 35, 25, 20, 15, 12, 10, 12, 10, 13, 11, 12)\n\n## 2.3 Индекс CPUE (промысловый индекс)\nCPUEIndex &lt;- c(27.427120, 26.775958, 16.811997, 22.979653, 29.048568, 29.996072, 16.476301,\n17.174455, 10.537272, 14.590435,  8.286352, 11.394168, 15.537878, 13.791166,\n11.527548, 15.336093, 12.154069, 15.568450, 16.221933, 13.421132)\n\n## 2.4 Индекс BESS (научная съемка)\nBESSIndex &lt;- c( NA, 16.270375, 20.691355, 15.141784, 18.594620, 15.975548, 13.792012,\n13.328805, 11.659744, 11.753855,  9.309859,  7.104886,  7.963839,  9.161322,\n10.271221,  9.822960, 10.347376, 11.703610, 13.679876, 13.413696)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#кросс-корреляции-с-временным-лагом-между-индексами-и-уловами",
    "href": "chapter 5.html#кросс-корреляции-с-временным-лагом-между-индексами-и-уловами",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.3 Кросс-корреляции с временным лагом между индексами и уловами",
    "text": "6.3 Кросс-корреляции с временным лагом между индексами и уловами\n\n# График кросс-корреляции: Catch и BESSindex (только данные без пропусков)\nccf(na.omit(Catch), na.omit(BESSIndex),\n    main = \"Кросс-корреляция: Уловы и BESSindex\",\n    xlab = \"Лаг (годы)\", ylab = \"Корреляция\")\n\n\n# График кросс-корреляции: Catch и CPUEIndex (только данные без пропусков)\nccf(na.omit(Catch), na.omit(CPUEIndex),\n    main = \"Кросс-корреляция: Уловы и CPUEIndex\",\n    xlab = \"Лаг (годы)\", ylab = \"Корреляция\")\n\n\n\n\nРис. 1.: График кросс-корреляции: Catch и BESSindex\n\n\n\n\n\nРис. 2.: График кросс-корреляции: Catch и CPUEIndex",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#подготовка-данных-для-spict",
    "href": "chapter 5.html#подготовка-данных-для-spict",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.4 Подготовка данных для SPiCT",
    "text": "6.4 Подготовка данных для SPiCT\n\n## 3.1 Форматирование данных в список SPiCT\ninput_new &lt;- list(\n  timeC = Year,     # Годы вылова\n  obsC = Catch,     # Значения вылова\n  timeI = list(     # Временные точки для индексов:\n    Year + 0.5,     #  CPUE - середина года (июль)\n    Year + 0.75     #  BESS - 3/4 года (октябрь)\n  ),\n  obsI = list(\n    CPUEIndex,      # Значения индекса CPUE\n    BESSIndex       # Значения индекса BESS\n  )\n)\n\n## 3.2 Проверка и подготовка входных данных\ninp &lt;- check.inp(input_new, verbose = TRUE)\n\nФункция check.inp() выполняет подготовку и валидацию входных данных перед моделированием. Основные задачи функции:\nПроверяет наличие обязательных элементов: timeC (временные точки вылова), obsC (значения вылова). Проверяет соответствие индексов (timeI и obsI): одинаковое количество элементов в списках, соответствие длин временных рядов. Обработка пропущенных значений. Автоматически обрабатывает NA в начале вектора CPUEIndex (2005-2010). Удаляет или маркирует пропущенные значения в соответствии с настройками пакета и пр.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#настройка-модели",
    "href": "chapter 5.html#настройка-модели",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.5 Настройка модели",
    "text": "6.5 Настройка модели\n\n# ------------------- 4. НАСТРОЙКА МОДЕЛИ --------------------\n\n## 4.1 Установка априорных распределений\ninp$priors$logn &lt;- c(log(2), 0.1, 1)   # Прайер для n (модель Шефера)\ninp$ini$logn &lt;- log(2)                  # Начальное значение\ninp$phases$logn &lt;- -1                   # Фиксируем параметр (не оцениваем)\n\ninp$priors$logK &lt;- c(5, 0.7, 1)       # Прайер для емкости среды (K)\ninp$priors$logbkfrac &lt;- c(log(0.75),0.25,1) # Начальный уровень эксплуатации\n\n\n\n## 4.2 Настройка неопределенности данных\n# Повышаем неопределенность для последнего года вылова\ninp$stdevfacC[length(inp$stdevfacC)] &lt;- 2 \n\n# Повышаем неопределенность для последнего значения BESS\ninp$stdevfacI[[2]][length(inp$stdevfacI[[2]])] &lt;- 2 \n\n## 4.3 Настройка временного шага\ninp$dteuler &lt;- 1/16  # Более точная дискретизация (по умолчанию 1)\n\n## 4.4 Включение оценки ковариации\ninp$getJointPrecision &lt;- TRUE # Для оценки случайных эффектов\n\nУстановка априорных распределений\ninp$priors$logn &lt;- c(log(2), 0.1, 1) inp$ini$logn &lt;- log(2) inp$phases$logn &lt;- -1\n\nПрайер для параметра n: Устанавливаем лог-нормальное распределение для экспоненты в продукционном уравнении, фиксируя модель Шефера (n=2).\nНачальное значение: Задаем стартовую точку для оптимизации как log(2), что соответствует n=2.\nФиксация параметра: Флаг -1 исключает n из оценки, делая его константой (упрощает модель).\nБиологический смысл: Обеспечивает реалистичную форму кривой производства (параболическую).\n\ninp$priors$logK &lt;- c(5, 0.7, 1)\n\nПрайер для ёмкости среды: Задаем лог-нормальное распределение для K.\nПараметры: Медиана exp(5)≈148 тыс.т, SD=0.7 в лог-шкале.\nЗначение 1: Активирует использование прайера в расчетах.\nНазначение: Отражает экспертные знания о возможном диапазоне K.\n\ninp$priors$logbkfrac &lt;- c(log(0.75),0.25,1)\n\nПриор для начальной биомассы: Определяет распределение для B₀/K.\nПараметры: Медиана 0.75 (начальная биомасса 75% от K), SD=0.25.\nБиологический смысл: Отражает гипотезу, что запас изначально был близок к неиспользуемому.\nВажность: Помогает оценить начальные условия при ограниченных данных.\n\nНастройка неопределенности данных\ninp$stdevfacC[length(inp$stdevfacC)] &lt;- 2\n\nЦель: Увеличить неопределенность последнего года вылова.\nЗначение 2: Стандартная ошибка увеличивается вдвое.\nПричина: Последние данные часто предварительные или неполные.\nЭффект: Снижает влияние потенциально ненадежной точки на оценку запаса.\n\ninp$stdevfacI[[2]][length(inp$stdevfacI[[2]])] &lt;- 2\n\nЦель: Повысить неопределенность последнего значения научного индекса (BESS).\nСинтаксис: [[2]] указывает на второй индекс в списке.\nОбоснование: Данные съемок могут требовать последующих корректировок.\nРезультат: Модель становится менее чувствительной к потенциальным аномалиям.\n\nНастройка временного шага\ninp$dteuler &lt;- 1/16\n\nЦель: Улучшить точность численного интегрирования.\nЗначение: Шаг расчета ≈23 дня (вместо годового).\nНеобходимость: Для короткоживущих видов (н-р, креветка) с быстрой динамикой.\nЭффект: Точнее учитывает внутригодовые изменения и сезонность.\nЦена: Увеличивает время расчета в 3-5 раз.\n\nВключение оценки ковариации\ninp$getJointPrecision &lt;- TRUE\n\nЦель: Рассчитать полную ковариационную матрицу параметров.\nНеобходимость: Для корректной оценки неопределенности производных показателей (B/BMSY).\nЧто делает: Учитывает взаимосвязи между параметрами и скрытыми состояниями.\nПреимущество: Более реалистичные доверительные интервалы.\nОграничение: Увеличивает время расчета на 20-30%.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#визуализация-входных-данных",
    "href": "chapter 5.html#визуализация-входных-данных",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.6 Визуализация входных данных",
    "text": "6.6 Визуализация входных данных\n\n# ----------------- 5. ВИЗУАЛИЗАЦИЯ ВХОДНЫХ ДАННЫХ -----------------\n\n## 5.1 Общий график данных\nplotspict.data(inp)\n\nplotspict.ci(inp)\n\n\n\n\nРис. 3.: Визуализация входных данных",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#запуск-модели",
    "href": "chapter 5.html#запуск-модели",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.7 Запуск модели",
    "text": "6.7 Запуск модели\n\n# ------------------- 6. ЗАПУСК МОДЕЛИ --------------------\n\n## 6.1 Настройка оптимизатора\ninp$optimiser.control = list(iter.max = 1e5, eval.max = 1e5)\n\n## 6.2 Подгонка модели\nfit &lt;- fit.spict(inp)\n\n## 6.3 Добавление OSA-остатков\nfit &lt;- calc.osa.resid(fit) \n\nНастройка оптимизатора\ninp$optimiser.control = list(iter.max = 1e5, eval.max = 1e5)\nЧто это делает:\n\nУвеличивает максимальное количество итераций (iter.max) и вычислений (eval.max) для алгоритма оптимизации до 100,000\nОбеспечивает, что процесс оптимизации не остановится преждевременно из-за ограничений по умолчанию\nОсобенно важно для сложных моделей с несколькими индексами или при плохой сходимости\nПомогает алгоритму найти глобальный минимум функции правдоподобия\nПредотвращает ошибки типа “maximum iterations reached”\n\nПодгонка модели\nfit &lt;- fit.spict(inp)\nЧто происходит:\n\nОценка параметров: Ищет значения параметров (K, r, q и др.), максимизирующие правдоподобие\nИнтегрирование уравнений: Решает дифференциальные уравнения модели с шагом dteuler\nРасчет неопределенности: Оценивает стандартные ошибки через обратную матрицу Гессе\nДиагностика сходимости: Проверяет успешность оптимизации (fit$opt$convergence)\nСохраняет результаты: Формирует объект fit со всеми выходами модели\n\nКлючевые процессы:\n\nЧисленная оптимизация (обычно алгоритм nlminb)\nИнтегрирование методом Эйлера\nРасчет логарифмического правдоподобия\nОценка матрицы Гессе\n\nOSA-остатки\nfit &lt;- calc.osa.resid(fit)\nЧто такое OSA-остатки:\n\nOne-Step-Ahead residuals - остатки “на один шаг вперед”\nДиагностический инструмент: Показывают, насколько хорошо модель предсказывает следующее наблюдение\nРасчет: Для каждого года t модель подгоняется по данным до t-1, затем сравнивается предсказание с фактическим значением в t\n\nЧто делают:\n\nОбнаружение систематических ошибок: Выявляют смещения в предсказаниях\nПроверка независимости: Автокорреляция в остатках указывает на неучтенные зависимости\nОценка распределения ошибок: Проверяют соответствие нормальному распределению\nИдентификация выбросов: Помогают найти аномальные точки данных",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#диагностика-сходимости",
    "href": "chapter 5.html#диагностика-сходимости",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.8 Диагностика сходимости",
    "text": "6.8 Диагностика сходимости\n\n# ----------------- 7. ДИАГНОСТИКА СХОДИМОСТИ -----------------\n\n## 7.1 Проверка сходимости\nfit$opt$convergence  # 0 = успешная сходимость\n\n## 7.2 Проверка конечных значений\nall(is.finite(fit$sd)) # TRUE = все параметры конечны\n\nДиагностика сходимости модели SPiCT\nПроверка сходимости (7.1)\nfit$opt$convergence — индикатор успешности оптимизации. Возвращаемое значение 0 означает, что алгоритм оптимизации успешно сошелся к точке максимума правдоподобия. Это важно, так как гарантирует:\n\nПараметры модели достигли стабильных значений\nГрадиент функции правдоподобия близок к нулю\nРезультаты статистически надежны\nМодель готова для дальнейшего анализа и прогнозирования\n\nИнтерпретация кодов:\n\n0: Успешная сходимость (идеальный результат)\n1: Достигнут лимит итераций (требует увеличения iter.max)\n10: Дегенерация симплекса (проблемы с данными)\nДругие коды указывают на специфические ошибки оптимизации\n\nПроверка конечных значений (7.2)\nall(is.finite(fit$sd)) — комплексная проверка корректности оценок неопределенности. Результат TRUE означает:\n\nВсе стандартные ошибки параметров являются вещественными числами\nОтсутствуют патологические значения (NaN, Inf, NA) в матрице Гессе\nКовариационная матрица положительно определена\nОценки неопределенности надежны для построения доверительных интервалов\n\nЧто проверяет:\n\nКорректность расчета стандартных ошибок\nОтсутствие вырожденных параметров\nЧисленную стабильность решения\nВозможность интерпретировать результаты\n\nПоследствия FALSE:\n\nНевозможно построить достоверные доверительные интервалы\nРиск ошибочных управленческих рекомендаций\nТребуется пересмотр модели (упрощение, изменение прайеров)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#диагностика-модели",
    "href": "chapter 5.html#диагностика-модели",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.9 Диагностика модели",
    "text": "6.9 Диагностика модели\n\n# ----------------- 8. ДИАГНОСТИКА МОДЕЛИ -----------------\n\n## 8.1 График остатков\nplotspict.osar(fit) \n\n## 8.2 Общая диагностика\nplotspict.diagnostic(fit)\n\n## 8.3 Сравнение приоров и апостериорных распределений\nplotspict.priors(fit)\n\n## 8.4 Проверка корреляции параметров\ncov2cor(fit$cov.fixed)        # Матрица корреляций\ncov2cor(fit$cov.fixed) &gt; 0.8  # Выявление сильных корреляций (&gt;0.8)\n\n\n\n\nРис. 4.: График остатков\n\n\nОстатки по времени (Residuals vs. time):\n-   Показывает разницу между наблюдаемыми значениями и предсказаниями модели\n\n-   Идеал: случайное распределение вокруг нуля\n\n    ### **Что означает \"Index 1 Bias p-val 0.7813\"?**\n\n    Это результат **статистического теста на систематическую ошибку** (bias) для первого индекса (в вашем случае - CPUE):\n\n    1.  **Index 1**: Это ваш индекс CPUE (первый в списке индексов)\n\n    2.  **Bias p-val**: p-value теста на наличие систематического смещения\n\n    3.  **0.7813**: Конкретное значение p-value\n\n    ### **Интерпретация значения 0.7813:**\n\n    -   **p-value \\&gt; 0.05**: Нет статистически значимых доказательств систематической ошибки (смещения)\n\n    -   **Высокое значение (0.7813)**: Сильно выше 0.05 → модель **не имеет значимого смещения** для этого индекса\n\n    -   **Практический смысл**: Модель адекватно описывает динамику CPUE без постоянного завышения или занижения\n\n\n\nРис. 5.: Общая диагностика\n\n\nСтруктура диагностического графика\n\nПервый столбец: Информация, связанная с данными по вылову (catch).\nВторой и третий столбцы: Информация, связанная с данными по индексам биомассы.\n\nСтроки графика содержат (сверху вниз):\n\nЛогарифмы исходных рядов данных:\n\nВерхний левый график в строке: log catch data (Логарифм данных по вылову).\nСрелний и правый графики в строке: log index data (Логарифм данных по индексу).\nЦель: Визуально оценить исходные данные.\n\nOSA-остатки:\n\nГрафик показывает разницу между наблюдаемыми и предсказанными на один шаг вперед значениями (в логарифмической шкале).\nЗаголовок графика содержит p-значение теста на смещение (bias test). Этот тест проверяет, отличается ли среднее остатков от нуля (систематическая ошибка).\n\nBias p-val: X.XXXX (p-значение теста на смещение).\nЗеленый заголовок: Тест НЕ значим (нет свидетельств систематической ошибки, p &gt; 0.05).\nКрасный заголовок: Тест значим (есть свидетельства систематической ошибки, p &lt;= 0.05).\n\nТри теста незначимы (p=0.4386 для вылова, p=0.7813 и p=0.9472 для индексов), заголовки зеленые.\n\nЭмпирическая автокорреляция остатков (ACF - Autocorrelation Function):\n\nГрафик показывает корреляцию остатков с их собственными лагированными значениями.\nВыполняется два теста на значимую автокорреляцию:\n\nТест Льюнга-Бокса (Ljung-Box test): Одновременный тест для нескольких лагов (здесь 4). Результат:\n\nLBox p-val: X.XXXX в заголовке графика.\nНа примере: Три теста незначимы (p=0.1348 для вылова, p=0.68 и p=0.3602 для индексов).\n\nТесты для отдельных лагов: Пунктирные горизонтальные линии на графике показывают критические значения для значимой автокорреляции на каждом конкретном лаге. Если столбики автокорреляции (вертикальные линии) выходят за эти пунктирные линии, это свидетельствует о значимой автокорреляции на данном лаге.\n\nНа примере : Никаких нарушений (значимой автокорреляции) не выявлено.\n\nТесты на нормальность остатков:\n\nQQ-график (Quantile-Quantile plot): Сравнивает квантили остатков с квантилями теоретического нормального распределения. Прямая линия указывает на нормальность.\nТест Шапиро-Уилка (Shapiro-Wilk test): Формальный тест на нормальность. Результат:\n\nShapiro p-val: X.XXXX в заголовке графика.\nНа примере: Три теста незначимы (p=0.1268 для вылова, p=0.9554 и p=0.9825 для индекса), нет свидетельств против нормальности.\n\n\n\nВывод для примера :\nДанные в этом примере не показали значимых нарушений предположений модели (нет систематической ошибки, автокорреляции или отклонения от нормальности остатков). Это повышает уверенность в полученных результатах моделирования.\nДля обсуждения возможных нарушений и способов их устранения читатель отсылается к Pedersen and Berg (2017) см. https://github.com/DTUAqua/spict/raw/master/spict/inst/doc/spict_handbook.pdf.\n\n\n\nРис. 6.: Сравнение априорных и апостериорных распределений\n\n\nn (параметр формы продукционной функции)\n\nОписание: Определяет форму продукционной кривой (зависимость роста биомассы от самой биомассы).\nИнтерпретация:\n\nn = 2: Классическая модель Шефера (симметричная кривая, максимум производства при B/K = 0.5).\nn ≠ 2: Обобщенная модель Пеллы-Томлинсона (асимметричная кривая).\n\nВажность: влияет на оценку Bmsy (биомасса при MSY) и статус запаса (B/Bmsy).\n\nalpha1, alpha2 (Параметры соотношения шумов для индексов)\n\nОписание: Логарифмы отношений стандартных отклонений ошибок наблюдения индексов (sdi1, sdi2) к стандартному отклонению процесса биомассы (sdb):\nalpha1 = log(sdi1) - log(sdb)\nalpha2 = log(sdi2) - log(sdb)\nИнтерпретация:\n\nОтражают относительную точность каждого индекса биомассы по сравнению с изменчивостью самой биомассы.\nalpha = 0 (sdi = sdb): Шум индекса равен шуму биомассы.\nalpha &lt; 0 (sdi &lt; sdb): Индекс точнее, чем изменчивость биомассы (хорошо).\nalpha &gt; 0 (sdi &gt; sdb): Индекс шумнее, чем изменчивость биомассы (плохо).\n\nКонтекст: Появляются, только если в модели используется два или более индексов биомассы.\n\nbeta (Параметр соотношения шумов для уловов)\n\nОписание: Логарифм отношения стандартного отклонения ошибок наблюдения уловов (sdc) к стандартному отклонению процесса промысловой смертности (sdf):\nbeta = log(sdc) - log(sdf)\nИнтерпретация:\n\nОтражает относительную точность данных по уловам по сравнению с изменчивостью промысловой смертности.\nbeta = 0 (sdc = sdf): Шум уловов равен шуму F.\nbeta &lt; 0 (sdc &lt; sdf): Данные по уловам точнее, чем изменчивость F (хорошо).\nbeta &gt; 0 (sdc &gt; sdf): Данные по уловам шумнее, чем изменчивость F (плохо).\n\n\nK (емкость среды)\n\nОписание: Максимальная равновесная биомасса неэксплуатируемого запаса (carrying capacity).\nИнтерпретация: Верхняя асимптота кривой роста. Один из самых важных и часто трудных для оценки параметров, особенно при ограниченных данных.\nЕдиницы измерения: Те же, что и у биомассы (например, тонны, тыс. особей).\n\n\n6.9.1 bkfrac (Начальная биомасса)\n\nОписание: Доля от K, которую составляла биомасса запаса в начальный год временного ряда:\nbkfrac = B₀ / K\nИнтерпретация:\n\nbkfrac = 1: Запас был в нетронутом состоянии в начальный год (B₀ = K).\nbkfrac &lt; 1: Запас уже был эксплуатируемым к началу ряда данных.\n\nВажность: Сильно влияет на реконструкцию исторической динамики биомассы, особенно если данные начинаются с периода интенсивного промысла.\n\nПочему именно эти параметры?\nФункция plotspict.priors(fit) по умолчанию фокусируется на параметрах, для которых:\n\nЗаданы явные априорные распределения пользователем (как logK или bkfrac в примерах).\nПрименены стандартные полу-информативные априоры SPiCT для стабилизации оценки в условиях ограниченных данных. К ним относятся n, alpha1, alpha2, beta. SPiCT использует их, так как эти параметры (особенно n и соотношения шумов) часто плохо определяются только данными улова и индекса.\n\nСравнение априора и апостериора показывает:\n\nНасколько данные обновили наши первоначальные представления (априорные) о параметре.\nНасколько информативны были априорные распределения.\nНадежность оценки: Сильное сужение апостериорного распределения относительно априорного говорит о том, что данные содержат информацию о параметре. Если апостериорное распределение почти совпадает с априорным, данные не добавили новой информации (оценка держится на априорном распределение (прайере)).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#проверка-корреляции-параметров",
    "href": "chapter 5.html#проверка-корреляции-параметров",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.10 Проверка корреляции параметров",
    "text": "6.10 Проверка корреляции параметров\n\n## 8.4 Проверка корреляции параметров\n&gt; cov2cor(fit$cov.fixed)        # Матрица корреляций\n              logm        logK        logq        logq      logsdb      logsdf\nlogm    1.00000000 -0.44139706  0.24731406  0.26680092  0.10226103  0.05866772\nlogK   -0.44139706  1.00000000 -0.78966591 -0.84564098 -0.14502822  0.03159055\nlogq    0.24731406 -0.78966591  1.00000000  0.92073272  0.09936208 -0.06019367\nlogq    0.26680092 -0.84564098  0.92073272  1.00000000  0.10661957 -0.06022707\nlogsdb  0.10226103 -0.14502822  0.09936208  0.10661957  1.00000000 -0.07548411\nlogsdf  0.05866772  0.03159055 -0.06019367 -0.06022707 -0.07548411  1.00000000\nlogsdi  0.04194204 -0.06650572  0.12991301  0.13206072  0.05802810 -0.02826486\nlogsdi -0.02144997  0.05063718 -0.08833322 -0.09060172  0.04142955  0.00688377\nlogsdc -0.03142644 -0.10430756  0.07611708  0.07988704  0.19380037 -0.38345908\n            logsdi       logsdi       logsdc\nlogm    0.04194204 -0.021449974 -0.031426440\nlogK   -0.06650572  0.050637182 -0.104307558\nlogq    0.12991301 -0.088333221  0.076117083\nlogq    0.13206072 -0.090601717  0.079887038\nlogsdb  0.05802810  0.041429548  0.193800371\nlogsdf -0.02826486  0.006883770 -0.383459078\nlogsdi  1.00000000 -0.024282304  0.029024203\nlogsdi -0.02428230  1.000000000 -0.005068705\nlogsdc  0.02902420 -0.005068705  1.000000000\n&gt; cov2cor(fit$cov.fixed) &gt; 0.8  # Выявление сильных корреляций (&gt;0.8)\n        logm  logK  logq  logq logsdb logsdf logsdi logsdi logsdc\nlogm    TRUE FALSE FALSE FALSE  FALSE  FALSE  FALSE  FALSE  FALSE\nlogK   FALSE  TRUE FALSE FALSE  FALSE  FALSE  FALSE  FALSE  FALSE\nlogq   FALSE FALSE  TRUE  TRUE  FALSE  FALSE  FALSE  FALSE  FALSE\nlogq   FALSE FALSE  TRUE  TRUE  FALSE  FALSE  FALSE  FALSE  FALSE\nlogsdb FALSE FALSE FALSE FALSE   TRUE  FALSE  FALSE  FALSE  FALSE\nlogsdf FALSE FALSE FALSE FALSE  FALSE   TRUE  FALSE  FALSE  FALSE\nlogsdi FALSE FALSE FALSE FALSE  FALSE  FALSE   TRUE  FALSE  FALSE\nlogsdi FALSE FALSE FALSE FALSE  FALSE  FALSE  FALSE   TRUE  FALSE\nlogsdc FALSE FALSE FALSE FALSE  FALSE  FALSE  FALSE  FALSE   TRUE\n&gt; \n\nАнализ матрицы корреляций параметров модели SPiCT\nКоманды выполняют два действия:\n\ncov2cor(fit$cov.fixed) - преобразует матрицу ковариаций в матрицу корреляций\ncov2cor(fit$cov.fixed) &gt; 0.8 - выявляет сильные корреляции (&gt;0.8)\n\nУмеренные корреляции (|r| &gt; 0.7):\n\nlogK и logq (-0.79):\nКлассическая отрицательная корреляция между емкостью среды и уловистостью. Означает, что:\n\nДанные можно объяснить либо:\n\nБольшим запасом (высокий K) с низкой уловистостью (низкий q)\nИли малым запасом (низкий K) с высокой уловистостью (высокий q)\n\nТипично для моделей с ограниченными данными",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#визуализация-резульататов",
    "href": "chapter 5.html#визуализация-резульататов",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.11 Визуализация резульататов",
    "text": "6.11 Визуализация резульататов\n\n----------------- 9. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ -----------------\n\n## 9.1 Основные графики\n\nplot(fit) \\# Комплексный отчет\n\n## 9.2 Биомасса в абсолютных величинах\n\nplotspict.biomass( fit, logax = FALSE, \\# Линейная шкала main = \"Абсолютная биомасса\", ylim = c(0, 250), \\# Ограничение по оси Y plot.obs = TRUE, \\# Отображать наблюдения xlab = \"Год\", CI = 0.95, \\# 95% доверительный интервал qlegend = FALSE, rel.axes = TRUE, rel.ci = TRUE )\n\n## 9.3 Относительная биомасса (B/Bmsy)\n\nplotspict.bbmsy(fit,qlegend = FALSE)\n\n## 9.4 Вылов\n\nplotspict.catch(fit,qlegend = FALSE)\n\n## 9.5 Относительная смертность (F/Fmsy)\n\nplotspict.ffmsy(fit,qlegend = FALSE)\n\n## 9.6 Продукционная кривая\n\nplotspict.production(fit)\n\n## 9.7 Kobe plot\n\nplotspict.fb(fit, ylim=c(0, 0.5), xlim=c(0, 200))\n\n\n\n\nРис. 7.: Комплексный отчет\n\n\n\n\n\nРис. 8.: Динамика абсолютной биомассы\n\n\n\n\n\nРис. 9.: Динамика относительной биомассы\n\n\n\n\n\nРис. 10.: Динамика относительной смертности\n\n\n\n\n\nРис. 11.: Продукционная кривая\n\n\n\n\n\nРис. 12.: Kobe plot\n\n\nГрафик показывает динамику биомассы и смертности от промысла с начального года (здесь 2005), обозначенного кругом, до конечного года (здесь 2025), обозначенного квадратом. Жёлтый ромб обозначает среднюю биомассу за длительный период при сохранении текущей (2025) промысловой нагрузки. Эта точка может быть интерпретирована как равновесное значение вылова и обозначена в легенде как E(B∞) как статистический способ выражения ожидания биомассы при t → ∞. Поскольку текущая промысловая смертность близка к FMSY, ожидаемая долгосрочная биомасса близка к BMSY. Серая затенённая область в форме банана обозначает 95% доверительную область пары FMSY, BMSY. Эту область важно визуализировать совместно, поскольку две контрольные точки имеют сильную (отрицательную) корреляцию.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#анализ-результатов",
    "href": "chapter 5.html#анализ-результатов",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.12 Анализ результатов",
    "text": "6.12 Анализ результатов\n\n# ----------------- 10. АНАЛИЗ РЕЗУЛЬТАТОВ -----------------\n\n## 10.1 Краткий отчет\nsummary(fit)\n\n## 10.2 Точечные оценки параметров\npars &lt;- sumspict.parest(fit)\n\n## 10.3 Ориентиры управления (стохастические)\nsumspict.srefpoints(fit)\n\n## 10.4 Ориентиры управления (детерминированные)\nsumspict.drefpoints(fit)\n\nКраткий отчет\n\nmary(fit)\nConvergence: 0  MSG: relative convergence (4)\nObjective function at optimum: -4.8303552\nEuler time step (years):  1/16 or 0.0625\nNobs C: 20,  Nobs I1: 20,  Nobs I2: 19\n\nResidual diagnostics (p-values)\n    shapiro   bias    acf   LBox shapiro bias acf LBox  \n C   0.1268 0.4386 0.0562 0.1348       -    -   .    -  \n I1  0.9554 0.7813 0.3360 0.6800       -    -   -    -  \n I2  0.9825 0.9472 0.1390 0.3602       -    -   -    -  \n\nPriors\n      logn  ~  dnorm[log(2), 0.1^2]\n  logalpha  ~  dnorm[log(1), 2^2]\n   logbeta  ~  dnorm[log(1), 2^2]\n      logK  ~  dnorm[log(148.413), 0.7^2]\n logbkfrac  ~  dnorm[log(0.75), 0.25^2]\n\nFixed parameters\n   fixed.value  \n n           2  \n\nModel parameter estimates w 95% CI \n           estimate       cilow       ciupp    log.est  \n alpha1  11.9450241   2.6794895  53.2502930  2.4803148  \n alpha2   5.0683878   1.1265510  22.8028330  1.6230228  \n beta     0.1879597   0.0371609   0.9507000 -1.6715278  \n r        0.3769549   0.2895415   0.4907586 -0.9756298  \n rc       0.3769549   0.2895415   0.4907586 -0.9756298  \n rold     0.3769549   0.2895415   0.4907586 -0.9756298  \n m       17.8600178  16.2681570  19.6076444  2.8825646  \n K      189.5188972 153.7796069 233.5642100  5.2444887  \n q1       0.1446167   0.1110579   0.1883161 -1.9336685  \n q2       0.1105024   0.0861636   0.1417164 -2.2027177  \n sdb      0.0179239   0.0040818   0.0787065 -4.0216194  \n sdf      0.3205493   0.2181678   0.4709762 -1.1377191  \n sdi1     0.2141016   0.1563268   0.2932285 -1.5413046  \n sdi2     0.0908454   0.0648365   0.1272875 -2.3985967  \n sdc      0.0602503   0.0143610   0.2527760 -2.8092469  \n \nDeterministic reference points (Drp)\n         estimate      cilow       ciupp   log.est  \n Bmsyd 94.7594486 76.8898035 116.7821050  4.551342  \n Fmsyd  0.1884774  0.1447707   0.2453793 -1.668777  \n MSYd  17.8600178 16.2681570  19.6076444  2.882565  \nStochastic reference points (Srp)\n        estimate      cilow       ciupp   log.est  rel.diff.Drp  \n Bmsys 94.710229 76.8402135 116.7361074  4.550822 -0.0005196907  \n Fmsys  0.188398  0.1447201   0.2452583 -1.669199 -0.0004216341  \n MSYs  17.843213 16.2542730  19.5874793  2.881623 -0.0009418271  \n\nStates w 95% CI (inp$msytype: s)\n                   estimate      cilow       ciupp    log.est  \n B_2024.94      118.8335053 96.9318694 145.6837887  4.7777234  \n F_2024.94        0.1031939  0.0646828   0.1646340 -2.2711455  \n B_2024.94/Bmsy   1.2547061  1.0845329   1.4515812  0.2269014  \n F_2024.94/Fmsy   0.5477442  0.3417896   0.8778024 -0.6019469  \n\nPredictions w 95% CI (inp$msytype: s)\n                 prediction       cilow       ciupp    log.est  \n B_2026.00      123.0873098 100.3197792 151.0219217  4.8128939  \n F_2026.00        0.1031941   0.0464383   0.2293155 -2.2711437  \n B_2026.00/Bmsy   1.2996200   1.1150104   1.5147950  0.2620719  \n F_2026.00/Fmsy   0.5477452   0.2458405   1.2204040 -0.6019451  \n Catch_2025.00   12.4909882   7.3033391  21.3634865  2.5250074  \n E(B_inf)       137.4532783          NA          NA  4.9232841  \n&gt; \n\nАнализ результатов модели SPiCT\n1. Сходимость модели\nConvergence: 0 MSG: relative convergence (4)\nИнтерпретация: Код 0 указывает на успешную сходимость оптимизации. Сообщение “relative convergence” подтверждает, что алгоритм достиг локального минимума с заданной точностью. Результаты могут считаться валидными.\n2. Целевая функция\nObjective function at optimum: -4.8303552\nИнтерпретация: Значение логарифмической апостериорной плотности (с учетом априорных распределений) в точке оптимума. Более высокие значения (менее отрицательные) указывают на лучшее соответствие модели данным.\n3. Дискретизация времени\nEuler time step (years): 1/16 or 0.0625\nИнтерпретация: Для решения дифференциальных уравнений использован шаг Эйлера 0.0625 года (~23 дня), что обеспечивает высокую точность расчетов.\n4. Данные наблюдений\nNobs C: 20,  Nobs I1: 20,  Nobs I2: 19\nИнтерпретация:\n\nC: 20 точек данных по вылову (2005-2024 гг.)\nI1: 20 значений индекса CPUE\nI2: 19 значений индекса BESS (отсутствует первое наблюдение)\n\n5. Диагностика остатков\nResidual diagnostics (p-values)\nshapiro   bias    acf   LBox\nC   0.1268 0.4386 0.0562 0.1348\nI1  0.9554 0.7813 0.3360 0.6800\nI2  0.9825 0.9472 0.1390 0.3602  \nКлючевые тесты:\n\nShapiro-Wilk: Нормальность остатков (p &gt; 0.05 → нормальность не отвергается)\nBias test: Систематическая ошибка (p &gt; 0.05 → смещение отсутствует)\nACF/Ljung-Box: Автокорреляция (p &lt; 0.1 для вылова → слабая автокорреляция)\nЗаключение: Остатки удовлетворительны, кроме возможной слабой автокорреляции в данных по вылову.\n\n6. Априорные распределения\nPriors\nlogn  ~  dnorm[log(2), 0.1^2]       # Фиксирован n = 2 (модель Шефера)   \nlogK  ~  dnorm[log(148.413), 0.7^2]  # K ~ 148.4 тыс. тонн (CV=70%)   \nlogbkfrac ~ dnorm[log(0.75), 0.25^2] # Начальная эксплуатация B/K = 0.75\nИнтерпретация: Использованы информативные априорные распределения для ключевых параметров, что характерно для data-limited подходов.\n7. Оценки параметров модели\nModel parameter estimates w 95% CI\n      estimate       cilow       ciupp  \nK      189.5 [153.8 - 233.6]  # Емкость среды (тыс. тонн)  \nr        0.38 [0.29 - 0.49]   # Внутренняя скорость роста \nq1       0.14 [0.11 - 0.19]   # Catchability CPUE \nq2       0.11 [0.09 - 0.14]   # Catchability BESS  \nsdf      0.32 [0.22 - 0.47]   # SD процесса для F\nКлючевые выводы: - Высокая неопределенность оценки K (дов. интервал ±40%) - Умеренная скорость восстановления запаса (r ≈ 38% в год) - Индекс BESS имеет более высокую catchability, чем CPUE\n8. Ориентиры управления\nDeterministic reference points (Drp)\nestimate      95% CI  \nBmsyd   94.8 [76.9 - 116.8]  # Биомасса при MSY  \nFmsyd    0.19 [0.14 - 0.25]  # Смертность при MSY  \nMSYd    17.9 [16.3 - 19.6]   # Макс. устойчивый вылов  \n\nStochastic reference points (Srp)          \nestimate      rel.diff.Drp    \nBmsys   94.7         -0.05%        # Незначительные отличия от детерм. модели  \nFmsys    0.19        -0.04%\nИнтерпретация: Результаты устойчивы к стохастичности модели. MSY ≈ 18 тыс. тонн.\n9. Состояние запаса в 2024 г.\nStates w 95% CI (inp$msytype: s)                   \nestimate      95% CI  \nB_2024.94        118.8 [96.9 - 145.7]  # Абсолютная биомасса (тыс. т) \nF_2024.94          0.10 [0.06 - 0.16]  # Смертность \nB/Bmsy             1.25 [1.08 - 1.45]  # Биомасса выше Bmsy\nF/Fmsy             0.55 [0.34 - 0.88]  # Эксплуатация ниже Fmsy\nОценка состояния: Запас находится в благополучном состоянии (B &gt; Bmsy, F &lt; Fmsy), но с высокой неопределенностью.\n10. Прогнозы\nPredictions w 95% CI\nB_2026.00      123.1 [100.3 - 151.0]  # Прогноз биомассы\nCatch_2025.00   12.5 [7.3 - 21.4]     # Прогноз вылова на 2025 г.  \nE(B_inf)       137.5                   # Ожидаемая равновесная биомасса\nПрогнозные показатели: - Биомасса продолжит умеренный рост - Рекомендуемый вылов на 2025 г. ≈ 12.5 тыс. тонн (дов. интервал ±57%) - Потенциальная равновесная биомасса на 16% выше текущей\nКлючевые выводы:\n\nМодель успешно сошлась с удовлетворительными остатками\nЗапас оценивается выше целевого уровня (B/Bmsy &gt; 1)\nЭксплуатация находится на безопасном уровне (F/Fmsy &lt; 1)\nРекомендуемый вылов на 2025 г. — 12.5 [7.3 - 21.4] тыс. тонн\nОсновные источники неопределенности: оценка K и прогноз вылова",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#ретроспективный-анализ",
    "href": "chapter 5.html#ретроспективный-анализ",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.13 Ретроспективный анализ",
    "text": "6.13 Ретроспективный анализ\n\n# -------------- 11. РЕТРОСПЕКТИВНЫЙ АНАЛИЗ --------------\n\n## 11.1 Запуск ретроспективного анализа\nfit &lt;- retro(fit)\n\n## 11.2 Визуализация ретроспективы\nplotspict.retro(fit, add.mohn = TRUE, CI = 0.95)\n\n## Интерпретация коэффициента Мона (Mohn's rho):\n## Долгоживущие виды: |rho| &gt; 0.2 значимо\n## Короткоживущие виды: |rho| &gt; 0.3 значимо\n\n\n\n\nРис. 13.: Визуализация ретроспективного анализа\n\n\nСуть ретроспективного анализа (Retrospective Analysis)\nЦель: Оценка устойчивости модели и выявление систематических смещений (ретроспективного сдвига) в оценках состояния запаса при добавлении новых данных.\nМетод:\n\nМодель последовательно переоценивается с исключением по 1 последнему году данных (например: 2005-2023, 2005-2022 и т.д.)\nДля каждого урезанного периода рассчитываются показатели (B/Bmsy, F/Fmsy) в перекрывающиеся годы\nОценки сравниваются с “базовой” моделью (со всеми данными)\n\nКоэффициент Мона (Mohn’s rho)\nФормула расчета:\nρ = 1/N * Σ [ (X_retro,i - X_base,i) / X_base,i ]\nгде:\n\nN – число исключенных лет\nX_retro,i – оценка параметра (напр. B/Bmsy) в году i по урезанным данным\nX_base,i – оценка того же параметра в году i по полным данным\n\nИнтерпретация результатов в вашем случае:\n        FFmsy         BBmsy   0.0028358361 -0.0002021046 \n\nДля F/Fmsy: ρ = 0.0028 (0.28%)\n\nПоложительное значение: текущие оценки F/Fmsy слегка завышены по сравнению с ретроспективой\nВеличина &lt; 0.3% – незначима\n\nДля B/Bmsy: ρ = -0.0002 (-0.02%)\n\nОтрицательное значение: текущие оценки B/Bmsy слегка занижены\nВеличина &lt; 0.1% – пренебрежимо мала\n\n\nВывод для модели:\n\nКоэффициенты Мона близки к нулю (|ρ| &lt; 0.005)\nОтсутствует статистически значимый ретроспективный сдвиг\nМодель демонстрирует высокую устойчивость к добавлению новых данных\nРезультаты можно считать надежными\n\n\nВажно! Значимый сдвиг (|ρ| &gt; 0.2-0.3) указывает на:\n\nНедостаточность данных\nПроблемы со спецификацией модели\nСистематические ошибки в данных\nНеобходимость пересмотра модели",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 5.html#прогнозирование-и-сценарии-управления",
    "href": "chapter 5.html#прогнозирование-и-сценарии-управления",
    "title": "6  Продукционная модель SPiCT",
    "section": "6.14 Прогнозирование и сценарии управления",
    "text": "6.14 Прогнозирование и сценарии управления\n\n# ----------- 12. ПРОГНОЗИРОВАНИЕ И СЦЕНАРИИ УПРАВЛЕНИЯ -----------\n\n## 12.1 Установка интервала управления\ninp$maninterval &lt;- c(2025, 2026) # Годы прогноза\n\n## 12.2 Базовые сценарии управления\nfit &lt;- manage(fit)\n\n## 12.3 Пользовательские сценарии (постоянный вылов)\ncatchvals = c(10, 12, 15, 17) # Варианты вылова в тыс.тонн\n\nfor(i in seq_along(catchvals)){\n  fit &lt;- add.man.scenario(\n    fit,\n    scenarioTitle = paste0(\"Постоянный вылов \", catchvals[i], \" тыс.т\"),\n    cabs = catchvals[i]  # Абсолютный вылов\n  )\n}\n\n## 12.4 Сводка по сценариям управления\nsumspict.manage(fit, include.unc = TRUE) # С учетом неопределенности\n\nПолучаем:\n\nSPiCT timeline:\n                                                  \n      Observations              Management        \n    2005.00 - 2025.00        2025.00 - 2026.00    \n |-----------------------| ----------------------|\n\nManagement evaluation: 2026.00\n\nPredicted catch for management period and states at management evaluation time:\n\n                                 C B/Bmsy F/Fmsy\n1. Keep current catch         11.8   1.31   0.52\n2. Keep current F             12.5   1.30   0.55\n3. Fish at Fmsy               22.0   1.20   1.00\n4. No fishing                  0.0   1.42   0.00\n5. Reduce F by 25%             9.5   1.33   0.41\n6. Increase F by 25%          15.4   1.27   0.68\n7. MSY hockey-stick rule      22.0   1.20   1.00\n8. ICES advice rule           19.9   1.23   0.90\n9. Постоянный вылов 10 тыс.т  10.0   1.32   0.43\n10. Постоянный вылов 12 тыс.т 12.0   1.30   0.53\n11. Постоянный вылов 15 тыс.т 15.0   1.27   0.66\n12. Постоянный вылов 17 тыс.т 17.0   1.25   0.76\n\n95% confidence intervals for states:\n\n                              B/Bmsy.lo B/Bmsy.hi F/Fmsy.lo F/Fmsy.hi\n1. Keep current catch              1.12      1.52      0.23      1.15\n2. Keep current F                  1.12      1.51      0.25      1.22\n3. Fish at Fmsy                    1.00      1.44      0.45      2.23\n4. No fishing                      1.25      1.63      0.00      0.00\n5. Reduce F by 25%                 1.15      1.54      0.18      0.92\n6. Increase F by 25%               1.08      1.49      0.31      1.53\n7. MSY hockey-stick rule           1.00      1.44      0.45      2.23\n8. ICES advice rule                1.03      1.46      0.40      2.00\n9. Постоянный вылов 10 тыс.т       1.14      1.54      0.19      0.97\n10. Постоянный вылов 12 тыс.т      1.12      1.52      0.24      1.17\n11. Постоянный вылов 15 тыс.т      1.09      1.49      0.30      1.48\n12. Постоянный вылов 17 тыс.т      1.06      1.48      0.34      1.69\n\n\n\n\nРис. 14.: Правило хоккейной клюшки и ICES\n\n\nПроцесс прогнозирования и оценки сценариев управления\n1. Установка горизонта прогнозирования\ninp$maninterval &lt;- c(2025, 2026)\n\nЦель: Определить период, для которого делаются прогнозы (2025-2026 гг.)\nМеханика: Модель будет рассчитывать состояние запаса и возможный вылов на эти годы\n\n2. Базовые сценарии управления\nfit &lt;- manage(fit)\nАвтоматически генерируются стандартные сценарии:\n\ncurrentCatch: Сохранение текущего вылова (среднее за последние 3 года)\ncurrentF: Сохранение текущего уровня смертности (F)\nFmsy: Эксплуатация на уровне FMSY\nnoF: Полное прекращение промысла\nreduceF25: Снижение F на 25%\nincreaseF25: Увеличение F на 25%\nmsyHockeyStick: Правило “хоккейной клюшки” (F=0 при B&gt;BMSY, F=FMSY при B≥BMSY)\nices: Правило ICES (F пропорционален уровню биомассы)\n\n3. Пользовательские сценарии\ncatchvals = c(10, 12, 15, 17) for(i in seq_along(catchvals)){   fit &lt;- add.man.scenario(     fit,     scenarioTitle = paste0(\"Постоянный вылов \", catchvals[i], \" тыс.т\"),     cabs = catchvals[i]   ) }\n\nСтратегия: Фиксированный вылов указанного объема в 2025-2026 гг.\nДиапазон: От консервативного (10 тыс.т) до рискованного (17 тыс.т)\n\n4. Сводка результатов\nsumspict.manage(fit, include.unc = TRUE)\nКлючевые выводы из результатов (на 2026 г.)\n1. Прогноз состояния запаса\n\n\n\n\n\n\n\n\nПоказатель\nЗначение\nИнтерпретация\n\n\n\n\nB/BMSY\n1.25-1.31\nЗапас выше целевого уровня (B&gt;BMSY)\n\n\nF/FMSY\n0.52-0.55\nЭксплуатация ниже предельной (F&lt;FMSY)\n\n\n\n2. Сравнение сценариев\n\n\n\n\n\n\n\n\n\n\nСценарий\nВылов (тыс.т)\nB/BMSY\nF/FMSY\nРиск перелова\n\n\n\n\nБезопасные:\n\n\n\n\n\n\nnoF (нет промысла)\n0.0\n1.42\n0.00\nНет\n\n\nreduceF25\n9.5\n1.33\n0.41\nНизкий\n\n\nВылов 10 тыс.т\n10.0\n1.32\n0.43\nНизкий\n\n\nОптимальные:\n\n\n\n\n\n\ncurrentCatch\n11.8\n1.31\n0.52\nНизкий\n\n\nВылов 12 тыс.т\n12.0\n1.30\n0.53\nНизкий\n\n\nРискованные:\n\n\n\n\n\n\nincreaseF25\n15.4\n1.27\n0.68\nУмеренный\n\n\nВылов 15 тыс.т\n15.0\n1.27\n0.66\nУмеренный\n\n\nОпасные:\n\n\n\n\n\n\nFMSY\n22.0\n1.20\n1.00\nВысокий\n\n\nВылов 17 тыс.т\n17.0\n1.25\n0.76\nВысокий\n\n\n\n3. Анализ неопределенности\nДля ключевых сценариев:\n\nВылов 12 тыс.т:\nF/FMSY&gt; = 0.53 [0.24-1.17] → 10% вероятность превышения FMSY\nВылов 15 тыс.т:\nF/FMSY = 0.66 [0.30-1.48] → 30% вероятность превышения FMSY\nВылов 17 тыс.т:\nF/FMSY = 0.76 [0.34-1.69] → 45% вероятность превышения FMSY\n\nРекомендации по управлению\n\nОптимальный вылов: 12 тыс. тонн\n\nСохраняет запас в безопасной зоне (B/BMSY &gt; 1.3)\nМинимизирует риск перелова (F/FMSY &lt; 0.55)\nУчитывает неопределенность модельных оценок\n\nПредельно допустимый вылов: 15 тыс. тонн\n\nТребует усиленного мониторинга\nНеобходим ежегодный пересмотр квот\n\nНе рекомендуются:\n\nСценарии с F≥FMSY (22 тыс.т)\nФиксированный вылов &gt;15 тыс.т\nСтратегии, приводящие к снижению B/BMSY &lt; 1.25\n\n\n\nКритический фактор: Высокая неопределенность прогноза вылова (дов. интервал 7.3-21.4 тыс.т для текущего сценария) требует осторожного подхода.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Продукционная модель SPiCT</span>"
    ]
  },
  {
    "objectID": "chapter 6.html",
    "href": "chapter 6.html",
    "title": "7  Продукционная модель JABBA",
    "section": "",
    "text": "7.1 Введение\nНачнём с ловушки. Когда у нас в руках есть гладкое ОДУ про запас — что‑нибудь вроде dB/dt = r·B·(1 − B/K) − C(t) — страшно хочется почувствовать рычаг управления: подберём r и K, оценим текущую B, поставим правильный C(t) и поведём систему по траектории к MSY. Иллюзия контроля рождается там, где математика выглядит законченной, а данные — нет. В промысловой реальности индексы обилия шумны, выборка смещена, «уловистость» q дрейфует, а режимы среды меняются быстрее, чем сходятся наши апостериоры. Модель даёт ощущение руля, но часть океана — это шторм, который рулит нами. Даниэль Канеман сказал бы, что в такие моменты «Система 1» радостно дорисовывает уверенность, а «Система 2» обязана включить тормоза и проверить допущения, чувствительность и ретроспективную устойчивость. Биологические системы редко ведут себя «по учебнику», и именно поэтому дисциплина проверки важнее изящества формулы.\nВ этом занятии мы работаем с JABBA — байесовской стохастической продукционной моделью, близкой по философии к SPiCT. Оба инструмента из одной семьи: они ставят биологическую динамику в основу, честно разделяют процессные и наблюдательные ошибки, и главное — выводят всю неопределённость в явный вид. Мы будем строить оценку из уловов и пары индексов (CPUE и BESS), задавать слабые прайеры, запускать MCMC, и не верить числам до тех пор, пока не проверим сходимость, остатки, ретроспективу (Mohn’s ρ) и предсказательную способность на «срезах» (MASE). В духе Нассима Талеба примем, что толстые хвосты и чёрные лебеди — не метафора, а свойство данных: редкие годы с аномальными условиями и сбоями наблюдений способны «перевернуть стол» оценок. В духе Стивена Пинкера не будем впадать в цинизм: рациональность, систематический сбор данных и прозрачные модели — это всё ещё лучший путь к более разумным решениям; просто оптимизм должен быть «проверенным на бордюре» ошибок и интервалов.\nЗачем именно JABBA в курсе. Во‑первых, реплицируемость: код, входные таблицы, фиксированная директория результатов — вы сможете восстановить каждый шаг и проверить каждое предположение. Во‑вторых, явная работа с неопределённостью: априоры на r, K, ψ и q, апостериоры параметров и траекторий, доверительные интервалы для B/BMSY и F/FMSY, вероятностные выводы, а не одинокие «оценки». В‑третьих, диагностика «по умолчанию»: от Geweke/Heidel до фазовых графиков Кобэ, от остатков до process deviations. В‑четвёртых, прогноз — не как гадание, а как веер сценариев с аккуратным учётом параметрической и процессной вариабельности. Хорошая модель — это карта, удобная для навигации, а не фотография местности. Мы будем постоянно помнить, что наши карты полезны ровно настолько, насколько мы отслеживаем их масштаб, погрешность и зоны плохой видимости.\nС методологической стороны всё просто и сложно одновременно. Просто — потому что базовая биология логистична: при малой биомассе прирост почти пропорционален B, близко к K — прирост затухает; вылов вычитает «сверху». Сложно — потому что r и K плохо идентифицируются без информативных периодов (высоких B или резких спадов), q плавает между флотами и годами, а процессная ошибка σ² смешивается с наблюдательной τ². Байесовский подход помогает не только «усреднить» неопределённость, но и сделать её объектом управления: мы можем принимать решения, которые устойчивы к диапазону правдоподобных миров, а не к одному единственному. Мы не ищем «замысел» в данных, мы подбираем механизм, который лучше других воспроизводит наблюдаемую адаптивную динамику, и признаём эволюционную торгуемость параметров — их компромисс между точностью и устойчивостью.\nУправление запасами — это всегда история, которую общество рассказывает себе о будущем: «мы извлечём столько‑то, и запас останется устойчивым». Математическая часть истории — необходима, но недостаточна; у неё есть герои (B/BMSY и F/FMSY), есть антагонисты (перелов, неучтённый вылов, сдвиги среды), и есть мораль: если вы не тестируете собственный нарратив ретроспективой и внешней валидацией, он превращается в миф. Поэтому в этом практикуме мы делаем упор на три вещи. Во‑первых, читаем таблицы параметров сквозь призму сходимости: PPMR и PPVR для многомерной стабильности, Geweke/Heidel для цепей. Во‑вторых, различаем «точность» и «калибровку»: модель может хорошо ранжировать годы (низкий MASE), но давать завышенную амплитуду (остатки и process deviations подскажут, где именно). В‑третьих, смотрим на устойчивость решений: Mohn’s ρ близок к нулю — хорошо; «веер» ретроспективы не расползается — ещё лучше; прогноз под реалистичными сценариями не пересекает опасные квадранты Кобэ — то, что нужно для рекомендаций.\nВажное предупреждение про иллюзию контроля. JABBA и SPiCT не волшебные палочки, а инструменты, которые позволяют количественно выразить вашу неопределённость. Если прайеры на r чрезмерно широки, индексы плохо калиброваны по q, а периоды высокой биомассы отсутствуют, «красивые» графики всё равно останутся красивыми — но управленческая уверенность будет мнимой. Здесь полезна привычка думать об антикхрупкости: формулируйте решения, которые переживут «плохие» годы без катастрофы — например, коридоры вылова, адаптивные пороги, ежегодную перекалибровку модели с новыми данными, стресс‑тесты при r на нижней границе ДИ. И полезна доля оптимизма: последовательное накопление данных (хороших, прозрачных, воспроизводимых) сделает модель лучше — и это не вера, а эмпирический факт в дисциплинах, где стандарты учёта повышались.\nНаконец, почему мы показываем на одном материале JABBA и проводим параллели со SPiCT. Потому что полезно видеть одну и ту же задачу через родственные, но не идентичные лупы: отличия в реализации процессной ошибки, спецификации наблюдательных дисперсий, в настроечных «рычагах» MCMC и диагностике. Консенсус между инструментами — это не гарантия истины, но хорошая проверка на то, что вы не «подогрели» результат особенностями одной конкретной реализации. Если же инструменты расходятся — это повод вернуться к данным: к шкалам, пропускам, структурным разрывам, к возможной нестационарности q и к тем самым чёрным лебедям, которые любят объявляться в самый неудобный момент.\nС этим набором интеллектуальной «защиты от иллюзий» мы и идём дальше: загрузим уловы и индексы, зададим прайеры, соберём модель, проверим, как она дышит на диагностике, посмотрим ретроспективу и только после этого — аккуратно поговорим про прогнозы. Не потому, что модели плохи, а потому что океан велик, а мы — скромны и внимательны. Именно так появляются решения, которые приносят пользу в реальном управлении, а не только на красивых слайдах.\nИ так, библиотека JABBA https://github.com/jabbamodel/JABBA - оценка запаса с помощью стохастической версии продукционной модели и байесовского подхода. JABBA и SPiCT – наиболее распространенные в международной практике инструменты, реализующие продукционный подход к оценке запасов гидробионтов при нехватки данных.\nПомимо R JABBA требует дополнительно установки JAGS. Для демонстрации основных функций пакета в настоящем скрипте используются входные данные из примера SPiCT, поэтому в скрипте демонстрации модели JABBA сценарий моделирования назван “SPiCT_adapted”.\nСкрипт этого практического занятия можно скачать по ссылке.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Продукционная модель JABBA</span>"
    ]
  },
  {
    "objectID": "chapter 6.html#подготовка-среды-и-загрузка-данных",
    "href": "chapter 6.html#подготовка-среды-и-загрузка-данных",
    "title": "7  Продукционная модель JABBA",
    "section": "7.2 Подготовка среды и загрузка данных",
    "text": "7.2 Подготовка среды и загрузка данных\nВ данном разделе выполняется базовая настройка среды R для работы с пакетом JABBA. Инициируется загрузка двух необходимых пакетов: JABBA (основной инструмент оценки запасов) и reshape2 (для преобразования структур данных). Создается целевая директория “NEW JABBA” для автоматического сохранения всех результатов анализа, после чего рабочая среда переключается на эту папку.\nФормируются три обязательных компонента входных данных:\n\nДанные по вылову (catch): Годовые значения уловов за 20-летний период (2005-2024 гг.), представленные в виде вектора из 20 числовых значений.\nДва индекса обилия:\n\n\nCPUE (улов на единицу усилия): 20 наблюдений\nBESS (альтернативный индекс, например, данные съемок): 19 наблюдений (первое значение отсутствует, обозначено как NA)\n\n\nСтандартные ошибки (SE) для индексов: Для упрощения примера задаются фиксированным коэффициентом вариации (CV=20%). Это означает, что для каждого ненулевого значения индекса SE рассчитывается как 20% от его величины. Пропуски в индексах (NA) автоматически сохраняются как NA в таблице SE.\n\nВсе данные структурируются в три таблицы с единой временной осью (2005-2024 гг.): отдельно для уловов, значений индексов и их стандартных ошибок. Эта подготовка обеспечивает корректный формат входных данных, необходимых для последующего построения продукционной модели в JABBA.\n\n# ------------------------- 1. ПОДГОТОВКА СРЕДЫ ---------------------------\n\n# Загрузка необходимых пакетов\nlibrary(JABBA) # Основной пакет для оценки запасов\nlibrary(reshape2) # Для преобразования данных (функция dcast)\n\n# Установка рабочей директории (папки, где будут храниться результаты)\nsetwd(\"C:/BAKANEV/JABBA\")\n\n# Создание папки для результатов анализа\nassessment &lt;- \"NEW JABBA\" # Название оценки\noutput.dir &lt;- file.path(getwd(), assessment) # Создание пути к папке\ndir.create(output.dir, showWarnings = FALSE) # Создание папки (если не существует)\nsetwd(output.dir) # Переход в созданную папку\n\n# ------------------------- 2. ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ ---------------------------\n\n# Создание вектора лет анализа\nYear &lt;- 2005:2024 # Последовательность лет от 2005 до 2024\n\n# Вектор данных об уловах (catch)\nCatch &lt;- c(5,7,6,10,14,25,28,30,32,35,25,20,15,12,10,12,10,13,11,12)\n\n# Вектор данных индекса обилия CPUE (catch per unit effort)\nCPUE &lt;- c(27.4,26.8,16.8,23.0,29.0,30.0,16.5,17.2,10.5,14.6,8.3,11.4,15.5,13.8,11.5,15.3,12.2,15.6,16.2,13.4)\n# Вектор данных индекса обилия BESS\nBESS &lt;- c(NA,16.3,20.7,15.1,18.6,16.0,13.8,13.3,11.7,11.8,9.3,7.1,8.0,9.2,10.3,9.8,10.3,11.7,13.7,13.4)\n\n# Форматирование данных в таблицы для JABBA\ncatch_data &lt;- data.frame(year = Year, catch = Catch) # Таблица уловов\ncpue_data &lt;- data.frame(year = Year, CPUE = CPUE, BESS = BESS) # Таблица индексов\n\n# Расчет стандартных ошибок (SE) для индексов\n# Используем коэффициент вариации (CV) = 20% (0.2)\n\nse_data &lt;- data.frame(\n year = Year,\n CPUE = ifelse(is.na(CPUE), NA, 0.2),\n BESS = ifelse(is.na(BESS), NA, 0.2))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Продукционная модель JABBA</span>"
    ]
  },
  {
    "objectID": "chapter 6.html#настройка-и-запуск-модели-jabba",
    "href": "chapter 6.html#настройка-и-запуск-модели-jabba",
    "title": "7  Продукционная модель JABBA",
    "section": "7.3 Настройка и запуск модели JABBA",
    "text": "7.3 Настройка и запуск модели JABBA\nВ этом разделе выполняется конфигурация и запуск JABBA. Сначала с помощью функции build_jabba формируется структура входных данных для модели, куда передаются подготовленные таблицы уловов, индексов CPUE/BESS и их стандартных ошибок. Указывается название оценки (“NEW JABBA”) и сценарий моделирования (“SPiCT_adapted”). В качестве биологической основы выбрана модель Шефера (логистический рост). Настраиваются ключевые априорные распределения: для темпа роста популяции (r) задано нормальное распределение N(0.2±0.5), для емкости среды (K) - логнормальное LN(189.6±0.795), для начальной заполненности запаса (ψ) - бета-распределение Beta(0.75±0.25). Установлена оценка процессной ошибки (sigma.est=TRUE) и слабоинформативные априоры для дисперсии наблюдений.\nНепосредственный запуск Байесовской оценки выполняется функцией fit_jabba, которая использует MCMC-алгоритм. Конфигурация MCMC включает 50,000 итераций с отбрасыванием первых 10,000 (фаза “burn-in”), прореживанием цепей в 5 раз и запуском 2 независимых цепей для проверки сходимости. Результатом работы является объект fit, содержащий апостериорные распределения параметров, оценки биомассы и диагностику модели, которые будут использоваться для последующего анализа состояния запаса.\n\n# ------------------- 3. НАСТРОЙКА И ЗАПУСК МОДЕЛИ JABBA --------------------\n\n\n# Создание входных данных для модели\n\njbinput &lt;- build_jabba(\ncatch = catch_data,# Данные об уловах\ncpue = cpue_data,# Данные индексов обилия\nse = se_data,# Стандартные ошибки\nassessment = assessment, # Название оценки\nscenario = \"SPiCT_adapted\", # Сценарий модели\nmodel.type = \"Schaefer\", # Тип модели (Шефера)\nsigma.est = TRUE, # Оценивать изменчивость процесса?\nr.prior = c(0.2, 0.5),# Априорное распределение для r (среднее, SD)\nK.prior = c(189.6, 0.795),# Априорное для K (среднее, SD)\npsi.prior = c(0.75, 0.25),# Априорное для начального заполнения\nigamma = c(0.001, 0.001), # Параметры для дисперсии наблюдений\nverbose = FALSE # Отключить подробный вывод\n)\n\n# Запуск Байесовской модели (MCMC)\n\nfit &lt;- fit_jabba(\njbinput,# Входные данные\nni = 50000, # Общее количество итераций\nnb = 10000, # Количество \"выжигаемых\" итераций (burn-in)\nnt = 5,# Частота прореживания (thinning)\nnc = 2 # Количество цепей MCMC\n)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Продукционная модель JABBA</span>"
    ]
  },
  {
    "objectID": "chapter 6.html#анализ-параметров-модели-jabba-fitpars",
    "href": "chapter 6.html#анализ-параметров-модели-jabba-fitpars",
    "title": "7  Продукционная модель JABBA",
    "section": "7.4 Анализ параметров модели JABBA (fit$pars)",
    "text": "7.4 Анализ параметров модели JABBA (fit$pars)\nКоманда fit$pars выведет таблицу, содержит медианные значения, 95% доверительные интервалы и результаты тестов сходимости MCMC для ключевых характеристик модели.\n\n&gt; fit$pars\n\n        Median        LCI           UCI          Geweke.p Heidel.p\nK      257.137636616 178.4613660245 442.25933810 0.533    0.401\nr      0.268924223   0.1517493450   0.42756749   0.324    0.864\nq.1    0.102043488   0.0565010978   0.15490243   0.920    0.967\nq.2    0.078076035   0.0433099875   0.11858679   0.986    0.954\npsi    0.875023951   0.5909908905   1.23643434   0.970    0.470\nsigma2 0.003477230   0.0005645686   0.02110338   0.369    0.563\ntau2.1 0.006817871   0.0006978604   0.05031429   0.845    0.392\ntau2.2 0.002775946   0.0004835058   0.01978467   0.522    0.445\nm      2.000000000   2.0000000000   2.00000000   NaN      NA\n&gt;\n\nБиологические параметры:\n\nK(емкость среды): Медиана 257.14 единиц биомассы, например тыс.тонн (95% ДИ: 178.46–442.26). Широкий доверительный интервал отражает типичную неопределенность при отсутствии данных о периоде, когда запас приближался к нетронутому состоянию.\nr(темп роста): Медиана 0.269 год⁻¹ (95% ДИ: 0.152–0.428) соответствует биологическим ожиданиям для многих промысловых видов рыб.\nψ(начальная биомасса): Медиана 0.875 (95% ДИ: 0.591–1.236) указывает, что в 2005 году биомасса составляла ~87.5% от K.\n\nПараметры наблюдений:\n\nq.1(коэффициент уловистости (улавливаемости) CPUE): Медиана 0.102 (95% ДИ: 0.056–0.155)\nq.2(коэффициент уловистости (улавливаемости) BESS): Медиана 0.078 (95% ДИ: 0.043–0.119) Широкие доверительные интервалы (&gt;50% от медианы) подтверждают недостаточную информативность данных для точной оценки этих параметров.\n\nОценки ошибок:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nПараметр\n\nМедиана\n\n95% ДИ\n\nНазначение\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsigma2\n\n0.0035\n\n[0.0006, 0.021]\n\nДисперсия ошибки процесса\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntau2.1\n\n0.0068\n\n[0.0007, 0.050]\n\nДисперсия ошибки CPUE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntau2.2\n\n0.0028\n\n[0.0005, 0.020]\n\nДисперсия ошибки BESS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nСоотношение σ²/τ² показывает:\n\nДля CPUE: 0.0035/0.0068 ≈ 0.51\n\nДля BESS: 0.0035/0.0028 ≈ 1.25\n\n\n\n\n\n\nЭто свидетельствует, что неопределенность в большей степени обусловлена погрешностью данных, чем стохастичностью биологической динамики.\nДиагностика сходимости MCMC: Все p-значения тестов (Geweke, Heidel) превышают 0.05, что подтверждает сходимость цепей:\nУправленческие выводы:\n1.Из-за неопределенности в оценках K и q интерпретацию следует фокусировать на относительных показателях (B/Bmsy, F/Fmsy).\n2.Для повышения точности модели требуется:\n\nДополнительные данные за периоды высокой биомассы (для уточнения K)\nКалибровка индексов обилия (для снижения τ²)\n\n3.Высокие значения τ² указывают на необходимость улучшения качества данных CPUE и BESS.\nДиагностика и визуализация результатов\nПосле выполнения байесовской оценки в JABBA проводится комплексная диагностика и визуализация результатов. Автоматически генерируется набор стандартных графиков через функцию jabba_plots, которые сохраняются в рабочую директорию. Для углубленного анализа последовательно строятся специализированные графики: динамика уловов с модельными значениями, согласованность наблюдаемых и предсказанных индексов CPUE/BESS, сравнение априорных и апостериорных распределений ключевых параметров (r, K, q), а также диагностика остатков модели. Особое внимание уделяется сходимости MCMC-цепей для исключения вычислительных ошибок.\nВажнейшая часть анализа - визуализация временных трендов: абсолютной биомассы (B), промысловой смертности (F) и их соотношений с целевыми уровнями (B/Bmsy, F/Fmsy). Фазовый портрет и Кобэ-график интегрируют эти показатели, наглядно отображая историческую и текущую позицию запаса относительно ориентиров управления (перелов/недолов). Дополнительно выполняются тест на случайность остатков, логарифмические аппроксимации и анализ отклонений процесса, что обеспечивает всестороннюю проверку адекватности модели. Финал этапа - экспорт рассчитанных временных рядов (биомасса, F, B/Bmsy и др.) в CSV-файл для дальнейшего использования.\n\n# ------------------- 4. ДИАГНОСТИКА И ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ --------------------\n\n# Генерация стандартных диагностических графиков\n\njbplot_ensemble(fit)\njabba_plots(fit, output.dir = output.dir)\n\n# Индивидуальные графики для детального анализа:\n\njbplot_catch(fit)# График уловов\njbplot_cpuefits(fit)# Сравнение модельных и наблюдаемых индексов\njbplot_ppdist(fit)# Распределения априорных и апостериорных параметров\njbplot_residuals(fit)# Остатки модели\njbplot_mcmc(fit)# Диагностика сходимости MCMC\njbplot_trj(fit, type = \"B\")# Динамика биомассы\njbplot_trj(fit, type = \"F\")# Динамика промысловой смертности\njbplot_trj(fit, type = \"BBmsy\")# Отношение *B/B~msy~*\njbplot_trj(fit, type = \"FFmsy\")# Отношение *F/F~msy~*\njbplot_spphase(fit)# **График продуктивности запаса с разметкой фаз Kobe**\njbplot_kobe(fit)# Кобэ-график (*B/B~msy~* vs *F/F~msy~*)\n\n# Дополнительные диагностики:\n\njbplot_runstest(fit)# Тест на случайность остатков\njbplot_logfits(fit)# Графики в логарифмической шкале\n\njbplot_procdev(fit)# Отклонения процесса\n\n# Сохранение временных рядов результатов\n\nwrite.csv(fit$timeseries, file = \"results.csv\", row.names = FALSE)\n\n\n\n\nРис. 1.: Генерация стандартных графиков: относительной и абсолютной промысловой биомассы, а также смертности, ошибки процесса и динамики вылова.\n\n\n\n\n\nРис. 2.: Сравнение модельных и наблюдаемых индексов\n\n\nГрафик сравнения позволяет визуально оценить адекватность модели реальным данным и выявить систематические расхождения. Он отображает: 1.Наблюдаемые значенияиндексов (например, CPUE и BESS) в виде точек с вертикальными отрезками, отражающими доверительные интервалы на основе стандартных ошибок. 2.Модельные предсказания в виде сплошной линии с затененной областью (50 и 95% доверительные интервалы апостериорного распределения).\nКлючевые аспекты интерпретации:\nСогласованность: Если модельная кривая проходит в пределах доверительных интервалов наблюдаемых точек, это свидетельствует о хорошем описании трендов. Смещения: Систематическое занижение/завышение предсказаний для определенных периодов указывает на недостатки модели (например, недоучет факторов среды или нелегального вылова). Чувствительность индексов: Различия в точности аппроксимации CPUE и BESS помогают оценить, какой индекс информативнее отражает динамику биомассы. Аномалии: Резкие выбросы точек за пределы доверительной зоны модели сигнализируют о годах с нетипичными условиями (ошибки данных, природные катаклизмы). Практическое значение: График отвечает на вопрос — способна ли выбранная продукционная модель (в данном случае Шефера) достоверно воспроизводить историческую динамику запаса, что является основой для корректных прогнозов.\n\n\n\nРис. 3.: Распределения априорных и апостериорных параметров\n\n\nЭтот график предоставляет инструмент для анализа влияния данных на исходные предположения модели. Он визуализирует:\n1.Априорные распределения(темные области): - Заданные до анализа (например:r ~ N(0.2, 0.5),K ~ LN(189.6, 0.795)). - Отражают экспертные гипотезы или литературные данные о параметрах.\n2.Апостериорные распределения(светлые области): - Рассчитанные в ходе Байесовского вывода (MCMC) после учета данных (уловы, индексы). - Показывают,как фактическая информация модифицировала первоначальные предположения.\nАспекты интерпретации:\nСдвиг пиков: Если апостериор смещен относительно априора (напр., пик для r сдвинулся от 0.2 к 0.3) – данные “перевесили” априорную гипотезу. Сужение кривой: Резкое сокращение дисперсии апостериора (напр., для K) свидетельствует о высокой информативности данных по этому параметру. Конфликт: Если апостериорный пик находится в “хвосте” априора (напр., априор для q задан N(1,0.2), а апостериор с пиком при 2.5) – сигнал о несоответствии данных или модели.\nПараметры-индикаторы:\n\nr (темп роста): Четкий апостериор указывает на надежную оценку продуктивности запаса.\nK (емкость среды): Узкий апостериор – уверенность в оценке исторической биомассы.\nψ (начальная заполненность): Расхождение с априором может указывать на ошибку в задании начальных условий.\n\nДиагностическое значение:\nЕсли апостериоры близки к априорам – данные не внесли новую информацию (требует пересмотра индексов). Если апостериоры асимметричны/многомодальны – возможны проблемы идентификации параметров (необходимы дополнительные диагностики).\nВывод: График отвечает на вопрос – насколько исходные биологические гипотезы подтвердились реальными данными, что критично для обоснованности оценки.\nКасательно графиков коэффициентов пропорциональности или улавливаемости q. Здесь, для простоты примера, вручную не задаются. JABBA по умолчанию (автоматически) назначает слабый или малоинформативный априор: q ~ LogNormal(mean = 1, SD = 1000) Это практически равномерное распределение в широком диапазоне (от ~0 до +∞). Поэтому на графиках не видны темные области априорных распределений q1 и q2. Физический смысл: Мы не знаем, во сколько раз индекс отличается от абсолютной биомассы.\nPPMR и PPVR: диагностические показатели в JABBA\nВ байесовском анализе, который лежит в основе работы JABBA, ключевым этапом является проверка сходимости MCMC-цепей. Именно для этой цели используются диагностические показатели PPMR (Potential Scale Reduction Factor for Multivariate Monitoring) и PPVR (Potential Scale Reduction Factor for Predictive Variance). Эти статистики позволяют оценить, насколько надежны полученные оценки параметров модели. PPMR представляет собой многомерный аналог классического R-hat критерия и оценивает сходимость сразу для всех параметров модели, учитывая их взаимосвязи. Он рассчитывается как корень из отношения дисперсии между цепями к дисперсии внутри цепей. Идеальное значение PPMR должно быть близко к 1, а значения выше 1.1 указывают на серьезные проблемы со сходимостью. PPVR же фокусируется конкретно на сходимости дисперсии апостериорных предсказаний, что особенно важно для оценки надежности доверительных интервалов прогнозируемых величин, таких как B/Bmsy или F/Fmsy. Значения PPVR также должны стремиться к 1.\nВ вашем конкретном случае анализ этих показателей дает неоднозначную картину. Для параметра K (несущая способность) мы видим практически идеальную сходимость: PPMR равен 0.97, что даже немного меньше 1, что указывает на превосходную стабильность оценок между цепями, а крайне низкое значение PPVR (0.109) говорит о высокой согласованности в оценке неопределенности. Это позволяет с уверенностью доверять полученным значениям K. Однако ситуация с параметром r (темп роста популяции) вызывает серьезные опасения. Значение PPMR 1.471 существенно превышает критический порог, что свидетельствует о явных проблемах со сходимостью цепей. Хотя PPVR 0.322 выглядит лучше, это не компенсирует высокий PPMR. Такая ситуация часто возникает при слишком широких априорных распределениях или недостатке информативных данных, особенно в периоды роста популяции. Для параметра ψ (начальная заполненность запаса) ситуация промежуточная: PPMR 1.174 находится на границе допустимого, а PPVR 0.501 указывает на умеренную согласованность в оценке неопределенности.\nДля улучшения сходимости рекомендуется предпринять несколько шагов. Во-первых, стоит значительно увеличить количество итераций MCMC - например, до 100000 для ni и 20000 для фазы “burn-in” (nb). Во-вторых, необходимо пересмотреть априорные распределения, особенно для параметра r: возможно, стоит уменьшить стандартное отклонение с 0.5 до 0.2-0.3, если есть экспертные основания для такого ужесточения. В-третьих, стоит проверить качество входных данных - достаточно ли репрезентативны имеющиеся индексы обилия, особенно в ключевые периоды динамики популяции. Важно понимать, что при сохраняющихся проблемах со сходимостью абсолютные оценки биомассы и темпа роста могут оставаться ненадежными, однако относительные показатели, такие как B/Bmsy, часто оказываются более устойчивыми к подобным проблемам. Это связано с тем, что они в меньшей степени зависят от абсолютных значений проблемных параметров. Таким образом, несмотря на выявленные сложности, модель может оставаться полезной для принятия управленческих решений, особенно если фокусироваться на относительных показателях состояния запаса.\n\n\n\nРис. 4.: Остатки модели\n\n\nГрафик остатков в JABBA (jbplot_residuals) представляет собой важный диагностический инструмент, позволяющий оценить качество соответствия модели реальным данным. На этом графике отображаются остатки - разницы между наблюдаемыми значениями индексов обилия (CPUE и BESS) и их модельными оценками. В вашем случае график строится на основе конкретных значений остатков, где для CPUE они варьируются от -0.38 до +0.31, а для BESS - от -0.20 до +0.17, с отсутствующим значением (NA) для BESS в 2005 году.\nГрафик имеет несколько ключевых элементов. Во-первых, это боксплоты, которые визуализируют распределение остатков для каждого индекса в целом, показывая медиану, квартили и возможные выбросы. Во-вторых, на график накладывается сглаженная линия (loess), которая помогает выявить систематические тренды в остатках. Например, если остатки демонстрируют явную тенденцию к увеличению или уменьшению со временем, это может указывать на неучтенные факторы в модели. В ваших данных остатки CPUE показывают некоторую изменчивость, но без явного тренда, в то время как BESS имеет более стабильные остатки с меньшим разбросом.\nИнтерпретация остатков имеет решающее значение. В идеале остатки должны быть случайно распределены вокруг нуля без видимых закономерностей. Наличие кластеров положительных или отрицательных остатков в определенные периоды может указывать на систематические ошибки модели. В нашем случае отсутствие явных трендов на графике - хороший знак, хотя отдельные выбросы, такие как отрицательный остаток CPUE в 2013 году (-0.38), заслуживают внимания. Размеры остатков также важны: значения, превышающие по модулю 0.5, считаются значительными и могут указывать на проблемы с данными или спецификацией модели.\nГрафик остатков особенно полезен для сравнения разных индексов. В нашем анализе видно, что остатки CPUE имеют больший разброс по сравнению с BESS, что может говорить либо о более высокой вариабельности данных CPUE, либо о том, что модель хуже описывает эту компоненту. Отсутствие данных BESS для 2005 года (NA) корректно обрабатывается графиком. Важно отметить, что систематические смещения остатков вверх или вниз могут указывать на проблемы с калибровкой коэффициентов уловистости (q), что согласуется с ранее выявленными сложностями в их оценке.\n\n\n\nРис. 5.: График продуктивности запаса с разметкой фаз Кобэ с динамикой вылова\n\n\nГрафик представляет собой комплексную визуализацию, объединяющую три ключевых аспекта анализа продукционной модели: 1) продукционную функцию (дуга), 2) динамику вылова относительно биомассы и 3) фазовые переходы состояния запаса в координатах Кобэ-графика\nНа графике по оси X отображается биомасса (в абсолютных единицах или относительно Bmsy), а по оси Y — вылов и чистая продукция (surplus production). Кривая производственной функции (обычно параболическая для модели Шефера) показывает зависимость между биомассой и устойчивым выловом. Точки на графике представляют фактические годовые значения биомассы и вылова, соединенные линиями в хронологическом порядке. Цвет точек кодирует фазовое состояние запаса согласно классификации Kobe: зеленый — устойчивое состояние (B/Bmsy, F/Fmsy), желтый/оранжевый — перелов (B/Bmsy, F/Fmsy или B/Bmsy, F/Fmsy), красный — коллапс (B/Bmsy, F/Fmsy).\n\n\n\nРис. 6.: Кобэ-график (B/Bmsy vs F/Fmsy) с динамикой вылова\n\n\nГрафик Кобэ, генерируемый функцией jbplot_kobe(fit), представляет собой фазовый портрет, где по оси X отложено отношение биомассы к целевому уровню (B/Bmsy), а по оси Y — отношение промысловой смертности к устойчивому уровню (F/Fmsy). Этот график разделен на четыре квадранта, определяющих статус запаса. Зеленый квадрант (B/Bmsy &gt; 1, F/Fmsy &lt; 1) соответствует устойчивому состоянию без перелова. Желтый квадрант (B/Bmsy &gt; 1, F/Fmsy &gt; 1) сигнализирует о риске перелова. Оранжевый квадрант (B/Bmsy &lt; 1, F/Fmsy &gt; 1) указывает на активный перелов, а желтый (B/Bmsy &lt; 1, F/Fmsy &lt; 1) — на фазу восстановления запаса.\nВ нашем анализе целевые ориентиры рассчитаны на основе параметров модели: Bmsy = K/2 = 257.14/2 ≈ 128.57, а MSY = r×K/4 ≈ 0.269×257.14/4 ≈ 17.29. На графике отображена траектория запаса с 2005 по 2024 годы, где каждая точка соответствует медианным оценкам за год, а соединяющие их линии показывают хронологическую динамику. В 2005 году запас находился в идеальном состоянии: B/Bmsy = 1.83 (95% ДИ: 1.25–2.52), F/Fmsy = 0.15 (0.09–0.26), что помещает его глубоко в зеленый квадрант.\nПериод 2010-2015 годов демонстрирует критическое ухудшение состояния: к 2013 году B/Bmsy снижается до 0.89 (0.64–1.15), а F/Fmsy достигает пика 1.74 (1.16–2.86) в 2011 году, что соответствует оранжевому квадранту активного перелова. Это совпадает с историческими данными о максимальных уловах (28-35 единиц в 2009-2013 гг.), превышающих расчетный MSY (17.29). Последующий период (2016-2024) показывает восстановление: к 2024 году B/Bmsy возрастает до 1.29 (0.90–1.64), а F/Fmsy снижается до 0.51 (0.31–0.93), возвращая запас в зеленый квадрант.\nТекущее положение (2024 год) указывает на восстановление запаса, однако горизонтально вытянутое облако неопределенности для B/Bmsy отражает чувствительность к оценке параметра K, чей широкий доверительный интервал (178–442) обусловлен исторической нехваткой данных о периоде высокой биомассы. Вертикальная компактность F/Fmsy подтверждает относительно точную оценку промысловой смертности. Управленческая рекомендация основывается на медианных значениях: поддержание F/Fmsy на уровне ≈0.5 позволит сохранить запас в устойчивом состоянии. Однако из-за неопределенности в оценке B/Bmsy (риск попадания в желтый квадрант при нижней границе ДИ 0.90) необходим ежегодный мониторинг с обновлением модели по новым данным. Исторический пример перелова 2011-2013 годов демонстрирует последствия превышения F/Fmsy &gt;1.5, что должно учитываться при установке лимитов вылова.\n\n7.4.1 Дополнительные диагностики\n\n# Дополнительные диагностики:\njbplot_runstest(fit)    # Тест на случайность остатков\njbplot_logfits(fit)     # Графики в логарифмической шкале\njbplot_procdev(fit)     # Отклонения процесса\n\n# Сохранение временных рядов результатов\nwrite.csv(fit$timeseries, file = \"results.csv\", row.names = FALSE)\n\n\n\n\nРис. 7.: График теста серий (runs test) для диагностики остатков\n\n\nДублирование графика остатков, но для каждого индекса выводится отдельный график. На графике отображаются остатки модели (разницы между наблюдаемыми и предсказанными значениями индексов обилия) в виде последовательности точек, упорядоченных по времени.Тест серий (runs test) анализирует последовательность чередований положительных и отрицательных остатков. Случайное распределение остатков (что является желаемым результатом) будет проявляться в частом чередовании положительных и отрицательных значений. Напротив, наличие длинных серий (несколько положительных или отрицательных остатков подряд) может указывать на:\n\nНеучтенные временные зависимости в данных\nНеадекватность структуры модели (например, пропущенные важные переменные)\nСистематические ошибки в данных\nНеправильную спецификацию функциональной формы модели\n\nГрафик отклонений процесса (Process Deviations) в JABBA\n\n\n\nРис. 8.: Отклонения процесса\n\n\nГрафик отклонений процесса (jbplot_procdev(fit)) отображает различия между фактической динамикой биомассы и теоретическими предсказаниями продукционной модели. Эти отклонения (Bdev) количественно выражают влияние неучтенных моделью факторов на популяцию — от климатических аномалий до изменений в кормовой базе. В нашем анализе медианные значения Bdev колеблются в узком диапазоне от -0.024 (2011) до +0.021 (2017), при этом все 95% доверительные интервалы включают ноль. Это свидетельствует об отсутствии статистически значимых отклонений, что подтверждает адекватность базовой модели Шефера. Биологически положительные Bdev указывают на неожиданный рост биомассы (например, за счет улучшения условий воспроизводства), тогда как отрицательные — на незапланированные потери (эпизоотии, незарегистрированную смертность).\nОсобый интерес представляют два периода: 2011 год с минимальным Bdev (-0.024) совпадает с пиком промысловой нагрузки (F/Fmsy ≈ 1.74), что может отражать дополнительную естественную смертность, вызванную переловом. В 2017 году положительное отклонение (+0.021) соответствует фазе активного восстановления запаса. Важно, что 80% лет показывают абсолютные значения Bdev &lt; 0.01 — исключительно высокий показатель, подчеркивающий надежность модели. Приемлемым диапазоном считаются отклонения в пределах ±0.05 при сохранении доверительных интервалов, пересекающих ноль; наши данные существенно строже этих критериев.\nСущественное влияние на отклонения процесса оказывает неучтенный вылов. Если такой вылов присутствует, модель, не получая данных о реальном изъятии, будет систематически переоценивать биомассу. Это проявляется как стабильно отрицательные Bdev, особенно выраженные в периоды интенсивного промысла. Например, при ежегодном неучтенном изъятии в 20% от официального улова, медианные отклонения сместились бы в зону -0.05…-0.10, а доверительные интервалы перестали бы включать ноль. В нашем случае отсутствие таких систематических сдвигов (разброс Bdev симметричен относительно нуля) позволяет заключить, что неучтенный вылов не является критическим фактором для данной популяции. Однако для окончательных выводов требуется анализ ретроспективных данных по промысловому усилию и независимая верификация учетных методик.\nПомимо визуализации, используя нижеприведенный скрипт, можно получить фактические значения, например, с 90%-ным доверительным интервалом: years &lt;- fit$yr\n\n# Агрегировать Bdev по годам (медиана и 90% интервал)\n\nproc_dev \\&lt;- aggregate(Bdev \\~ year, data = fit\\$kbtrj, FUN = function(x) c(median = median(x), lci = quantile(x, 0.05), uci = quantile(x, 0.95)))\n\nprint(proc_dev)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Продукционная модель JABBA</span>"
    ]
  },
  {
    "objectID": "chapter 6.html#ретроспективный-анализ",
    "href": "chapter 6.html#ретроспективный-анализ",
    "title": "7  Продукционная модель JABBA",
    "section": "7.5 Ретроспективный анализ",
    "text": "7.5 Ретроспективный анализ\nМы переходим к ретроспективному анализу (hindcasting), который является важным инструментом для оценки устойчивости модели и ее чувствительности к новым данным. В ретроспективном анализе модель последовательно переоценивается с исключением последних лет данных (по одному году за раз, в данном случае от 1 до 5 лет). Это позволяет проверить, насколько сильно меняются оценки ключевых параметров и статуса запаса при поступлении новых данных.\nВ вашем скрипте ретроспективный анализ запускается функцией `hindcast_jabba()`, которая использует исходные настройки модели (`jbinput`) и результаты базовой оценки (`fit`). Аргумент `peels = 1:5` указывает, что нужно последовательно удалять от 1 до 5 последних лет данных. Результаты сохраняются в объект `hc`.\nЗатем с помощью функции `jbplot_retro()` визуализируются результаты ретроспективного анализа. График показывает, как меняется оценка биомассы (или B/Bmsy) при исключении данных за последние годы. На графике будет изображена траектория базовой оценки (со всеми данными) и траектории, полученные при удалении 1, 2, 3, 4 и 5 лет. Также рассчитывается и отображается статистика Мона (Mohn’s rho), которая количественно оценивает смещение ретроспективных оценок относительно базовой.\n\n# ------------------- 5. РЕТРОСПЕКТИВНЫЙ АНАЛИЗ --------------------\n\n# Создание папки для результатов ретроспективы\nretro.dir &lt;- file.path(output.dir, \"retro\")\ndir.create(retro.dir, showWarnings = FALSE)\n\n# Запуск ретроспективного анализа (убираем по 1-5 лет)\nhc &lt;- hindcast_jabba(jbinput = jbinput, fit = fit, peels = 1:5)\n\n# Визуализация ретроспективного анализа\nmohnsrho &lt;- jbplot_retro(\n  hc, \n  as.png = FALSE,         # Чтобы сохранить как PNG-файл установите TRUE\n  output.dir = retro.dir,\n  xlim = c(2007, 2022)   # Ограничения по годам на графике\n)\n\n# Кросс-валидация\nmase &lt;- jbplot_hcxval(hc, as.png = FALSE, output.dir = retro.dir)\n\nСтатистика Мона (Mohn’s rho):\nКлючевой показатель смещения, рассчитываемый как относительная разница между ретроспективной и базовой оценкой в год исключения:\n\\[\n\\rho = \\frac{X_{\\text{ретро}} - X_{\\text{база}}}{X_{\\text{база}}}\n\\]\n\n&gt; mohnsrho\n                 B           F         Bmsy        Fmsy         procB\n2024   -0.01949857  0.01635894  0.031098563 -0.05870471  0.0019184659\n2023    0.06336652 -0.05341885 -0.009782004  0.03945419 -0.0004440846\n2022   -0.04605748  0.04811313 -0.043482407  0.10142418 -0.0029895725\n2021   -0.04274631  0.03790912  0.014020696 -0.03150997  0.0011936097\n2020   -0.04268405  0.04748624 -0.034149276  0.04684297 -0.0006575862\nrho.mu -0.01752398  0.01928972 -0.008458886  0.01950133 -0.0001958335\n               MSY\n2024    0.02871203\n2023   -0.02266077\n2022   -0.03747819\n2021    0.02474720\n2020    0.00148479\nrho.mu -0.00103899\n&gt; \n\n\n\n\nРис. 9.: Графики ретроспективного анализа\n\n\n\nНаши результаты:\n\nБиомасса (*Bм): ρ=−0.018 (слабое отрицательное смещение)\nСмертность (F): ρ=+0.019 (слабое положительное смещение)\nMSY: ρ=−0.001 (незначимое смещение) Критерий: ∣ρ∣&lt;0.2 приемлемо наши значения ≪0.2.\n\nВизуализация (jbplot_retro) показывает:\n\nКак меняется траектория биомассы при исключении данных\n“Веер” расходящихся линий: чем сильнее расхождение, тем выше нестабильность модели\nВ вашем случае линии остаются близкими — модель устойчива.\n\n\nМетрика MASE (Mean Absolute Scaled Error).\nКросс-валидация с помощью jbplot_hcxval() оценивает предсказательную способность модели. Для каждого “среза” (peel) модель предсказывает индекс обилия для удаленных лет, а затем эти предсказания сравниваются с фактическими наблюдениями. Рассчитывается MASE (Mean Absolute Scaled Error) — средняя абсолютная ошибка прогноза, нормированная на ошибку наивного прогноза (который предполагает, что будущее значение равно последнему наблюдённому).\n\\[\n\\mathrm{MASE} = \\frac{\\text{Средняя ошибка прогноза}}{\\text{Средняя ошибка наивного прогноза}}\n\\]\n\nMASE &lt; 1: Модель лучше наивного метода (предсказывающего “завтра=сегодня”)\nMASE &gt; 1: Модель работает хуже наивного метода\n\n\n\n\nРис. 10.: Метрика MASE\n\n\n\n&gt; mase\n  Index      MASE  MASE.adj     MAE.PR   MAE.base n.eval\n1  CPUE 0.6806899 0.6402326 0.13413055 0.19705090      5\n2  BESS 0.9539647 0.6632760 0.07763672 0.08138322      5\n3 joint 0.7605651 0.7605651 0.10588363 0.13921706     10\n\nНаши результаты показывают, что для CPUE модель предсказывает лучше, чем наивный метод (0.68&lt;1), а для BESS — немного хуже (0.95≈1). Совместный MASE (0.76) указывает на удовлетворительную общую прогнозную способность. Однако для индекса BESS стоит обратить внимание на возможные улучшения.\n\n\n\n\n\n\n\n\nИндекс\nMASE\nИнтерпретация\n\n\n\n\nCPUE\n0.68\nХорошо: Прогноз на 32% точнее наивного\n\n\nBESS\n0.95\nУдовлетворительно: Почти эквивалентен наивному методу\n\n\nСовместно\n0.76\nПриемлемая общая точность\n\n\n\nПочему это важно?\n\nДля управления запасами:\n\n\nСтабильность Mohn’s rho (ρ≈0) означает, что текущие рекомендации по вылову не изменятся радикально при получении новых данных.\nНизкий MASE подтверждает надежность краткосрочных прогнозов.\n\n\nДля диагностики модели:\n\n\nСистематическое смещение ρB&gt;0.3 могло бы указывать на переоценку запаса (риск перелова).\nMASE &gt; 1 для BESS требует улучшения описания этого индекса (например, через калибровку q).\n\n\nИсторический контекст:\nВ наших данных слабое смещение для MSY (ρ=−0.001) подтверждает, что модель корректно определяет максимальный устойчивый вылов (17.29), несмотря на проблемы с оценкой r (PPMR=1.47).\n\nВыводы для нашего случая:\nРетроспективный анализ показал:\n\nВысокую стабильность: Смещения параметров статистически незначимы.\nУдовлетворительную прогнозную силу: Особенно для CPUE (MASE=0.68).\nОбласть улучшения: Индекс BESS требует внимания (MASE=0.95), возможно, за счет включения ковариат или пересмотра ошибок наблюдений.\nНадежность управленческих выводов: Текущие оценки статуса запаса (B/Bmsy = 1.29, F/Fmsy = 0.51) устойчивы к добавлению новых данных.\n\nЭтот этап завершает валидацию модели, подтверждая, что она пригодна для разработки рекомендаций по управлению промыслом.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Продукционная модель JABBA</span>"
    ]
  },
  {
    "objectID": "chapter 6.html#прогнозирование",
    "href": "chapter 6.html#прогнозирование",
    "title": "7  Продукционная модель JABBA",
    "section": "7.6 Прогнозирование",
    "text": "7.6 Прогнозирование\nПрогнозирование в JABBA представляет собой заключительный этап оценки запасов, позволяющий смоделировать будущую динамику популяции при различных сценариях управления промыслом. В нашем случае был выполнен 10-летний стохастический прогноз (2025-2034 гг.), основанный на текущем состоянии запаса, где биомасса в 2024 году оценивается в 148.6 тыс. т при соотношении B/Bmsy = 1.29 и промысловой нагрузке F/Fmsy = 0.51. Были рассмотрены четыре сценария годового изъятия: консервативный (10 тыс. т), умеренные (12 и 14 тыс. т) и интенсивный (16 тыс. т), что соответствует 58-93% от расчетного максимального устойчивого улова (MSY = 17.29).\n\n# ------------------- 6. ПРОГНОЗИРОВАНИЕ --------------------\n\n# Прогноз на основе F (ловушечное усилие)\nfw1 &lt;- fw_jabba(\n  fit,\n  nyears = 10,       # Длина прогноза (лет)\n  imp.yr = 1,        # Год внедрения новых правил\n  imp.values = seq(10, 16, 2), # Варианты управления (уровни улова)\n  quant = \"Catch\",   # Прогнозировать по уловам\n  type = \"abs\",      # Абсолютные значения\n  stochastic = TRUE  # Стохастический прогноз\n)\n\n# Графики ансамбля прогнозов\njbpar(mfrow = c(3, 2)) # Настройка макета графиков (3 строки, 2 столбца)\njbplot_ensemble(fw1)    # Основной график прогнозов\n\n\n\n\nРис. 11.: Прогностические графики\n\n\nКлючевой особенностью прогноза является его стохастическая природа — каждый сценарий учитывает неопределенность параметров модели, включая вариабельность темпа роста популяции (r) и ёмкости среды (K), а также ошибку процесса (σ² = 0.0035). Это позволяет получить не точечные предсказания, а вероятностные распределения будущих состояний. Результаты показывают дифференцированную динамику: при вылове 10-12 тыс. т биомасса демонстрирует устойчивый рост (до 178 и 168 тыс. т к 2034 году), сценарий с 14 тыс. т стабилизирует запас на уровне около 158 тыс. т, тогда как интенсивный вылов (16 тыс. т) приводит к постепенному снижению биомассы до 147 единиц.\nБиологическая интерпретация этих траекторий основывается на соотношении прогнозируемых показателей с целевыми ориентирами. Для сценария C16 к 2034 году ожидается приближение B/Bmsy к 1.14, что хотя и остается выше единицы, но указывает на сокращение “буферного” запаса. Особое внимание следует уделить чувствительности модели к оценке параметра r, чья высокая неопределенность (PPMR = 1.47) может существенно влиять на долгосрочные прогнозы — например, если реальный темп роста окажется ближе к нижней границе доверительного интервала (0.15), сценарий C16 может привести к переходу в зону перелова уже к 2030 году.\nУправленческие рекомендации, вытекающие из анализа, предлагают компромисс между экономической эффективностью и предосторожностью. Оптимальным признается диапазон вылова 12-14 единиц, обеспечивающий 70-80% от потенциального прироста продукции без риска снижения запаса ниже целевого уровня. Сценарий C16 может рассматриваться как временная мера только при наличии подтверждающих данных о высоком продуктивном потенциале популяции, но требует ежегодного мониторинга с коррекцией лимитов. Визуализация результатов через jbplot_ensemble() наглядно демонстрирует “веер” траекторий, где расхождение доверительных интервалов усиливается к концу периода прогноза — это прямое отражение кумулятивного эффекта неопределенности параметров и случайных факторов среды.\nВажным аспектом является интеграция прогноза в адаптивную систему управления: установив начальный лимит на уровне 14 тыс. т, следует планировать повторные оценки по данным ежегодных съемок, что позволит корректировать вылов в зависимости от фактического состояния запаса. Такой подход минимизирует риски, связанные с ограниченной точностью продукционных моделей при работе с данными низкой разрешающей способности. Исторический урок нашего анализа — пример перелова 2011-2013 годов — напоминает, что превышение F/Fmsy &gt; 1.5 способно за несколько лет подорвать даже запас, находившийся в благополучном состоянии.\nПри работе с JABBA есть трудности в получении различных графиков и фактических значений, например прогнозных. Ниже приводятся скрипты получения отдельных прогностических графиков и таблицы прогнозных значений выловов и биомасс.\n\n# График для B/Bmsy с кастомизацией\njbplot_ensemble(\n  fw1,\n  subplots = c(1),        # Только B/Bmsy\n  add = TRUE,             # Добавить к текущему графику\n  xlim = c(2020, 2035),   # Ограничение по годам\n  legend.loc = \"topleft\"  # Позиция легенды\n)\n\n\n\n\nРис. 12.: Отдельный график прогноза\n\n\nИзвлечение прогостических данных:\n\n# Фильтрация данных прогноза (2025-2034) для выбранных сценариев\nforecast_data &lt;- subset(\n  fw1, \n  year %in% 2025:2034 &    # Годы прогноза\n    run %in% c(\"C10\", \"C12\", \"C14\", \"C16\") & # Сценарии управления\n    type == \"prj\"           # Только прогнозные значения\n)\n\n# Расчет медиан биомассы (B) по годам и сценариям\nmedian_B &lt;- aggregate(\n  B ~ year + run,          # Формула: группировка по году и сценарию\n  data = forecast_data, \n  FUN = median             # Функция агрегации\n)\n\n# Расчет медиан улова (Catch) по годам и сценариям\nmedian_Catch &lt;- aggregate(\n  Catch ~ year + run, \n  data = forecast_data, \n  FUN = median\n)\n\n# Преобразование в широкий формат (годы по строкам, сценарии по столбцам)\nb_table &lt;- dcast(median_B, year ~ run, value.var = \"B\")\ncatch_table &lt;- dcast(median_Catch, year ~ run, value.var = \"Catch\")\n\n# Вывод таблиц\nprint(\"Медианная биомасса:\")\nprint(b_table)\n\nprint(\"Медианные уловы:\")\nprint(catch_table)\n\n# Сохранение таблиц\nwrite.csv(b_table, \"biomass_forecast.csv\", row.names = TRUE)\nwrite.csv(catch_table, \"catch_forecast.csv\", row.names = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Продукционная модель JABBA</span>"
    ]
  },
  {
    "objectID": "chapter 7.html",
    "href": "chapter 7.html",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "",
    "text": "8.1 Введение\nНачнём с ловушки, в которую легко попасть всем нам. Когда перед глазами аккуратные таблицы предикторов и графики «запас–пополнение», возникает приятное ощущение, что, выбрав правильную модель и пару красивых параметров, мы можем управлять будущим — будто поставив регулятор на приборной панели, повысим пополнение в нужный год. Это и есть иллюзия контроля: данные неполны, среда нестационарна, а часть закономерностей — просто шум, который мозг охотно принимает за сигнал. Наша задача — осознанно тормозить, проверять допущения, избегать утечки информации из будущего в прошлое и признавать, что длинные хвосты — не исключение, а рабочая среда экологии. При этом скатываться в цинизм не нужно: рациональный, аккуратно спланированный анализ действительно улучшает предсказания и решения — в смысле медленного, но реального прогресса, если мы дисциплинируем энтузиазм диагностикой и воспроизводимостью. Биология редко ведёт себя как учебник: нелинейности, пороги, запаздывания и переменная «уловистость» индикаторов — всё это часть истории, и именно поэтому важно строить не одну «идеальную» модель, а цикл проверки конкурирующих объяснений, опираясь и на предметную логику, и на статистическую строгость.\nЭта практическая работа — про такой цикл. Мы изучаем зависимость пополнения R3haddock от среды и нерестового запаса, идём от подготовки данных до прогноза с доверительными интервалами, намеренно сопоставляя разные семейства моделей. Сначала приводим предикторы к численному виду, осмысленно обходимся с пропусками и сокращаем мультиколлинеарность, чтобы не строить интерпретацию на «двух зеркальных термометрах». Затем запускаем автоматический отбор признаков двумя методами — Boruta (в духе нелинейной, «лесной» логики) и LASSO (строгая сжатая линейная постановка). Их пересечение, скорректированное биологическим смыслом (например, сохранением нерестового запаса haddock68), даёт устойчивый стартовый набор факторов: температуры и океанография (T…, O…), биотические индикаторы (например, codTSB) и нерестовый запас. Далее мы намеренно не женимся на одном «красивом» уравнении: ставим рядом механистические модели «запас–пополнение» (Рикер и Бивертон–Холт) и статистические LM/GLM/GAM, чтобы увидеть, где данные действительно поддерживают горб плотностной зависимости, а где кривая честно выходит на плато; где линейная аппроксимация достаточна, а где гладкие функции выявляют оптимумы, пороги и нелинейные эффекты. Такой параллельный взгляд — это не прихоть, а способ не перепутать удобную историю с реальной динамикой.\nКритический момент — валидация во времени. Обычная случайная кросс‑валидация льстит нам, подмешивая будущее в прошлое; мы избегаем этого через time‑slice с расширяющимся окном и горизонтом прогноза, а затем фиксируем результат внешним хронологическим тестом. Так мы проверяем не только «как хорошо объяснили вчера», но и «как не обманулись насчёт завтра». По итогам сравнения берём не «победителя по AIC/Р² любой ценой», а устойчивую к хронологии схему — иногда это GAM, иногда более простая GLM, а порой и ансамбль, который признаёт, что комбинирование разных источников знания (простого и гибкого) часто надёжнее любого одиночного героя. Прогноз на 2022–2024 мы даём не как цифры на камне, а как веер с 50% и 95% интервалами — потому что у природы длинные хвосты, и задача аналитика — показать диапазон правдоподобного, а не притворяться владельцем хрустального шара. И наконец — про нарратив: любой вывод модели — это маленькая история о будущем запаса. Она приемлема для управления только тогда, когда прошла проверку данными, диагностикой и попыткой опровержения альтернативами. В этом практикуме мы именно так и поступаем: даём данным говорить, а себе — сомневаться, сравнивать и проверять. Именно так рождаются прогнозы, которые выдерживают столкновение с реальностью.\nИ так в сухом остатке, в этой практической работе представлен цикл прикладного анализа зависимости пополнения запаса гидробионта от факторов среды (в том числе нерестового запаса): от подготовки данных и отбора предикторов до сравнения нескольких семейств моделей, выбора устойчивой к хронологии прогностической схемы и построения прогноза с доверительными интервалами. Подход ориентирован на начинающих, но использует современные приёмы: автоматический отбор признаков (Boruta, LASSO), сопоставление линейных/нелинейных моделей, time-slice валидацию и ансамблевый прогноз. • Целевая переменная: R3haddock — пополнение запаса. • Кандидатные предикторы: гидрометеорология (температуры T…), океанография (O…), биотические показатели (например, codTSB) и нерестовый запас (haddock68). • Цель анализа: понять, какие факторы и в каких формах оказываются значимыми, отобрать рабочий набор моделей и получить прогноз на 2022–2024 с оценкой неопределенности.\nНастоящий анализ разделен на несколько этапов:\nВходные данные для работы скрипта: RECRUITMENT.xlsx, а также промежуточный файл с готовым набором предикторов: selected_predictors_dataset.csv.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 7.html#введение",
    "href": "chapter 7.html#введение",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "",
    "text": "Выбор предикторов. Скрипт можно скачать по ссылке\nПостроение биологически мотивированных (механистических) нелинейных классических моделей «запас-пополнение» Рикера и Бивертона-Холта. Анализ их значимости и сравнение с моделями LM/GLM/GAM. Скрипт можно скачать по ссылке\nПостроение классических статистических моделей LM/GLM/GAM. Анализ их прогностических способностей и выполнение прогноза. Скрипт можно скачать по ссылке\nПолный цикл ( СКРИПТ) прикладного анализа зависимости пополнения запаса гидробионта от факторов среды, включающий:\n\n\nа) выбор предикторов;\nб) базовое сравнение различных моделей;\nв) выбор лучшей прогностической модели;\nг) ансамблевый прогноз.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 7.html#выбор-предикторов",
    "href": "chapter 7.html#выбор-предикторов",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "8.2 Выбор предикторов",
    "text": "8.2 Выбор предикторов\nВ процессе анализа факторов, влияющих на пополнение запасов гидробионтов, важным этапом является тщательная подготовка данных и отбор наиболее информативных предикторов, поскольку качество последующих моделей напрямую зависит от качества входных данных. Начиная с первичной обработки, мы приводим все потенциальные предикторы к числовому формату, так как большинство статистических и машинно-обучаемых моделей требуют именно такой представления данных, при этом заменяем строковые обозначения пропущенных значений «NA» на стандартные NA, что позволяет системе R корректно обрабатывать отсутствующие наблюдения. Для заполнения пропусков мы применяем медианную импутацию, которая представляет собой простой и устойчивый к выбросам метод, поскольку медиана менее чувствительна к экстремальным значениям по сравнению со средним. Хотя существуют и более сложные альтернативы, такие как множественная импутация с использованием пакета mice, KNN-импутация через recipes::step_impute_knn или даже методы, специально разработанные для временных рядов, например, фильтр Калмана или ARIMA-модели, медианная импутация остается практичным выбором для начального этапа анализа, особенно когда объем данных ограничен или временные зависимости не являются доминирующими. Следующим важным этапом является анализ корреляционной структуры данных, поскольку высокая мультиколлинеарность между предикторами может серьезно ухудшить интерпретацию моделей и завысить дисперсию оценок параметров, особенно в линейных моделях. Для автоматического выявления и устранения сильно коррелированных переменных мы используем функцию findCorrelation с пороговым значением коэффициента корреляции 0.8, что позволяет сохранить лишь один представитель из каждой группы высококоррелированных переменных. Хотя альтернативными подходами могут служить диагностика по значениям VIF или применение методов снижения размерности, таких как PLS или PCA, удаление явно коррелированных предикторов оказывается наиболее прямолинейным решением для обеспечения стабильности последующих моделей. Для автоматического отбора наиболее значимых предикторов мы применяем два дополнительных метода, которые по-разному подходят к этой задаче и тем самым обеспечивают взаимную проверку результатов. Boruta представляет собой обертку над алгоритмом Random Forest, которая генерирует «теневые» переменные, полученные путем случайного перемешивания исходных признаков, и сравнивает важность реальных предикторов с этими теневыми копиями, сохраняя только те переменные, чья важность статистически превосходит уровень шума. Этот метод особенно эффективен при наличии нелинейных зависимостей и взаимодействий между переменными, демонстрируя высокую устойчивость к шуму, хотя и требует больше вычислительных ресурсов и может излишне благоволить к группам коррелированных признаков. Параллельно мы применяем LASSO-регрессию из пакета glmnet, которая использует L1-регуляризацию для зануления коэффициентов слабо влияющих предикторов, тем самым выполняет отбор признаков в процессе оценки модели. При выборе оптимального значения параметра регуляризации lambda мы сознательно предпочитаем значение lambda.1se, которое соответствует более простой модели, но при этом находится в пределах одной стандартной ошибки от минимального значения ошибки, так как этот консервативный подход часто обеспечивает лучшую обобщающую способность на небольших выборках, характерных для экологических данных. Однако LASSO имеет свои ограничения: он чувствителен к масштабу переменных, что делает центрирование и стандартизацию обязательными предварительными шагами, и предполагает линейную форму зависимости между предикторами и откликом, что может не соответствовать реальной биологической природе процессов. Финальный набор предикторов формируется как объединение результатов Boruta и LASSO с учетом биологической логики, что повышает устойчивость отбора к случайным флуктуациям, присущим каждому отдельному методу, и гарантирует включение ключевых переменных, таких как нерестовый запас (haddock68), который биологически должен влиять на пополнение запаса. Для предварительной проверки значимости отобранных предикторов мы строим простую линейную модель, которая не предназначена для окончательного прогноза, но служит в качестве sanity-check, позволяя оценить порядок величины эффектов и выявить явно незначимые или противоречащие биологической логике переменные. Важно отметить несколько нюансов и потенциальных подводных камней, с которыми можно столкнуться на этом этапе: если распределение целевой переменной R3haddock сильно скошено, может потребоваться лог-трансформация или использование моделей, специально разработанных для положительных откликов, таких как Gamma GLM; корреляция между переменными не обязательно отражает причинно-следственные связи, и при удалении высококоррелированных предикторов мы можем потерять полезную информацию, поэтому в некоторых случаях лучше применять методы, сохраняющие информацию из всех переменных, например, PLS или GAM; наконец, медианная импутация, хотя и проста в применении, может быть недостаточно точной для временных рядов, где хронологически осмысленная импутация, такая как скользящая медиана или интерполяция, часто дает более реалистичные результаты, учитывающие естественную динамику экологических процессов. Таким образом, этап подготовки данных и отбора предикторов представляет собой критически важный фундамент для последующего построения качественных моделей прогнозирования пополнения рыбных запасов, где баланс между статистической строгостью и биологической интерпретируемостью определяет успех всего анализа.\nСкрипт целиком можно скачать по ссылке\n\n# ==============================================================================\n# 1) ВЫБОР ПРЕДИКТОРОВ\n# ------------------------------------------------------------------------------\n# Цель блока: привести данные к числовому виду, обработать пропуски, сократить\n# мультиколлинеарность (сильные корреляции), а затем автоматически выделить\n# кандидатов-предикторов двумя методами (Boruta, LASSO). В конце сформируем\n# финальный пул признаков и проверим их значимость в простой LM.\n# ==============================================================================\n\n# Установка и подключение необходимых библиотек\n# Для автоматического отбора предикторов нам понадобятся дополнительные пакеты\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\nЗагрузка требуемого пакета: pacman\n\npacman::p_load(\n  readxl, tidyverse, caret, corrplot, mgcv, randomForest, xgboost,\n  Boruta,GGally, FactoMineR, glmnet, recipes, rsample  # Новые библиотеки для автоматического отбора\n)\n\n# Очистка среды и установка рабочей директории\n# Совет: rm(list=ls()) очищает все объекты в памяти R; setwd задаёт папку,\n# где искать/сохранять файлы. Убедитесь, что путь корректен на вашей машине.\nrm(list = ls())\nsetwd(\"C:/RECRUITMENT/\")\n\n# Пакеты для расширенного отбора предикторов\n# Boruta — обёртка над Random Forest для отбора признаков;\n# glmnet — регуляризация (LASSO/ElasticNet) для отбора/усиления обобщающей способности;\n# FactoMineR — PCA и другие многомерные методы (используем как утилиту).\nlibrary(Boruta)   # Алгоритм обертки для отбора признаков\nlibrary(glmnet)   # LASSO-регрессия\nlibrary(FactoMineR) # PCA анализ\n\n\n# Загрузка и первичная обработка данных\n# Шаги: фильтруем годы, приводим типы к числовому, заменяем строковые \"NA\" на NA.\nDATA &lt;- readxl::read_excel(\"RECRUITMENT.xlsx\", sheet = \"RECRUITMENT\") %&gt;%\n  filter(YEAR &gt; 1989 & YEAR &lt; 2022) %&gt;%\n  # Преобразуем необходимые столбцы в числовой формат\n  mutate(\n    across(starts_with(\"T\"), as.numeric),\n    across(starts_with(\"I\"), as.numeric),\n    across(starts_with(\"O\"), as.numeric),\n  ) %&gt;%\n  # Обработка пропущенных значений (заменяем строку \"NA\" на NA)\n  mutate(across(where(is.character), ~na_if(., \"NA\")))\n\n# 1. Подготовка данных -------------------------------------------------------\n# Выделим все возможные предикторы, включая географию и индексы трески\n# Примечание: оставляем только числовые переменные, т.к. большинство моделей\n# требует числовой вход без категориальных уровней.\npredictors &lt;- DATA %&gt;% \n  select(-YEAR, -R3haddock) %&gt;% \n  select_if(is.numeric) # Только числовые переменные\n\n# Целевая переменная\nresponse &lt;- DATA$R3haddock\n\n# В статистическом анализе мы различаем:\n# - Отклик (response/target variable) - то, что мы пытаемся предсказать (в нашем случае R3haddock)\n# - Предикторы (predictors/features) - переменные, которые могут объяснять изменения отклика\n# Для корректного анализа важно, чтобы предикторы были числовыми или преобразованы в числовой формат.\n\n# 2. Обработка пропусков -----------------------------------------------------\n# Заполнение медианными значениями — простой и устойчивый способ справиться с NA.\n# Альтернативы: множественная иммутация (mice), KNN-impute и др.\npredictors_filled &lt;- predictors %&gt;%\n  mutate(across(everything(), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))\n\n# Заполнение медианой - простой и устойчивый метод обработки пропусков для числовых переменных.\n# Медиана предпочтительнее среднего, так как менее чувствительна к выбросам.\n\n# 3. Предварительный анализ корреляций ---------------------------------------\n# Зачем: высокие корреляции затрудняют интерпретацию и могут вредить ряду моделей.\ncor_matrix &lt;- cor(predictors_filled, use = \"complete.obs\")\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", tl.cex = 0.7)\n\n\n\n\n\n\n\n# Удаляем высокоскоррелированные предикторы (r &gt; 0.8)\n# Это механическое сокращение мультиколлинеарности до этапа отбора.\nhigh_cor &lt;- findCorrelation(cor_matrix, cutoff = 0.8)\npredictors_filtered &lt;- predictors_filled[, -high_cor]\n\n# Высокая корреляция между предикторами (мультиколлинеарность) может привести к нестабильности моделей.\n# Например, если два предиктора почти идентичны, модель может неустойчиво распределять их влияние на отклик.\n# Удаление сильно коррелированных переменных (r &gt; 0.8) помогает улучшить интерпретируемость и стабильность моделей.\n\n\n# 4. Автоматизированный отбор Boruta (обертка Random Forest) -----------------\n# Идея: определить признаки, которые важнее, чем случайный шум (shadow features).\n\n\n# Визуализация результатов\nplot(boruta_output, cex.axis = 0.7, las = 2)\n\n\n\n\n\n\n\nboruta_stats &lt;- attStats(boruta_output)\nselected_vars &lt;- getSelectedAttributes(boruta_output, withTentative = TRUE)\n\n# Boruta - это алгоритм отбора признаков, основанный на методе случайного леса.\n# Он сравнивает важность реальных переменных с \"теневыми\" переменными (случайными копиями),\n# чтобы определить, действительно ли переменная информативна.\n# Результаты Boruta показывают: \n#   - Confirmed (зеленые) - значимые предикторы\n#   - Tentative (желтые) - предикторы, близкие к порогу значимости\n#   - Rejected (красные) - незначимые предикторы\n\n\n# 5. LASSO с более строгим критерием ------------------------------------------\n# Идея: L1-регуляризация зануляет коэффициенты «слабых» предикторов.\n# Выбор lambda.1se вместо lambda.min — более консервативный (простая модель).\nx &lt;- as.matrix(predictors_filtered)\ny &lt;- response\n\n# LASSO (Least Absolute Shrinkage and Selection Operator) - метод регрессии с L1-регуляризацией,\n# который одновременно выполняет отбор признаков и оценку коэффициентов. [[8]]\n# Параметр lambda контролирует силу регуляризации:\n#   - lambda.min дает наименьшую ошибку, но может включать шумовые переменные\n#   - lambda.1se (на 1 стандартную ошибку больше) дает более простую модель с меньшим риском переобучения\n# Для прогнозирования мы предпочитаем более строгий критерий (lambda.1se), чтобы модель была устойчивее. [[1]]\n\n# Кросс-валидация\ncv_fit &lt;- cv.glmnet(x, y, alpha = 1, nfolds = 10)\nplot(cv_fit)\n\n\n\n\n\n\n\n# ИСПОЛЬЗУЕМ lambda.1se вместо lambda.min — СТРОЖЕ!\nlasso_coef &lt;- coef(cv_fit, s = \"lambda.1se\")  # &lt;-- Ключевое изменение!\nlasso_vars &lt;- rownames(lasso_coef)[lasso_coef[,1] != 0][-1]  # исключаем (Intercept)\n\n\n# 6. Сравнение отобранных предикторов ----------------------------------------\n# Полезно видеть, какие признаки отмечают оба метода (устойчивые кандидаты).\ncat(\"Boruta selected:\", length(selected_vars), \"variables\\n\")\n\nBoruta selected: 3 variables\n\nprint(selected_vars)\n\n[1] \"codTSB\" \"T12\"    \"I5\"    \n\ncat(\"\\nLASSO selected:\", length(lasso_vars), \"variables\\n\")\n\n\nLASSO selected: 5 variables\n\nprint(lasso_vars)\n\n[1] \"codTSB\" \"T12\"    \"NAO3\"   \"NAO4\"   \"NAO5\"  \n\n# 7. Финальный набор предикторов (объединение результатов) -------------------\n# Логика: объединяем списки, добавляем биологически важные переменные вручную.\nfinal_vars &lt;- union(selected_vars, lasso_vars) \n\n# Добавляем обязательные переменные по биологической логике\nmandatory &lt;- c(\"haddock68\")\nfinal_vars &lt;- union(final_vars, mandatory) %&gt;% unique()\n\n# Мы объединяем результаты двух методов отбора признаков для большей надежности.\n# Также добавляем переменную haddock68 (нерестовый запас), так как биологически \n# логично, что пополнение запаса напрямую зависит от численности производителей. \n# Это пример интеграции экспертных знаний в статистический анализ - важный принцип \n# при работе с данными в биологических науках.\n\n# 8. Проверка значимости -----------------------------------------------------\n# Быстрая оценка значимости с LM: не как окончательный вывод, а как sanity-check.\nfinal_model &lt;- lm(response ~ as.matrix(predictors_filled[, final_vars]))\nsummary(final_model)\n\n\nCall:\nlm(formula = response ~ as.matrix(predictors_filled[, final_vars]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-270986  -82376   -1037   98086  276129 \n\nCoefficients:\n                                                      Estimate Std. Error\n(Intercept)                                         -1.082e+06  3.943e+05\nas.matrix(predictors_filled[, final_vars])codTSB    -2.346e-01  5.536e-02\nas.matrix(predictors_filled[, final_vars])T12        3.864e+05  7.198e+04\nas.matrix(predictors_filled[, final_vars])I5        -1.825e+02  2.572e+03\nas.matrix(predictors_filled[, final_vars])NAO3      -5.801e+04  3.129e+04\nas.matrix(predictors_filled[, final_vars])NAO4       8.345e+04  3.035e+04\nas.matrix(predictors_filled[, final_vars])NAO5      -7.278e+04  2.488e+04\nas.matrix(predictors_filled[, final_vars])haddock68  1.232e-01  4.515e-01\n                                                    t value Pr(&gt;|t|)    \n(Intercept)                                          -2.744 0.011305 *  \nas.matrix(predictors_filled[, final_vars])codTSB     -4.238 0.000288 ***\nas.matrix(predictors_filled[, final_vars])T12         5.368 1.64e-05 ***\nas.matrix(predictors_filled[, final_vars])I5         -0.071 0.944028    \nas.matrix(predictors_filled[, final_vars])NAO3       -1.854 0.076118 .  \nas.matrix(predictors_filled[, final_vars])NAO4        2.750 0.011146 *  \nas.matrix(predictors_filled[, final_vars])NAO5       -2.925 0.007412 ** \nas.matrix(predictors_filled[, final_vars])haddock68   0.273 0.787227    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 158600 on 24 degrees of freedom\nMultiple R-squared:  0.7173,    Adjusted R-squared:  0.6348 \nF-statistic: 8.698 on 7 and 24 DF,  p-value: 2.58e-05\n\n# 9. Формирование финального датасета ----------------------------------------\n# Собираем набор с откликом и выбранными предикторами; удалим строки с NA.\nmodel_data &lt;- DATA %&gt;%\n  select(R3haddock, all_of(final_vars)) %&gt;%\n  drop_na()\n\n# Просмотр структуры финальных данных\nglimpse(model_data)\n\nRows: 32\nColumns: 8\n$ R3haddock &lt;dbl&gt; 812363, 389416, 99474, 98946, 118812, 63028, 147657, 83270, ~\n$ codTSB    &lt;dbl&gt; 913000, 1347064, 1687381, 2197863, 2112773, 1849957, 1697388~\n$ T12       &lt;dbl&gt; 4.72, 4.66, 4.24, 3.90, 3.96, 4.27, 4.16, 4.07, 4.23, 5.08, ~\n$ I5        &lt;dbl&gt; 43, 55, 26, 49, 56, 28, 52, 51, 69, 68, 41, 48, 50, 63, 40, ~\n$ NAO3      &lt;dbl&gt; 1.46, -0.20, 0.87, 0.67, 1.26, 1.25, -0.24, 1.46, 0.87, 0.23~\n$ NAO4      &lt;dbl&gt; 2.00, 0.29, 1.86, 0.97, 1.14, -0.85, -0.17, -1.02, -0.68, -0~\n$ NAO5      &lt;dbl&gt; -1.53, 0.08, 2.63, -0.78, -0.57, -1.49, -1.06, -0.28, -1.32,~\n$ haddock68 &lt;dbl&gt; 74586, 79205, 53195, 36337, 49122, 81514, 172177, 160886, 96~\n\n# Визуализация важности переменных\n# Внимание: важности от RF — относительные; сопоставляйте с предметной логикой.\nvar_importance &lt;- randomForest(R3haddock ~ ., data = model_data, importance = TRUE)\nvarImpPlot(var_importance, main = \"Важность предикторов\")\n\n\n\n\n\n\n\n# Перед окончательным выбором модели мы проверяем значимость предикторов с помощью линейной регрессии.\n# Функция summary() показывает p-значения коэффициентов - если p &lt; 0.05, переменная считается статистически значимой. \n# Визуализация важности переменных с помощью случайного леса дает дополнительную перспективу,\n# показывая, какие переменные наиболее информативны для предсказания без предположений о линейности.\n\n# ==============================================================================\n#  ПОДГОТОВКА ДАННЫХ\n# Создаём NAOspring, фиксируем финальный набор признаков, сохраняем CSV.\n# ------------------------------------------------------------------------------\n# Цель блока: стандартизировать набор признаков для дальнейшего сравнения\n# моделей и обеспечить воспроизводимость (фиксированный CSV с нужными полями).\n# ==============================================================================\n\n# 1.1 Пакеты и окружение\n# Примечание: блок повторяет базовую инициализацию для автономного запуска.\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(readxl, tidyverse, caret, corrplot)\n\nrm(list = ls())\nset.seed(123)\nsetwd(\"C:/RECRUITMENT/\")\n\n# 1.2 Загрузка исходных данных и приведение типов\nDATA &lt;- readxl::read_excel(\"RECRUITMENT.xlsx\", sheet = \"RECRUITMENT\") %&gt;%\n  filter(YEAR &gt; 1989 & YEAR &lt; 2022) %&gt;%\n  mutate(\n    across(starts_with(\"T\"), as.numeric),\n    across(starts_with(\"I\"), as.numeric),\n    across(starts_with(\"O\"), as.numeric),\n    across(where(is.character), ~na_if(., \"NA\"))\n  )\n\n# 1.3 Создаём NAOspring (если есть NAO3, NAO4, NAO5)\n# Идея: агрегируем весенний индекс NAO как среднее за месяцы 3–5.\nif (all(c(\"NAO3\",\"NAO4\",\"NAO5\") %in% names(DATA))) {\n  DATA &lt;- DATA %&gt;%\n    mutate(NAOspring = rowMeans(pick(NAO3, NAO4, NAO5), na.rm = TRUE)) %&gt;%\n    select(-NAO3, -NAO4, -NAO5)\n}\n\n# NAO (North Atlantic Oscillation) - важный климатический индекс, влияющий описывающий изменения атмосферного давления\n# над Северной Атлантикой. В частности, он отражает разницу в атмосферном давлении между Исландской депрессией и\n# Азорским максимумом. NAO влияет на силу и направление западных ветров, а также на траектории штормов в Северной Атлантике. \n# Мы создаем NAOspring как среднее значение за весенние месяцы (марта, апреля, мая),\n# так как именно в этот период происходят ключевые процессы, влияющие на нерест трески. \n# Создание составных переменных на основе экспертных знаний часто улучшает качество моделей.\n\n# 1.4 Финальный учебный набор предикторов (фиксируем)\n# Важно: проверяем присутствие нужных колонок и формируем компактный датасет.\nneeded &lt;- c(\"codTSB\", \"T12\", \"I5\", \"NAOspring\", \"haddock68\")\nstopifnot(all(needed %in% names(DATA)))\n\n# Сохраняем YEAR в CSV (ниже он будет отброшен при обучении, но нужен для графика)\nmodel_data &lt;- DATA %&gt;%\n  select(YEAR, all_of(needed), R3haddock) %&gt;%\n  drop_na()\n\nwrite.csv(model_data, \"selected_predictors_dataset.csv\", row.names = FALSE)\nglimpse(model_data)\n\nRows: 32\nColumns: 7\n$ YEAR      &lt;dbl&gt; 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, ~\n$ codTSB    &lt;dbl&gt; 913000, 1347064, 1687381, 2197863, 2112773, 1849957, 1697388~\n$ T12       &lt;dbl&gt; 4.72, 4.66, 4.24, 3.90, 3.96, 4.27, 4.16, 4.07, 4.23, 5.08, ~\n$ I5        &lt;dbl&gt; 43, 55, 26, 49, 56, 28, 52, 51, 69, 68, 41, 48, 50, 63, 40, ~\n$ NAOspring &lt;dbl&gt; 0.64333333, 0.05666667, 1.78666667, 0.28666667, 0.61000000, ~\n$ haddock68 &lt;dbl&gt; 74586, 79205, 53195, 36337, 49122, 81514, 172177, 160886, 96~\n$ R3haddock &lt;dbl&gt; 812363, 389416, 99474, 98946, 118812, 63028, 147657, 83270, ~",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 7.html#модели-запас-пополнение-рикера-и-бивертона-холта",
    "href": "chapter 7.html#модели-запас-пополнение-рикера-и-бивертона-холта",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "8.3 Модели «запас-пополнение» Рикера и Бивертона-Холта",
    "text": "8.3 Модели «запас-пополнение» Рикера и Бивертона-Холта\nМодели запас-пополнение представляют собой фундаментальный инструмент в оценке водных биоресурсов, которые гораздо больше, чем просто математические кривые, — это формализованные выражения фундаментальных биологических представлений о том, как численность родительского стада определяет успех следующего поколения. Среди классических моделей этого типа наиболее широко используются модель Рикера и модель Бивертона-Холта, каждая из которых отражает различные гипотезы о биологических процессах, происходящих в популяции. Модель Рикера, предложенная Уильямом Рикером в 1954 году и имеющая характерный горб на графике, выражается уравнением R = a*S*exp(-b*S), где R обозначает пополнение, S — нерестовый запас, а параметры a и b имеют четкую биологическую интерпретацию: a соответствует максимальной продуктивности на единицу запаса при очень низких плотностях, фактически отражая максимальное пополнение (количество рекрутов) на одного производителя, а b характеризует степень плотностной зависимости, определяющей точку, после которой начинается снижение из-за внутривидовой конкуренции. Эта модель предсказывает, что с ростом нерестового запаса пополнение сначала увеличивается, достигает максимума, а затем снижается, что отражает явление перенаселенности, когда чрезмерная плотность производителей приводит к конкуренции за ресурсы, нехватке корма для личинок, усилению каннибализма или даже эпидемиям, что в итоге снижает выход молоди — мы буквально видим, как чрезмерный успех закладывает семена будущего коллапса пополнения. В отличие от нее, модель Бивертона-Холта, разработанная в 1957 году, имеет вид R = aS/(1+b*S) и предполагает, что пополнение асимптотически приближается к предельному значению a/b, называемому Rmax, с увеличением нерестового запаса, без последующего снижения, что соответствует ситуации, когда основной лимитирующий фактор — это не внутривидовая конкуренция, а внешние условия: ограниченное количество нерестовых площадок, хищничество, которое не зависит от плотности, или просто конечная пропускная способность экосистемы для молоди. Эта модель идеально описывает сценарий, когда кривая плавно выходит на плато, символизируя насыщение, и представляет собой альтернативную логику, где главным ограничивающим фактором являются внешние, а не внутривидовые процессы.\nПри оценке параметров этих нелинейных моделей мы сталкиваемся с необходимостью применения специализированных методов, поскольку обычный метод наименьших квадратов не справляется с их сложной структурой; в нашем анализе мы используем улучшенный алгоритм nlsLM из пакета minpack.lm, который сочетает метод Левенберга-Марквардта с возможностью наложения ограничений на параметры, что важно для обеспечения биологической правдоподобности результатов, так как параметры a и b должны оставаться положительными. Для получения надежных начальных оценок параметров в модели Рикера мы применяем функцию srStarts из пакета FSA, которая автоматически определяет разумные стартовые значения на основе анализа данных, тогда как для модели Бивертона-Холта мы используем комбинацию автоматических и ручных подходов, оценивая a как среднее отношение R/S при низких значениях запаса и устанавливая разумные начальные значения для b с последующей защитой от некорректных значений. Однако подбор модели — это только полдела, и критически важно провести тщательную диагностику, поскольку самая большая ошибка — слепо применять эти модели, не задумываясь об их предпосылках. Мы строим график остатков, потому что любая закономерность в их распределении — это сигнал о том, что модель не уловила какой-то важный процесс в данных. Мы смотрим на доверительные интервалы параметров; если они невероятно широки, значит, наша модель перепараметризована для имеющихся данных, и её прогностическая сила будет сомнительной. Модель Рикера не будет работать, если в вашей системе нет механизма перенаселения, а модель Бивертона-Холта окажется бесполезной, если пополнение продолжает расти или, наоборот, обрушивается после достижения пика. Именно поэтому мы всегда начинаем с простого графика «запас-пополнение» — его форма сама подскажет, какая из концепций более адекватна для конкретной популяции.\nНо реальный мир часто бывает сложнее этих двух идеализированных сценариев. Что если система ведёт себя по-рикеровски при высокой численности, но при низкой — работает иначе? Здесь на помощь приходит модификация — модель Рикера с порогом, или hockey-stick модель, которая сочетает в себе линейный рост при малых запасах и плато или спад при высоких, что может быть биологически более оправдано для многих запасов, находящихся под прессом промысла. И здесь мы подходим к самому главному — интеграции классики и современности. Эти модели не являются застывшими реликтами, а служат мощным инструментом для создания гипотез. Если модель Рикера плохо описывает данные, особенно в области низких значений запаса, это прямой сигнал о том, что возможно, существует какой-то дополнительный лимитирующий фактор, не учтенный в модели. Возможно, это температура воды на ключевой стадии развития икры, наличие хищников или доступность корма. Таким образом, классические модели становятся трамплином для более сложного анализа, включающего средовые предикторы. Мы можем включить параметры модели Рикера в качестве фиксированных эффектов в GAM или использовать предсказания классической модели в качестве одного из входных признаков для Random Forest. Этот синтез позволяет нам сохранить биологическую интерпретируемость классических моделей и добавить к ним гибкость и прогностическую силу машинного обучения для учета сложных, нелинейных влияний окружающей среды. В сущности, мы строим мост между глубоким, но узким знанием, заключенным в одной кривой, и широким, но зачастую “черно-ящичным” прогнозом сложного алгоритма, пытаясь получить лучшее из двух миров. Среди распространенных подводных камней при работе с моделями запас-пополнение следует отметить высокую чувствительность к начальным значениям параметров, что может приводить к сходимости к локальным минимумам, необходимость учета неоднородности дисперсии ошибок, особенно при работе с данными, охватывающими широкий диапазон значений запаса, и влияние временных лагов, поскольку пополнение в текущем году может зависеть не только от нерестового запаса в том же году, но и от условий прошлых лет. Кроме того, чистые модели запас-пополнение часто оказываются недостаточными для точного прогнозирования, так как пополнение зависит не только от размера нерестового запаса, но и от множества экологических факторов, что делает целесообразным развитие этих моделей в направлении включения дополнительных предикторов, как это продемонстрировано в последующих разделах нашего анализа. Тем не менее, классические модели Рикера и Бивертона-Холта остаются важной отправной точкой в анализе динамики рыбных популяций, предоставляя интерпретируемую основу для понимания механизмов регулирования численности и служа эталоном для оценки добавленной ценности более сложных моделей, что особенно важно в условиях ограниченных данных, характерных для многих водных экосистем.\n\n# ==============================================================================\n# ПРАКТИЧЕСКОЕ ЗАНЯТИЕ: АНАЛИЗ ФАКТОРОВ, ВЛИЯЮЩИХ НА ПОПОЛНЕНИЕ \n# (КЛАССИЧЕСКИЕ МОДЕЛИ ЗАПАС-ПОПОЛНЕНИЕ)\n# Курс: \"Оценка водных биоресурсов в среде R (для начинающих)\"\n# ==============================================================================\n\n# Установка и подключение ТОЛЬКО необходимых библиотек\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,    # Манипуляции с данными и визуализация\n  FSA,          # Начальные оценки для моделей запас-пополнение\n  minpack.lm,   # Улучшенный алгоритм нелинейной регрессии (nlsLM)\n  car,          # Проверка допущений моделей\n  mgcv,         # Построение GAM-моделей\n  investr,      # Доверительные интервалы для нелинейных моделей\n   caret)       # Расчет RMSE\n\n# Очистка среды и установка рабочей директории\nrm(list = ls())\nsetwd(\"C:/RECRUITMENT/\")\n\n# 1. ЗАГРУЗКА ДАННЫХ -----------------------------------------------------------\nmodel_data &lt;- read.csv(\"selected_predictors_dataset.csv\", \n                      header = TRUE, \n                      stringsAsFactors = FALSE)\n\n# Проверка структуры данных\nstr(model_data)\n\n'data.frame':   32 obs. of  7 variables:\n $ YEAR     : int  1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 ...\n $ codTSB   : int  913000 1347064 1687381 2197863 2112773 1849957 1697388 1537459 1350918 1199169 ...\n $ T12      : num  4.72 4.66 4.24 3.9 3.96 4.27 4.16 4.07 4.23 5.08 ...\n $ I5       : int  43 55 26 49 56 28 52 51 69 68 ...\n $ NAOspring: num  0.6433 0.0567 1.7867 0.2867 0.61 ...\n $ haddock68: int  74586 79205 53195 36337 49122 81514 172177 160886 96380 37977 ...\n $ R3haddock: int  812363 389416 99474 98946 118812 63028 147657 83270 359701 386866 ...\n\n# Проверка на пропущенные значения (должно быть 0 после предобработки)\nsum(is.na(model_data))\n\n[1] 0\n\n# 2. ПОДГОТОВКА ДАННЫХ ДЛЯ МОДЕЛЕЙ ЗАПАС-ПОПОЛНЕНИЕ ----------------------------\nrec_data &lt;- data.frame(\n  S = model_data$haddock68,  # Нерестовый запас\n  R = model_data$R3haddock   # Пополнение\n)\n\n# 3. ПОДГОНКА МОДЕЛИ РИКЕРА ----------------------------------------------------\nricker_starts &lt;- FSA::srStarts(R ~ S, data = rec_data, type = \"Ricker\")\nricker_model &lt;- minpack.lm::nlsLM(\n  R ~ a * S * exp(-b * S),\n  data = rec_data,\n  start = ricker_starts,\n  lower = c(a = 0, b = 0)\n)\n\n# 4. ПОДГОНКА МОДЕЛИ БИВЕРТОНА-ХОЛТА -------------------------------------------\na_start &lt;- mean(rec_data$R[rec_data$S &lt; quantile(rec_data$S, 0.25)] / \n                rec_data$S[rec_data$S &lt; quantile(rec_data$S, 0.25)], na.rm = TRUE)\n\nif (is.na(a_start) || a_start &lt;= 0) a_start &lt;- 0.001\n\nbh_model &lt;- minpack.lm::nlsLM(\n  R ~ (a * S) / (1 + b * S),\n  data = rec_data,\n  start = list(a = a_start, b = 0.0001),\n  lower = c(a = 0.0001, b = 0.00001),\n  control = nls.lm.control(maxiter = 200)\n)\n\n# 5. ОЦЕНКА КАЧЕСТВА МОДЕЛЕЙ ---------------------------------------------------\n\ncalculate_R2 &lt;- function(model, data) {\n  predicted &lt;- predict(model, newdata = data)\n  residuals &lt;- data$R - predicted\n  SSE &lt;- sum(residuals^2)\n  SST &lt;- sum((data$R - mean(data$R))^2)\n  R2 &lt;- 1 - (SSE / SST)\n  n &lt;- nrow(data)\n  p &lt;- length(coef(model))\n  adj_R2 &lt;- 1 - ((n - 1) / (n - p - 1)) * (1 - R2)\n  return(list(R2 = R2, adj_R2 = adj_R2))\n}\n\ncalculate_pvalue &lt;- function(model, data) {\n  predicted &lt;- predict(model, newdata = data)\n  residuals &lt;- data$R - predicted\n  SSE &lt;- sum(residuals^2)\n  SST &lt;- sum((data$R - mean(data$R))^2)\n  SSR &lt;- SST - SSE\n  n &lt;- nrow(data)\n  p &lt;- length(coef(model))\n  F_stat &lt;- (SSR / (p - 1)) / (SSE / (n - p))\n  p_value &lt;- pf(F_stat, df1 = p - 1, df2 = n - p, lower.tail = FALSE)\n  return(p_value)\n}\n\nricker_r2 &lt;- calculate_R2(ricker_model, rec_data)\nbh_r2 &lt;- calculate_R2(bh_model, rec_data)\n\nricker_p &lt;- calculate_pvalue(ricker_model, rec_data)\nbh_p &lt;- calculate_pvalue(bh_model, rec_data)\n\ncat(\"AIC Рикера:\", AIC(ricker_model), \"\\n\")\n\nAIC Рикера: 891.9919 \n\ncat(\"AIC Бивертона-Холта:\", AIC(bh_model), \"\\n\")\n\nAIC Бивертона-Холта: 894.2029 \n\n# 6. ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ --------------------------------------------------\nnew_data &lt;- data.frame(S = seq(min(rec_data$S), max(rec_data$S), length.out = 100))\nricker_ci &lt;- investr::predFit(ricker_model, newdata = new_data, interval = \"confidence\")\nbh_ci &lt;- investr::predFit(bh_model, newdata = new_data, interval = \"confidence\")\n\nplot_data &lt;- new_data %&gt;%\n  mutate(\n    ricker_pred = predict(ricker_model, newdata = .),\n    ricker_lwr = ricker_ci[, \"lwr\"],\n    ricker_upr = ricker_ci[, \"upr\"],\n    bh_pred = predict(bh_model, newdata = .),\n    bh_lwr = bh_ci[, \"lwr\"],\n    bh_upr = bh_ci[, \"upr\"]\n  )\n\nggplot() +\n  geom_point(data = rec_data, aes(x = S, y = R), \n             color = \"darkgray\", size = 3, alpha = 0.7) +\n  geom_ribbon(data = plot_data, aes(x = S, ymin = ricker_lwr, ymax = ricker_upr), \n              fill = \"red\", alpha = 0.2) +\n  geom_ribbon(data = plot_data, aes(x = S, ymin = bh_lwr, ymax = bh_upr), \n              fill = \"blue\", alpha = 0.2) +\n  geom_line(data = plot_data, aes(x = S, y = ricker_pred), \n            color = \"red\", linewidth = 1.2) +\n  geom_line(data = plot_data, aes(x = S, y = bh_pred), \n            color = \"blue\", linewidth = 1.2, linetype = \"dashed\") +\n  labs(\n    title = \"Сравнение моделей запас-пополнение\",\n    subtitle = paste0(\n      \"Рикер: R² = \", round(ricker_r2$R2, 2), \", p = \", format.pval(ricker_p, digits = 3),\n      \" | Бивертон-Холт: R² = \", round(bh_r2$R2, 2), \", p = \", format.pval(bh_p, digits = 3)\n    ),\n    x = \"Нерестовый запас\",\n    y = \"Пополнение\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n# 7. СРАВНЕНИЕ С ДРУГИМИ ТИПАМИ МОДЕЛЕЙ ----------------------------------------\n\nlm_model &lt;- lm(R3haddock ~ ., data = model_data)\nglm_model &lt;- glm(R3haddock ~ ., family = Gamma(link = \"log\"), data = model_data)\ngam_model &lt;- mgcv::gam(R3haddock ~ s(haddock68) + s(codTSB) + s(T12) + s(I5) + s(NAOspring),\n               data = model_data, method = \"REML\")\n\nmodel_comparison &lt;- data.frame(\n  Model = c(\"Рикер\", \"Бивертон-Холт\", \"LM\", \"GLM\", \"GAM\"),\n  AIC = c(AIC(ricker_model), AIC(bh_model), AIC(lm_model), AIC(glm_model), AIC(gam_model)),\n  R2 = c(\n    ricker_r2$R2, \n    bh_r2$R2, \n    summary(lm_model)$r.squared,\n    cor(model_data$R3haddock, predict(glm_model, type = \"response\"))^2,\n    summary(gam_model)$r.sq\n  )\n)\n\nprint(model_comparison)\n\n          Model      AIC          R2\n1         Рикер 891.9919 0.072305265\n2 Бивертон-Холт 894.2029 0.005938625\n3            LM 877.0895 0.573972535\n4           GLM 857.0346 0.566389771\n5           GAM 862.9064 0.738660678\n\n# 8. ИНТЕРПРЕТАЦИЯ РЕЗУЛЬТАТОВ -------------------------------------------------\ncat(\"\\nПараметры модели Рикера:\")\n\n\nПараметры модели Рикера:\n\ncat(\"\\na =\", coef(ricker_model)[1], \"- максимальная продукция потомства\")\n\n\na = 7.931302 - максимальная продукция потомства\n\ncat(\"\\nb =\", coef(ricker_model)[2], \"- коэффициент плотностной зависимости\")\n\n\nb = 7.4007e-06 - коэффициент плотностной зависимости\n\ncat(\"\\n\\nПараметры модели Бивертона-Холта:\")\n\n\n\nПараметры модели Бивертона-Холта:\n\ncat(\"\\na =\", coef(bh_model)[1], \"- максимальное пополнение на особь\")\n\n\na = 43.77667 - максимальное пополнение на особь\n\ncat(\"\\nb =\", coef(bh_model)[2], \"- коэффициент внутривидовой конкуренции\")\n\n\nb = 0.0001247403 - коэффициент внутривидовой конкуренции\n\n# ==============================================================================\n# 7. СРАВНЕНИЕ С ДРУГИМИ ТИПАМИ МОДЕЛЕЙ ----------------------------------------\n\n# Построение линейной модели LM\nlm_model &lt;- lm(R3haddock ~ ., data = model_data)\n\n# Диагностика\npar(mfrow = c(2, 2))\nplot(lm_model)\n\n\n\n\n\n\n\nvif(lm_model)  # Проверка мультиколлинеарности\n\n     YEAR    codTSB       T12        I5 NAOspring haddock68 \n 2.925549  3.439340  1.970023  1.332529  1.181897  2.522569 \n\n# Интерпретация\nsummary(lm_model)\n\n\nCall:\nlm(formula = R3haddock ~ ., data = model_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-279162 -111056  -35757  141083  324173 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.448e+07  1.224e+07   1.183 0.248123    \nYEAR        -7.863e+03  6.248e+03  -1.258 0.219869    \ncodTSB      -1.888e-01  7.817e-02  -2.416 0.023338 *  \nT12          4.339e+05  9.738e+04   4.456 0.000153 ***\nI5          -2.568e+03  3.058e+03  -0.840 0.409036    \nNAOspring   -7.666e+04  5.735e+04  -1.337 0.193325    \nhaddock68    2.782e-01  5.485e-01   0.507 0.616444    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 190800 on 25 degrees of freedom\nMultiple R-squared:  0.574, Adjusted R-squared:  0.4717 \nF-statistic: 5.614 on 6 and 25 DF,  p-value: 0.0008393\n\n# Построение обобщенной линейной модели GLM\nglm_model &lt;- glm(R3haddock ~ ., \n                family = Gamma(link = \"log\"), \n                data = model_data)\nsummary(glm_model)\n\n\nCall:\nglm(formula = R3haddock ~ ., family = Gamma(link = \"log\"), data = model_data)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.375e+01  3.667e+01   0.648   0.5231    \nYEAR        -8.601e-03  1.872e-02  -0.459   0.6499    \ncodTSB      -5.945e-07  2.342e-07  -2.539   0.0177 *  \nT12          1.411e+00  2.917e-01   4.837 5.68e-05 ***\nI5           7.430e-03  9.161e-03   0.811   0.4250    \nNAOspring    1.508e-02  1.718e-01   0.088   0.9307    \nhaddock68    9.422e-07  1.643e-06   0.573   0.5715    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.326724)\n\n    Null deviance: 19.8880  on 31  degrees of freedom\nResidual deviance:  8.4535  on 25  degrees of freedom\nAIC: 857.03\n\nNumber of Fisher Scoring iterations: 9\n\n# Построение обобщенной аддитивной модели GАM\nlibrary(mgcv)\ngam_model &lt;- gam(R3haddock ~ \n                 s(codTSB) + \n                 s(T12) + \n                 s(I5) + \n                 s(NAOspring) + \n                 s(haddock68),\n               data = model_data,\n               method = \"REML\")\nsummary(gam_model)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nR3haddock ~ s(codTSB) + s(T12) + s(I5) + s(NAOspring) + s(haddock68)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   320163      23724   13.49 4.37e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n               edf Ref.df     F p-value   \ns(codTSB)    2.354  2.899 1.684 0.17581   \ns(T12)       2.190  2.676 5.908 0.00357 **\ns(I5)        4.642  5.539 1.518 0.15004   \ns(NAOspring) 1.293  1.503 1.154 0.22854   \ns(haddock68) 1.824  2.200 0.629 0.65124   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.739   Deviance explained = 84.2%\n-REML = 361.62  Scale est. = 1.8011e+10  n = 32\n\nplot(gam_model, pages = 1, residuals = TRUE)\n\n\n\n\n\n\n\n# Таблица сравнения моделей\n# Сравнение моделей\nmodel_comparison &lt;- data.frame(\n  Model = c(\"Рикер\", \"Бивертон-Холт\", \"LM\", \"GLM\", \"GAM\"),\n  AIC = c(AIC(ricker_model), AIC(bh_model), AIC(lm_model), AIC(glm_model), AIC(gam_model)),\n  R2 = c(ricker_r2$R2, bh_r2$R2, summary(lm_model)$r.squared, \n         cor(model_data$R3haddock, predict(glm_model))^2, \n         summary(gam_model)$r.sq),  # Используем summary(gam_model)$r.sq для R^2\n  Adj_R2 = c(ricker_r2$adj_R2, bh_r2$adj_R2, summary(lm_model)$adj.r.squared, NA, \n             summary(gam_model)$r.sq),  # Используем summary(gam_model)$r.sq для Adjusted R^2\n  RMSE = c(RMSE(predict(ricker_model), rec_data$R), \n           RMSE(predict(bh_model), rec_data$R),\n           RMSE(predict(lm_model), model_data$R3haddock),\n           RMSE(predict(glm_model, type = \"response\"), model_data$R3haddock),\n           RMSE(predict(gam_model, type = \"response\"), model_data$R3haddock))\n)\n\n# Вывод таблицы\nprint(model_comparison)\n\n          Model      AIC          R2       Adj_R2     RMSE\n1         Рикер 891.9919 0.072305265  0.008326318 248869.6\n2 Бивертон-Холт 894.2029 0.005938625 -0.062617332 257617.8\n3            LM 877.0895 0.573972535  0.471725944 168650.7\n4           GLM 857.0346 0.502625978           NA 170386.6\n5           GAM 862.9064 0.738660678  0.738660678 102584.0\n\n# ==============================================================================\n# ВИЗУАЛИЗАЦИЯ ВСЕХ МОДЕЛЕЙ НА ОДНОМ ГРАФИКЕ \n# ==============================================================================\n\n# Фиксируем другие предикторы на их средних значениях (исключая haddock68)\nmean_values &lt;- model_data %&gt;%\n  select(-R3haddock, -haddock68) %&gt;%\n  summarise(across(everything(), ~ mean(.x, na.rm = TRUE)))\n\n# Расширяем new_data, добавляя средние значения других предикторов\nnew_data_full &lt;- new_data %&gt;%\n  bind_cols(mean_values[rep(1, nrow(new_data)), ]) %&gt;%\n  rename(haddock68 = S)  # Переименовываем S в haddock68 для совместимости\n\n# Получаем предсказания для всех моделей\nnew_data_full &lt;- new_data_full %&gt;%\n  mutate(\n    # Предсказания для моделей запаса-пополнения\n    ricker_pred = predict(ricker_model, newdata = data.frame(S = haddock68)),\n    bh_pred = predict(bh_model, newdata = data.frame(S = haddock68)),\n    \n    # Предсказания для линейной модели (LM)\n    lm_pred = predict(lm_model, newdata = .),\n    \n    # Предсказания для обобщенной линейной модели (GLM)\n    glm_pred = predict(glm_model, newdata = ., type = \"response\"),\n    \n    # Предсказания для обобщенной аддитивной модели (GAM)\n    gam_pred = predict(gam_model, newdata = ., type = \"response\")\n  )\n\n# Создаем длинный формат данных для ggplot\nplot_data &lt;- new_data_full %&gt;%\n  select(haddock68, ricker_pred, bh_pred, lm_pred, glm_pred, gam_pred) %&gt;%\n  pivot_longer(\n    cols = -haddock68,\n    names_to = \"model\",\n    values_to = \"prediction\"\n  ) %&gt;%\n  mutate(\n    model = case_when(\n      model == \"ricker_pred\" ~ \"Рикер\",\n      model == \"bh_pred\" ~ \"Бивертон-Холт\",\n      model == \"lm_pred\" ~ \"Линейная (LM)\",\n      model == \"glm_pred\" ~ \"Обобщенная линейная (GLM)\",\n      model == \"gam_pred\" ~ \"Обобщенная аддитивная (GAM)\",\n      TRUE ~ model\n    )\n  )\n\n# Создаем палитру цветов для моделей\nmodel_colors &lt;- c(\n  \"Рикер\" = \"#E41A1C\",          # Красный\n  \"Бивертон-Холт\" = \"#377EB8\",  # Синий\n  \"Линейная (LM)\" = \"#4DAF4A\",  # Зеленый\n  \"Обобщенная линейная (GLM)\" = \"#984EA3\", # Фиолетовый\n  \"Обобщенная аддитивная (GAM)\" = \"#FF7F00\" # Оранжевый\n)\n\n# Создаем график\nggplot() +\n  # Точки исходных данных\n  geom_point(data = rec_data, aes(x = S, y = R), \n             color = \"darkgray\", size = 2.5, alpha = 0.7) +\n  \n  # Линии предсказаний моделей\n  geom_line(data = plot_data, \n            aes(x = haddock68, y = prediction, color = model, linetype = model),\n            linewidth = 1.2) +\n  \n  # Настройка цветов и типов линий\n  scale_color_manual(values = model_colors) +\n  scale_linetype_manual(values = c(\n    \"Рикер\" = \"solid\",\n    \"Бивертон-Холт\" = \"dashed\",\n    \"Линейная (LM)\" = \"dotdash\",\n    \"Обобщенная линейная (GLM)\" = \"longdash\",\n    \"Обобщенная аддитивная (GAM)\" = \"twodash\"\n  )) +\n  \n  # Подписи и темы\n  labs(\n    title = \"Сравнение моделей зависимости пополнения от нерестового запаса\",\n    subtitle = \"Фиксация других предикторов на средних значениях\",\n    x = \"Нерестовый запас (тыс. тонн)\",\n    y = \"Пополнение (млн особей)\",\n    color = \"Модель\",\n    linetype = \"Модель\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray30\"),\n    axis.title = element_text(size = 12),\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\",\n    legend.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA, linewidth = 0.5)\n  ) +\n  guides(\n    color = guide_legend(nrow = 2, byrow = TRUE),\n    linetype = guide_legend(nrow = 2, byrow = TRUE)\n  )",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 7.html#статистические-модели-lmglmgam",
    "href": "chapter 7.html#статистические-модели-lmglmgam",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "8.4 Статистические модели LM/GLM/GAM",
    "text": "8.4 Статистические модели LM/GLM/GAM\nСтатистические модели линейной регрессии (LM), обобщенной линейной регрессии (GLM) и обобщенной аддитивной регрессии (GAM) представляют собой мощный и взаимодополняющий набор инструментов для анализа водных биоресурсов, позволяющий исследователям от простых линейных зависимостей переходить к сложным нелинейным взаимодействиям, сохраняя при этом интерпретируемость результатов. Линейная модель (LM) служит фундаментом для всего статистического анализа в гидробиологии, основываясь на предположении, что зависимость между предикторами и откликом является линейной, а остатки распределены нормально с постоянной дисперсией. Эта модель предоставляет простую интерпретацию коэффициентов как величины изменения отклика при единичном изменении предиктора, что особенно ценно при работе с такими биологическими показателями, как пополнение запаса или нерестовая биомасса. Однако при анализе водных биоресурсов мы часто сталкиваемся с данными, которые нарушают ключевые предположения LM: пополнение рыбы или беспозвоночного не может быть отрицательным, его распределение обычно сильно скошено вправо, а дисперсия часто увеличивается с ростом среднего значения. Именно здесь на помощь приходит обобщенная линейная модель (GLM), расширяющая возможности LM за счет введения двух ключевых компонентов — экспоненциального семейства распределений и связующей функции (link-function). Для данных о рыбных запасах особенно полезно Gamma-распределение с логарифмической связкой, которое учитывает положительность отклика и мультипликативную природу ошибок, характерную для биологических данных. В отличие от LM, где мы интерпретируем коэффициенты как абсолютные изменения, в GLM с лог-связкой коэффициенты отражают относительные изменения: увеличение предиктора на единицу приводит к умножению ожидаемого отклика на exp(коэффициент), что соответствует биологической реальности, где эффекты часто действуют мультипликативно, а не аддитивно. Но даже GLM сохраняет ограничение на линейность в преобразованном пространстве, что может быть недостаточным для описания сложных экологических зависимостей, таких как оптимальный диапазон температуры для нереста или пороговые эффекты средовых факторов. Здесь в игру вступают обобщенные аддитивные модели (GAM), которые заменяют линейные комбинации предикторов на гладкие функции, оцениваемые с помощью сплайнов, что позволяет моделировать практически любые нелинейные зависимости без предварительного задания их формы. GAM сохраняет интерпретируемость линейных моделей, так как каждая гладкая функция может быть визуализирована и проанализирована отдельно, показывая, как именно каждый фактор влияет на пополнение запаса, будь то монотонный рост, оптимум с максимумом или сложная колебательная зависимость. При работе с GAM особое внимание уделяется выбору степени гладкости, так как чрезмерно гибкие функции могут переобучиться на шум в данных, тогда как недостаточно гибкие не уловят реальные биологические закономерности; в пакете mgcv это решается автоматически через метод максимального правдоподобия с штрафом (REML), который балансирует качество подгонки и гладкость функций. Сравнивая эти модели с классическими моделями запас-пополнение, мы видим, что GAM может рассматриваться как их естественное обобщение: вместо фиксированной формы кривой Рикера или Бивертона-Холта GAM позволяет данным “говорить за себя”, выявляя оптимальную форму зависимости без предварительных гипотез, при этом сохраняя возможность включить нерестовый запас как один из гладких членов в модель, дополненный другими экологическими факторами. Однако при всей своей гибкости, GAM, как и LM с GLM, требует тщательной проверки предположений: мы анализируем графики остатков против предсказанных значений, чтобы убедиться в отсутствии систематических отклонений, проверяем нормальность остатков (для LM) или соответствие выбранному распределению (для GLM/GAM), и исследуем влияние влиятельных точек, которые могут исказить результаты, особенно в условиях ограниченных данных, характерных для гидробиологических исследований. Выбор между LM, GLM и GAM должен основываться не только на статистических критериях, таких как AIC или кросс-валидация, но и на биологической интерпретируемости результатов: иногда более простая модель с меньшей точностью предпочтительнее сложного “черного ящика”, особенно когда результаты должны быть понятны начальникам и менеджерам рыболовства. Практический подход, который обычно рекомендуется начинающим ихтиологам/гидробиологам, состоит в последовательном усложнении модели: начните с классической модели запас-пополнение, затем добавьте средовые факторы через LM/GLM, и только если зависимости явно нелинейны, перейдите к GAM, всегда проверяя, действительно ли усложнение модели приводит к биологически значимому улучшению понимания процесса. Важно помнить, что статистическая модель — это не самоцель, а инструмент для понимания биологических процессов, и даже самая изощренная модель бесполезна, если её результаты нельзя перевести на язык биологии и применить для устойчивого управления водными ресурсами. В конечном счете, сочетание классических представлений об экосистемах с современными статистическими методами позволяет нам строить мост между фундаментальной биологией и прикладной оценкой запасов, где каждая модель, от простой линейной регрессии до сложного GAM, вносит свой вклад в формирование целостного понимания динамики водных биоресурсов.\n\n# ==============================================================================\n# Версия: только LM / GLM(Gamma) / GAM\n# Без caret/train: стандартная оценка параметров lm/glm/gam, собственная time-slice CV,\n# выбор лучшей модели, прогноз 2022–2024, эмпирические интервалы и график.\n# ==============================================================================\n\n# 0) Пакеты и окружение --------------------------------------------------------\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  readxl, tidyverse, mgcv, lmtest, car, ggplot2, corrplot\n)\n\nrm(list = ls())\nset.seed(123)\nsetwd(\"C:/RECRUITMENT/\")  # при необходимости измените путь\n\n\n# 1) Подготовка данных ---------------------------------------------------------\n# Загрузка, приведение типов, создание NAOspring, фиксируем набор признаков\nDATA &lt;- readxl::read_excel(\"RECRUITMENT.xlsx\", sheet = \"RECRUITMENT\") %&gt;%\n  filter(YEAR &gt; 1989 & YEAR &lt; 2022) %&gt;%\n  mutate(\n    across(starts_with(\"T\"), as.numeric),\n    across(starts_with(\"I\"), as.numeric),\n    across(starts_with(\"O\"), as.numeric),\n    across(where(is.character), ~na_if(., \"NA\"))\n  )\n\nif (all(c(\"NAO3\",\"NAO4\",\"NAO5\") %in% names(DATA))) {\n  DATA &lt;- DATA %&gt;%\n    mutate(NAOspring = rowMeans(pick(NAO3, NAO4, NAO5), na.rm = TRUE)) %&gt;%\n    select(-NAO3, -NAO4, -NAO5)\n}\n\nneeded &lt;- c(\"codTSB\", \"T12\", \"I5\", \"NAOspring\", \"haddock68\")\nstopifnot(all(needed %in% names(DATA)))\n\nmodel_data &lt;- DATA %&gt;%\n  select(YEAR, all_of(needed), R3haddock) %&gt;%\n  drop_na() %&gt;%\n  arrange(YEAR)\n\nwrite.csv(model_data, \"selected_predictors_dataset.csv\", row.names = FALSE)\nglimpse(model_data)\n\nRows: 32\nColumns: 7\n$ YEAR      &lt;dbl&gt; 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, ~\n$ codTSB    &lt;dbl&gt; 913000, 1347064, 1687381, 2197863, 2112773, 1849957, 1697388~\n$ T12       &lt;dbl&gt; 4.72, 4.66, 4.24, 3.90, 3.96, 4.27, 4.16, 4.07, 4.23, 5.08, ~\n$ I5        &lt;dbl&gt; 43, 55, 26, 49, 56, 28, 52, 51, 69, 68, 41, 48, 50, 63, 40, ~\n$ NAOspring &lt;dbl&gt; 0.64333333, 0.05666667, 1.78666667, 0.28666667, 0.61000000, ~\n$ haddock68 &lt;dbl&gt; 74586, 79205, 53195, 36337, 49122, 81514, 172177, 160886, 96~\n$ R3haddock &lt;dbl&gt; 812363, 389416, 99474, 98946, 118812, 63028, 147657, 83270, ~\n\n# 2) Формулы моделей и вспомогательные функции --------------------------------\nf_lm  &lt;- as.formula(\"R3haddock ~ codTSB + T12 + I5 + NAOspring + haddock68\")\nf_gam &lt;- as.formula(\"R3haddock ~ s(codTSB,bs='tp',k=5) + s(T12,bs='tp',k=5) + s(I5,bs='tp',k=5) + s(NAOspring,bs='tp',k=5) + s(haddock68,bs='tp',k=5)\")\n\nrmse &lt;- function(a, p) sqrt(mean((a - p)^2, na.rm = TRUE))\nmae  &lt;- function(a, p) mean(abs(a - p), na.rm = TRUE)\nr2   &lt;- function(a, p) 1 - sum((a - p)^2, na.rm = TRUE) / sum((a - mean(a))^2, na.rm = TRUE)\n\nsafe_fit &lt;- function(expr) {\n  out &lt;- try(eval(expr), silent = TRUE)\n  if (inherits(out, \"try-error\")) NULL else out\n}\n\n\n# 3) Time-slice CV (expanding window, h=3) + хронологический тест -------------\nmd &lt;- model_data\nmd_for_fit &lt;- md %&gt;% select(codTSB, T12, I5, NAOspring, haddock68, R3haddock)\n\nn &lt;- nrow(md_for_fit)\nholdout_frac &lt;- 0.2\nn_test &lt;- max(4, ceiling(n * holdout_frac))\ntrain_ts &lt;- head(md_for_fit, n - n_test)\ntest_ts  &lt;- tail(md_for_fit, n_test)\n\nn_train &lt;- nrow(train_ts)\ninitial_frac &lt;- 0.6\nhorizon      &lt;- 3\ninitialWindow &lt;- max(10, floor(initial_frac * n_train))\nif (initialWindow + horizon &gt; n_train) initialWindow &lt;- n_train - horizon\n\n# Аккумулируем метрики и остатки по срезам\ncv_summ &lt;- tibble(Model = character(), RMSE = double(), MAE = double())\nresids_cv &lt;- list(LM = c(), GLM = c(), GAM = c())\n\nslice_id &lt;- 0\nfor (i in seq(initialWindow, n_train - horizon)) {\n  slice_id &lt;- slice_id + 1\n  idx_tr &lt;- 1:i\n  idx_te &lt;- (i+1):(i+horizon)\n  dtr &lt;- train_ts[idx_tr, ]\n  dte &lt;- train_ts[idx_te, ]\n\n  # LM\n  lm_fit &lt;- safe_fit(quote(lm(f_lm, data = dtr)))\n  if (!is.null(lm_fit)) {\n    pr &lt;- try(predict(lm_fit, newdata = dte), silent = TRUE)\n    if (!inherits(pr, \"try-error\")) {\n      cv_summ &lt;- add_row(cv_summ, Model = \"LM\", RMSE = rmse(dte$R3haddock, pr), MAE = mae(dte$R3haddock, pr))\n      resids_cv$LM &lt;- c(resids_cv$LM, dte$R3haddock - pr)\n    }\n  }\n\n  # GLM (Gamma)\n  glm_fit &lt;- safe_fit(quote(glm(f_lm, data = dtr, family = Gamma(link = \"log\"))))\n  if (!is.null(glm_fit)) {\n    pr &lt;- try(predict(glm_fit, newdata = dte, type = \"response\"), silent = TRUE)\n    if (!inherits(pr, \"try-error\")) {\n      cv_summ &lt;- add_row(cv_summ, Model = \"GLM\", RMSE = rmse(dte$R3haddock, pr), MAE = mae(dte$R3haddock, pr))\n      resids_cv$GLM &lt;- c(resids_cv$GLM, dte$R3haddock - pr)\n    }\n  }\n\n  # GAM (Gamma log), ограничиваем сложность k для стабильности на малом n\n  gam_fit &lt;- safe_fit(quote(mgcv::gam(f_gam, data = dtr, family = Gamma(link = \"log\"), method = \"REML\", select = TRUE)))\n  if (!is.null(gam_fit)) {\n    pr &lt;- try(predict(gam_fit, newdata = dte, type = \"response\"), silent = TRUE)\n    if (!inherits(pr, \"try-error\")) {\n      cv_summ &lt;- add_row(cv_summ, Model = \"GAM\", RMSE = rmse(dte$R3haddock, pr), MAE = mae(dte$R3haddock, pr))\n      resids_cv$GAM &lt;- c(resids_cv$GAM, dte$R3haddock - pr)\n    }\n  }\n}\n\n# Средние метрики по моделям\ncv_rank &lt;- cv_summ %&gt;% group_by(Model) %&gt;% summarise(RMSE = mean(RMSE, na.rm = TRUE), MAE = mean(MAE, na.rm = TRUE), .groups = \"drop\") %&gt;% arrange(RMSE, MAE)\nprint(cv_rank)\n\n# A tibble: 3 x 3\n  Model    RMSE     MAE\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 GLM   280259. 237187.\n2 LM    370298. 340884.\n3 GAM   504613. 432062.\n\nbest_model_name &lt;- cv_rank$Model[1]\ncat(sprintf(\"\\nЛучшая модель по time-slice CV: %s\\n\", best_model_name))\n\n\nЛучшая модель по time-slice CV: GLM\n\n# Хронологический тест: обучаем на всём train_ts, прогнозируем на test_ts\nfit_on &lt;- function(model_name, data) {\n  if (model_name == \"LM\") return(lm(f_lm, data = data))\n  if (model_name == \"GLM\") return(glm(f_lm, data = data, family = Gamma(link = \"log\")))\n  mgcv::gam(f_gam, data = data, family = Gamma(link = \"log\"), method = \"REML\", select = TRUE)\n}\n\npredict_on &lt;- function(fit, newdata, model_name) {\n  if (model_name == \"GLM\") return(predict(fit, newdata = newdata, type = \"response\"))\n  if (inherits(fit, \"gam\")) return(predict(fit, newdata = newdata, type = \"response\"))\n  predict(fit, newdata = newdata)\n}\n\nfit_train &lt;- fit_on(best_model_name, train_ts)\npred_te   &lt;- predict_on(fit_train, test_ts, best_model_name)\ntest_metrics &lt;- tibble(\n  Model = best_model_name,\n  RMSE  = rmse(test_ts$R3haddock, pred_te),\n  MAE   = mae (test_ts$R3haddock, pred_te),\n  R2    = r2  (test_ts$R3haddock, pred_te)\n)\nprint(test_metrics)\n\n# A tibble: 1 x 4\n  Model    RMSE     MAE    R2\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 GLM   182048. 141692. 0.363\n\n# 4) Диагностика моделей (подгонка на всех данных до 2021) -------------------\nfull_fit_df &lt;- md_for_fit\n\nlm_full  &lt;- lm(f_lm,  data = full_fit_df)\nglm_full &lt;- glm(f_lm, data = full_fit_df, family = Gamma(link = \"log\"))\ngam_full &lt;- mgcv::gam(f_gam, data = full_fit_df, family = Gamma(link = \"log\"), method = \"REML\", select = TRUE)\n\ncat(\"\\n[LM] Сводка:\\n\"); print(summary(lm_full))\n\n\n[LM] Сводка:\n\n\n\nCall:\nlm(formula = f_lm, data = full_fit_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-257877 -155326  -18935  101135  326940 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.189e+05  4.459e+05  -2.061 0.049455 *  \ncodTSB      -2.406e-01  6.722e-02  -3.579 0.001386 ** \nT12          3.679e+05  8.296e+04   4.435 0.000149 ***\nI5          -1.770e+03  3.025e+03  -0.585 0.563536    \nNAOspring   -5.125e+04  5.427e+04  -0.944 0.353710    \nhaddock68    4.385e-01  5.395e-01   0.813 0.423698    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 192900 on 26 degrees of freedom\nMultiple R-squared:  0.547, Adjusted R-squared:  0.4599 \nF-statistic: 6.279 on 5 and 26 DF,  p-value: 0.0006042\n\ncat(\"\\n[LM] VIF:\\n\"); print(car::vif(lm_full))\n\n\n[LM] VIF:\n\n\n   codTSB       T12        I5 NAOspring haddock68 \n 2.487391  1.398254  1.275233  1.035349  2.386554 \n\ncat(\"\\n[LM] Breusch–Pagan:\\n\"); print(lmtest::bptest(lm_full))\n\n\n[LM] Breusch–Pagan:\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  lm_full\nBP = 5.1481, df = 5, p-value = 0.3981\n\ncat(\"\\n[LM] Durbin–Watson:\\n\"); print(lmtest::dwtest(lm_full))\n\n\n[LM] Durbin–Watson:\n\n\n\n    Durbin-Watson test\n\ndata:  lm_full\nDW = 1.7745, p-value = 0.1264\nalternative hypothesis: true autocorrelation is greater than 0\n\nglm_resid &lt;- residuals(glm_full, type = \"pearson\")\ncat(\"\\n[GLM-Gamma] Сводка:\\n\"); print(summary(glm_full))\n\n\n[GLM-Gamma] Сводка:\n\n\n\nCall:\nglm(formula = f_lm, family = Gamma(link = \"log\"), data = full_fit_df)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.907e+00  1.288e+00   5.361 1.30e-05 ***\ncodTSB      -6.525e-07  1.942e-07  -3.359  0.00242 ** \nT12          1.341e+00  2.397e-01   5.593 7.08e-06 ***\nI5           8.270e-03  8.740e-03   0.946  0.35278    \nNAOspring    4.898e-02  1.568e-01   0.312  0.75725    \nhaddock68    1.117e-06  1.559e-06   0.717  0.48000    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.3107759)\n\n    Null deviance: 19.8880  on 31  degrees of freedom\nResidual deviance:  8.5197  on 26  degrees of freedom\nAIC: 855.29\n\nNumber of Fisher Scoring iterations: 8\n\ncat(sprintf(\"[GLM-Gamma] Pearson dispersion: %.3f\\n\", sum(glm_resid^2, na.rm = TRUE) / glm_full$df.residual))\n\n[GLM-Gamma] Pearson dispersion: 0.311\n\ncat(\"\\n[GAM] Сводка:\\n\"); print(summary(gam_full))\n\n\n[GAM] Сводка:\n\n\n\nFamily: Gamma \nLink function: log \n\nFormula:\nR3haddock ~ s(codTSB, bs = \"tp\", k = 5) + s(T12, bs = \"tp\", k = 5) + \n    s(I5, bs = \"tp\", k = 5) + s(NAOspring, bs = \"tp\", k = 5) + \n    s(haddock68, bs = \"tp\", k = 5)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.48948    0.08886   140.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                   edf Ref.df     F  p-value    \ns(codTSB)    1.7083407      4 4.917 7.43e-05 ***\ns(T12)       0.9669993      4 7.752 3.96e-06 ***\ns(I5)        0.0001638      4 0.000    0.616    \ns(NAOspring) 0.0001092      4 0.000    0.980    \ns(haddock68) 0.4529453      4 0.145    0.252    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.592   Deviance explained = 60.2%\n-REML = 426.83  Scale est. = 0.2527    n = 32\n\ncat(\"\\n[GAM] Concurvity (коротко):\\n\")\n\n\n[GAM] Concurvity (коротко):\n\nccv &lt;- try(mgcv::concurvity(gam_full, full = FALSE), silent = TRUE)\nif (!inherits(ccv, \"try-error\") && is.list(ccv)) {\n  # Выведем усечённо и безопасно\n  print(lapply(ccv, function(m) if (is.null(m)) NULL else round(m, 3)))\n} else {\n  cat(\"не удалось оценить concurvity\\n\")\n}\n\n$worst\n             para s(codTSB) s(T12) s(I5) s(NAOspring) s(haddock68)\npara            1     0.000  0.000 0.000        0.000        0.000\ns(codTSB)       0     1.000  0.554 0.298        0.178        0.821\ns(T12)          0     0.554  1.000 0.361        0.362        0.347\ns(I5)           0     0.298  0.361 1.000        0.182        0.132\ns(NAOspring)    0     0.178  0.362 0.182        1.000        0.216\ns(haddock68)    0     0.821  0.347 0.132        0.216        1.000\n\n$observed\n             para s(codTSB) s(T12) s(I5) s(NAOspring) s(haddock68)\npara            1     0.000  0.000 0.000        0.000        0.000\ns(codTSB)       0     1.000  0.392 0.166        0.045        0.385\ns(T12)          0     0.273  1.000 0.217        0.064        0.218\ns(I5)           0     0.166  0.283 1.000        0.047        0.044\ns(NAOspring)    0     0.031  0.128 0.114        1.000        0.049\ns(haddock68)    0     0.441  0.204 0.113        0.116        1.000\n\n$estimate\n             para s(codTSB) s(T12) s(I5) s(NAOspring) s(haddock68)\npara            1     0.000  0.000 0.000        0.000        0.000\ns(codTSB)       0     1.000  0.320 0.140        0.041        0.747\ns(T12)          0     0.281  1.000 0.201        0.068        0.243\ns(I5)           0     0.121  0.278 1.000        0.053        0.096\ns(NAOspring)    0     0.055  0.128 0.113        1.000        0.089\ns(haddock68)    0     0.544  0.205 0.099        0.136        1.000\n\ninvisible(try(mgcv::gam.check(gam_full), silent = TRUE))\n\n\n\n\n\n\n\n\n\nMethod: REML   Optimizer: outer newton\nfull convergence after 13 iterations.\nGradient range [-4.544099e-05,0.000320414]\n(score 426.834 & scale 0.2526969).\nHessian positive definite, eigenvalue range [5.946259e-06,16.95877].\nModel rank =  21 / 21 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n                   k'      edf k-index p-value\ns(codTSB)    4.000000 1.708341    1.12    0.69\ns(T12)       4.000000 0.966999    1.29    0.95\ns(I5)        4.000000 0.000164    0.86    0.20\ns(NAOspring) 4.000000 0.000109    0.99    0.50\ns(haddock68) 4.000000 0.452945    1.10    0.73\n\n# 5) Прогноз 2022–2024 и эмпирические интервалы ------------------------------\nbest_full &lt;- switch(best_model_name,\n  LM  = lm_full,\n  GLM = glm_full,\n  GAM = gam_full\n)\n\n# Остатки для PI: из CV выбранной модели, иначе из полного фита\nresids &lt;- if (length(resids_cv[[best_model_name]]) &gt; 5) resids_cv[[best_model_name]] else residuals(best_full)\n\nq025 &lt;- as.numeric(quantile(resids, 0.025, na.rm = TRUE))\nq250 &lt;- as.numeric(quantile(resids, 0.250, na.rm = TRUE))\nq750 &lt;- as.numeric(quantile(resids, 0.750, na.rm = TRUE))\nq975 &lt;- as.numeric(quantile(resids, 0.975, na.rm = TRUE))\n\nfc_start &lt;- 2022\npred_cols &lt;- c(\"codTSB\",\"T12\",\"I5\",\"NAOspring\",\"haddock68\")\nmu &lt;- md %&gt;% filter(YEAR &gt; 1989 & YEAR &lt; fc_start) %&gt;% summarise(across(all_of(pred_cols), ~mean(.x, na.rm = TRUE))) %&gt;% as.list()\n\nif (!exists(\"user_future\")) user_future &lt;- NULL\n\nbuild_future &lt;- function(years, mu, user_df = NULL) {\n  df &lt;- tibble::tibble(YEAR = years)\n  for (v in pred_cols) df[[v]] &lt;- mu[[v]]\n  if (!is.null(user_df)) {\n    for (i in seq_len(nrow(user_df))) {\n      yr &lt;- user_df$YEAR[i]\n      if (yr %in% years) {\n        idx &lt;- which(df$YEAR == yr)\n        for (v in intersect(pred_cols, names(user_df))) {\n          val &lt;- user_df[[v]][i]\n          if (!is.na(val)) df[[v]][idx] &lt;- val\n        }\n      }\n    }\n  }\n  df\n}\n\nfuture_years &lt;- fc_start:2024\nscenario_future &lt;- build_future(future_years, mu, user_future)\n\npredict_best &lt;- function(fit, newdata, model_name) {\n  if (model_name == \"GLM\") return(predict(fit, newdata = newdata, type = \"response\"))\n  predict(fit, newdata = newdata)\n}\n\npred_future &lt;- predict_best(best_full, scenario_future, best_model_name)\n\nforecast_tbl &lt;- tibble::tibble(\n  YEAR      = scenario_future$YEAR,\n  Model     = best_model_name,\n  pred_mean = as.numeric(pred_future),\n  PI50_low  = pred_future + q250, PI50_high = pred_future + q750,\n  PI95_low  = pred_future + q025, PI95_high = pred_future + q975\n)\n\n\nknitr::kable(\n  forecast_tbl %&gt;% dplyr::mutate(dplyr::across(where(is.numeric), ~round(.x, 2))),\n  caption = \"Holdout-метрики (округлено до 2 знаков)\"\n)\n\n\nHoldout-метрики (округлено до 2 знаков)\n\n\nYEAR\nModel\npred_mean\nPI50_low\nPI50_high\nPI95_low\nPI95_high\n\n\n\n\n2022\nGLM\n268057.6\n172383\n509783.3\n-203196.1\n1051570\n\n\n2023\nGLM\n268057.6\n172383\n509783.3\n-203196.1\n1051570\n\n\n2024\nGLM\n268057.6\n172383\n509783.3\n-203196.1\n1051570\n\n\n\n\n# 6) Визуализация 1990–2024 ---------------------------------------------------\npred_df &lt;- bind_rows(\n  md %&gt;% select(YEAR, all_of(pred_cols)),\n  scenario_future\n) %&gt;% distinct(YEAR, .keep_all = TRUE) %&gt;% arrange(YEAR)\n\npred_df$Pred      &lt;- as.numeric(predict_best(best_full, pred_df, best_model_name))\npred_df$PI50_low  &lt;- pred_df$Pred + q250\npred_df$PI50_high &lt;- pred_df$Pred + q750\npred_df$PI95_low  &lt;- pred_df$Pred + q025\npred_df$PI95_high &lt;- pred_df$Pred + q975\n\nhist_df &lt;- md %&gt;% select(YEAR, R3haddock)\n\nggplot() +\n  geom_ribbon(data = pred_df, aes(x = YEAR, ymin = PI95_low, ymax = PI95_high), fill = \"grey80\", alpha = 0.25) +\n  geom_ribbon(data = pred_df, aes(x = YEAR, ymin = PI50_low, ymax = PI50_high), fill = \"grey60\", alpha = 0.35) +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = Pred), color = \"steelblue4\", linewidth = 1) +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = Pred), color = \"steelblue4\", linewidth = 1, linetype = \"dashed\") +\n  geom_point(data = hist_df, aes(x = YEAR, y = R3haddock), color = \"black\", size = 2, alpha = 0.9) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0))) +\n  labs(title = paste0(\"Пополнение R3haddock: факт (1990–2021) и прогноз (2022–2024) — \", best_model_name),\n       subtitle = \"Прогноз — пунктир, интервалы — эмпирические из остатков\",\n       x = \"Год\", y = \"R3haddock\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# AIC-таблица (LM/GLM сопоставимы напрямую; для GAM также показываем ML)\ncat(\"\\nAIC (LM): \",  AIC(lm_full),  \"\\n\", sep = \"\")\n\n\nAIC (LM): 877.0549\n\ncat(\"AIC (GLM): \", AIC(glm_full), \"\\n\", sep = \"\")\n\nAIC (GLM): 855.2949\n\ngam_full_ml &lt;- mgcv::gam(f_gam, data = full_fit_df, family = Gamma(link = \"log\"), method = \"ML\", select = TRUE)\ncat(\"AIC (GAM, REML): \", AIC(gam_full),    \"\\n\", sep = \"\")\n\nAIC (GAM, REML): 850.8686\n\ncat(\"AIC (GAM, ML):   \", AIC(gam_full_ml), \"\\n\", sep = \"\")\n\nAIC (GAM, ML):   850.7948\n\n# ============================================================================\n# Конец\n# ============================================================================",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 7.html#полный-цикл-от-факторов-до-ансамблевого-прогноза",
    "href": "chapter 7.html#полный-цикл-от-факторов-до-ансамблевого-прогноза",
    "title": "8  Прогноз пополнения: от факторов до ансамбля",
    "section": "8.5 Полный цикл от факторов до ансамблевого прогноза",
    "text": "8.5 Полный цикл от факторов до ансамблевого прогноза\nПолный цикл анализа от идентификации ключевых факторов до создания надежного ансамблевого прогноза пополнения рыбных запасов представляет собой сложный, но систематизированный процесс, требующий как глубокого понимания биологических процессов, так и владения современными методами анализа данных. Начиная с формирования исходного набора предикторов, включающего как биологические переменные (нерестовый запас, биомасса хищников), так и комплексные океанографические показатели (температура, соленость, климатические индексы), мы проходим через строгую последовательность этапов, каждый из которых важен для конечного результата. На этапе подготовки данных мы не просто приводим информацию к числовому формату и заменяем строковые обозначения пропущенных значений «NA» на стандартные NA, но и проводим глубокий анализ корреляционной структуры, устраняя мультиколлинеарность через анализ корреляций и VIF-диагностику, что важно для корректной интерпретации последующих моделей. Для обработки пропусков мы применяем медианную импутацию, которая представляет собой простой и устойчивый к выбросам метод, хотя в некоторых случаях могут быть использованы и более сложные методы, такие как KNN-импутация или множественная импутация с использованием пакета MICE, особенно когда данные имеют сложную структуру или временные зависимости.\nЗатем следует этап отбора предикторов, где мы применяем два комплементарных метода: Boruta на основе Random Forest для выявления нелинейных зависимостей и LASSO-регрессию для линейного отбора с регуляризацией. Их объединение позволяет получить устойчивый набор предикторов, дополненный биологически значимыми переменными по экспертной оценке, что создает баланс между статистической значимостью и содержательной интерпретируемостью. Этот этап является мостом между классической ихтиологией и современными методами анализа, где экспертные знания биолога взаимодействуют с алгоритмической строгостью статистики, гарантируя включение ключевых факторов, таких как нерестовый запас, который должен присутствовать в модели по самой своей природе процесса пополнения.\nПосле подготовки данных мы переходим к сравнению различных семейств моделей через единую кросс-валидационную процедуру (5-fold CV) с последующим хронологическим тестированием на отложенной выборке. Помимо линейных и обобщенных линейных моделей (LM, GLM), обобщенных аддитивных моделей (GAM), мы тестируем современные алгоритмы машинного обучения: Random Forest для улавливания сложных нелинейных зависимостей и взаимодействий между факторами, будучи при этом устойчивым к шуму и выбросам; XGBoost, с его градиентным бустингом над деревьями решений, часто дающий высочайшую точность прогноза; SVM с радиальным ядром для сложных разделяющих поверхностей; и нейронные сети для автоматического извлечения признаков. Каждая модель оценивается по комплексу метрик: RMSE, MAE, R² и MAPE, что позволяет сравнивать их прогностическую силу на разных участках данных и выявлять модели, которые лучше всего справляются с конкретными аспектами прогнозирования.\nОсобое внимание уделяется временным характеристикам данных, поскольку при анализе водных биоресурсов мы имеем дело с временными рядами, где случайное перемешивание данных приведет к утечке информации из будущего в прошлое, искусственно завысив качество прогноза. Для решения этой проблемы мы применяем специализированную time-slice кросс-валидацию с расширяющимся окном и горизонтом прогноза 3 года, которая имитирует реальные условия прогнозирования, обучаясь только на данных из прошлого и проверяя на последующих периодах. Это позволяет оценить устойчивость моделей к временным сдвигам и их способность к экстраполяции, что критически важно для практических задач управления рыбными запасами.\nВыбор окончательной модели — это не просто вопрос максимальной точности на кросс-валидации, а сложный компромисс между точностью, интерпретируемостью и биологической правдоподобностью. Кульминацией цикла становится построение ансамблевой модели, комбинирующей сильные стороны отдельных алгоритмов. В нашем анализе оптимальный ансамбль (CUBIST + LM) строится через взвешенное усреднение предсказаний, где веса определяются на основе кросс-валидационной ошибки — например, 75% веса приходится на мощную нелинейную модель Cubist, а 25% — на простую и устойчивую линейную регрессию. Такой подход позволяет нивелировать индивидуальные недостатки моделей, сохранить интерпретируемость линейных моделей, где биолог может понять, как именно каждый фактор влияет на прогноз, и при этом использовать гибкость методов машинного обучения для захвата сложных нелинейных паттернов, которые могут ускользнуть от классических статистических методов.\nВажнейшим компонентом становится оценка неопределенности через эмпирические доверительные интервалы, построенные на основе распределения остатков ансамблевой модели. Мы используем квантили остатков из кросс-валидации для построения 50% и 95% доверительных интервалов, что позволяет получить не только точечный прогноз, но и меру его надежности, важную для принятия управленческих решений. Это дает возможность визуализировать не только ожидаемое значение пополнения, но и диапазон возможных сценариев, что особенно важно в условиях высокой экологической неопределенности.\nФинальная визуализация представляет собой совмещение исторических данных с прогнозом на 3 года вперед, где исторические данные отображаются сплошной линией, прогноз — пунктиром, а 50% и 95% доверительные интервалы — серыми лентами различной интенсивности. Такой график не только демонстрирует результат, но и позволяет визуально оценить точность модели на исторических данных и неопределенность будущих предсказаний, делая результаты доступными не только для статистиков, но и для управленцев и политиков, принимающих решения на основе этих прогнозов.\nПредставленный цикл является итеративным процессом: прогнозная точность ансамбля может быть улучшена через включение новых предикторов, изучение влияния предикторов с задержкой (лагами), тонкую настройку гиперпараметров моделей и обновление данных по мере их поступления. Этот подход представляет собой практический компромисс между статистической строгостью, вычислительной эффективностью и биологической интерпретируемостью, делая его мощным инструментом для решения прикладных задач оценки водных биоресурсов, где каждый этап, от первичной обработки данных до финального прогноза, подчинен одной цели — обеспечению устойчивого управления рыбными запасами на основе надежного научного анализа.\nСкрипт лучше скачать целиком).\n\n# ==============================================================================\n# ПРАКТИЧЕСКОЕ ЗАНЯТИЕ: АНАЛИЗ ФАКТОРОВ И ПРОГНОЗ ПОПОЛНЕНИЯ ЗАПАСА\n# Курс: \"Оценка водных биоресурсов в среде R (для начинающих)\"\n# Автор: Баканев С. В. Дата:20.08.2025\n# Структура:\n# 1) Подготовка данных и выбор предикторов\n# 2) Базовое сравнение моделей (5-fold CV + holdout)\n# 3) Выбор лучшей прогностической модели (time-slice CV на 3 года + хронологический тест)\n# 4) Прогноз 2022–2024 (ансамбль CUBIST+LM) и график 1990–2024 с ДИ\n# ------------------------------------------------------------------------------\n# Пояснения к занятию (для начинающих):\n# - Мы работаем с временным рядом пополнения запаса R3haddock и набором факторов\n#   среды/биомассы. Цель — построить понятные и проверяемые модели прогноза.\n# - Сначала отберём информативные предикторы (Boruta и LASSO), затем сравним\n#   разные модели машинного обучения на кросс-валидации (CV), после чего выберем\n#   лучшую схему по time-slice CV (учитывая хронологию), и сделаем прогноз.\n# ==============================================================================\n\n\n# ==============================================================================\n# 1) ВЫБОР ПРЕДИКТОРОВ\n# ------------------------------------------------------------------------------\n# Цель блока: привести данные к числовому виду, обработать пропуски, сократить\n# мультиколлинеарность (сильные корреляции), а затем автоматически выделить\n# кандидатов-предикторов двумя методами (Boruta, LASSO). В конце сформируем\n# финальный пул признаков и проверим их значимость в простой LM.\n# ==============================================================================\n\n# Установка и подключение необходимых библиотек\n# Для автоматического отбора предикторов нам понадобятся дополнительные пакеты\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  readxl, tidyverse, caret, corrplot, mgcv, randomForest, xgboost,\n  Boruta,GGally, FactoMineR, glmnet, recipes, rsample  # Новые библиотеки для автоматического отбора\n)\n\n# Очистка среды и установка рабочей директории\n# Совет: rm(list=ls()) очищает все объекты в памяти R; setwd задаёт папку,\n# где искать/сохранять файлы. Убедитесь, что путь корректен на вашей машине.\nrm(list = ls())\nsetwd(\"C:/RECRUITMENT/\")\n\n# Пакеты для расширенного отбора предикторов\n# Boruta — обёртка над Random Forest для отбора признаков;\n# glmnet — регуляризация (LASSO/ElasticNet) для отбора/усиления обобщающей способности;\n# FactoMineR — PCA и другие многомерные методы (используем как утилиту).\nlibrary(Boruta)   # Алгоритм обертки для отбора признаков\nlibrary(glmnet)   # LASSO-регрессия\nlibrary(FactoMineR) # PCA анализ\n\n\n# Загрузка и первичная обработка данных\n# Шаги: фильтруем годы, приводим типы к числовому, заменяем строковые \"NA\" на NA.\nDATA &lt;- readxl::read_excel(\"RECRUITMENT.xlsx\", sheet = \"RECRUITMENT\") %&gt;%\n  filter(YEAR &gt; 1989 & YEAR &lt; 2022) %&gt;%\n  # Преобразуем необходимые столбцы в числовой формат\n  mutate(\n    across(starts_with(\"T\"), as.numeric),\n    across(starts_with(\"I\"), as.numeric),\n    across(starts_with(\"O\"), as.numeric),\n  ) %&gt;%\n  # Обработка пропущенных значений (заменяем строку \"NA\" на NA)\n  mutate(across(where(is.character), ~na_if(., \"NA\")))\n\n# 1. Подготовка данных -------------------------------------------------------\n# Выделим все возможные предикторы, включая географию и индексы трески\n# Примечание: оставляем только числовые переменные, т.к. большинство моделей\n# требует числовой вход без категориальных уровней.\npredictors &lt;- DATA %&gt;% \n  select(-YEAR, -R3haddock) %&gt;% \n  select_if(is.numeric) # Только числовые переменные\n\n# Целевая переменная\nresponse &lt;- DATA$R3haddock\n\n# В статистическом анализе мы различаем:\n# - Отклик (response/target variable) - то, что мы пытаемся предсказать (в нашем случае R3haddock)\n# - Предикторы (predictors/features) - переменные, которые могут объяснять изменения отклика\n# Для корректного анализа важно, чтобы предикторы были числовыми или преобразованы в числовой формат.\n\n# 2. Обработка пропусков -----------------------------------------------------\n# Заполнение медианными значениями — простой и устойчивый способ справиться с NA.\n# Альтернативы: множественная иммутация (mice), KNN-impute и др.\npredictors_filled &lt;- predictors %&gt;%\n  mutate(across(everything(), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))\n\n# Заполнение медианой - простой и устойчивый метод обработки пропусков для числовых переменных.\n# Медиана предпочтительнее среднего, так как менее чувствительна к выбросам.\n\n# 3. Предварительный анализ корреляций ---------------------------------------\n# Зачем: высокие корреляции затрудняют интерпретацию и могут вредить ряду моделей.\ncor_matrix &lt;- cor(predictors_filled, use = \"complete.obs\")\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", tl.cex = 0.7)\n\n\n\n\n\n\n\n# Удаляем высокоскоррелированные предикторы (r &gt; 0.8)\n# Это механическое сокращение мультиколлинеарности до этапа отбора.\nhigh_cor &lt;- findCorrelation(cor_matrix, cutoff = 0.8)\npredictors_filtered &lt;- predictors_filled[, -high_cor]\n\n# Высокая корреляция между предикторами (мультиколлинеарность) может привести к нестабильности моделей.\n# Например, если два предиктора почти идентичны, модель может неустойчиво распределять их влияние на отклик.\n# Удаление сильно коррелированных переменных (r &gt; 0.8) помогает улучшить интерпретируемость и стабильность моделей.\n\n\n# 4. Автоматизированный отбор Boruta (обертка Random Forest) -----------------\n# Идея: определить признаки, которые важнее, чем случайный шум (shadow features).\n\n\n# Визуализация результатов\nplot(boruta_output, cex.axis = 0.7, las = 2)\n\n\n\n\n\n\n\nboruta_stats &lt;- attStats(boruta_output)\nselected_vars &lt;- getSelectedAttributes(boruta_output, withTentative = TRUE)\n\n# Boruta - это алгоритм отбора признаков, основанный на методе случайного леса.\n# Он сравнивает важность реальных переменных с \"теневыми\" переменными (случайными копиями),\n# чтобы определить, действительно ли переменная информативна. \n# Результаты Boruta показывают: \n#   - Confirmed (зеленые) - значимые предикторы\n#   - Tentative (желтые) - предикторы, близкие к порогу значимости\n#   - Rejected (красные) - незначимые предикторы\n\n\n# 5. LASSO с более строгим критерием ------------------------------------------\n# Идея: L1-регуляризация зануляет коэффициенты «слабых» предикторов.\n# Выбор lambda.1se вместо lambda.min — более консервативный (простая модель).\nx &lt;- as.matrix(predictors_filtered)\ny &lt;- response\n\n# LASSO (Least Absolute Shrinkage and Selection Operator) - метод регрессии с L1-регуляризацией,\n# который одновременно выполняет отбор признаков и оценку коэффициентов. \n# Параметр lambda контролирует силу регуляризации:\n#   - lambda.min дает наименьшую ошибку, но может включать шумовые переменные\n#   - lambda.1se (на 1 стандартную ошибку больше) дает более простую модель с меньшим риском переобучения\n# Для прогнозирования мы предпочитаем более строгий критерий (lambda.1se), чтобы модель была устойчивее. \n\n# Кросс-валидация\ncv_fit &lt;- cv.glmnet(x, y, alpha = 1, nfolds = 10)\nplot(cv_fit)\n\n\n\n\n\n\n\n# ИСПОЛЬЗУЕМ lambda.1se вместо lambda.min — СТРОЖЕ!\nlasso_coef &lt;- coef(cv_fit, s = \"lambda.1se\")  # &lt;-- Ключевое изменение!\nlasso_vars &lt;- rownames(lasso_coef)[lasso_coef[,1] != 0][-1]  # исключаем (Intercept)\n\n\n# 6. Сравнение отобранных предикторов ----------------------------------------\n# Полезно видеть, какие признаки отмечают оба метода (устойчивые кандидаты).\ncat(\"Boruta selected:\", length(selected_vars), \"variables\\n\")\n\nBoruta selected: 3 variables\n\nprint(selected_vars)\n\n[1] \"codTSB\" \"T12\"    \"I5\"    \n\ncat(\"\\nLASSO selected:\", length(lasso_vars), \"variables\\n\")\n\n\nLASSO selected: 5 variables\n\nprint(lasso_vars)\n\n[1] \"codTSB\" \"T12\"    \"NAO3\"   \"NAO4\"   \"NAO5\"  \n\n# 7. Финальный набор предикторов (объединение результатов) -------------------\n# Логика: объединяем списки, добавляем биологически важные переменные вручную.\nfinal_vars &lt;- union(selected_vars, lasso_vars) \n\n# Добавляем обязательные переменные по биологической логике\nmandatory &lt;- c(\"haddock68\")\nfinal_vars &lt;- union(final_vars, mandatory) %&gt;% unique()\n\n# Мы объединяем результаты двух методов отбора признаков для большей надежности.\n# Также добавляем переменную haddock68 (нерестовый запас), так как биологически \n# логично, что пополнение запаса напрямую зависит от численности производителей. \n# Это пример интеграции экспертных знаний в статистический анализ - важный принцип \n# при работе с данными в биологических науках.\n\n# 8. Проверка значимости -----------------------------------------------------\n# Быстрая оценка значимости с LM: не как окончательный вывод, а как sanity-check.\nfinal_model &lt;- lm(response ~ as.matrix(predictors_filled[, final_vars]))\nsummary(final_model)\n\n\nCall:\nlm(formula = response ~ as.matrix(predictors_filled[, final_vars]))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-270986  -82376   -1037   98086  276129 \n\nCoefficients:\n                                                      Estimate Std. Error\n(Intercept)                                         -1.082e+06  3.943e+05\nas.matrix(predictors_filled[, final_vars])codTSB    -2.346e-01  5.536e-02\nas.matrix(predictors_filled[, final_vars])T12        3.864e+05  7.198e+04\nas.matrix(predictors_filled[, final_vars])I5        -1.825e+02  2.572e+03\nas.matrix(predictors_filled[, final_vars])NAO3      -5.801e+04  3.129e+04\nas.matrix(predictors_filled[, final_vars])NAO4       8.345e+04  3.035e+04\nas.matrix(predictors_filled[, final_vars])NAO5      -7.278e+04  2.488e+04\nas.matrix(predictors_filled[, final_vars])haddock68  1.232e-01  4.515e-01\n                                                    t value Pr(&gt;|t|)    \n(Intercept)                                          -2.744 0.011305 *  \nas.matrix(predictors_filled[, final_vars])codTSB     -4.238 0.000288 ***\nas.matrix(predictors_filled[, final_vars])T12         5.368 1.64e-05 ***\nas.matrix(predictors_filled[, final_vars])I5         -0.071 0.944028    \nas.matrix(predictors_filled[, final_vars])NAO3       -1.854 0.076118 .  \nas.matrix(predictors_filled[, final_vars])NAO4        2.750 0.011146 *  \nas.matrix(predictors_filled[, final_vars])NAO5       -2.925 0.007412 ** \nas.matrix(predictors_filled[, final_vars])haddock68   0.273 0.787227    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 158600 on 24 degrees of freedom\nMultiple R-squared:  0.7173,    Adjusted R-squared:  0.6348 \nF-statistic: 8.698 on 7 and 24 DF,  p-value: 2.58e-05\n\n# 9. Формирование финального датасета ----------------------------------------\n# Собираем набор с откликом и выбранными предикторами; удалим строки с NA.\nmodel_data &lt;- DATA %&gt;%\n  select(R3haddock, all_of(final_vars)) %&gt;%\n  drop_na()\n\n# Просмотр структуры финальных данных\nglimpse(model_data)\n\nRows: 32\nColumns: 8\n$ R3haddock &lt;dbl&gt; 812363, 389416, 99474, 98946, 118812, 63028, 147657, 83270, ~\n$ codTSB    &lt;dbl&gt; 913000, 1347064, 1687381, 2197863, 2112773, 1849957, 1697388~\n$ T12       &lt;dbl&gt; 4.72, 4.66, 4.24, 3.90, 3.96, 4.27, 4.16, 4.07, 4.23, 5.08, ~\n$ I5        &lt;dbl&gt; 43, 55, 26, 49, 56, 28, 52, 51, 69, 68, 41, 48, 50, 63, 40, ~\n$ NAO3      &lt;dbl&gt; 1.46, -0.20, 0.87, 0.67, 1.26, 1.25, -0.24, 1.46, 0.87, 0.23~\n$ NAO4      &lt;dbl&gt; 2.00, 0.29, 1.86, 0.97, 1.14, -0.85, -0.17, -1.02, -0.68, -0~\n$ NAO5      &lt;dbl&gt; -1.53, 0.08, 2.63, -0.78, -0.57, -1.49, -1.06, -0.28, -1.32,~\n$ haddock68 &lt;dbl&gt; 74586, 79205, 53195, 36337, 49122, 81514, 172177, 160886, 96~\n\n# Визуализация важности переменных\n# Внимание: важности от RF — относительные; сопоставляйте с предметной логикой.\nvar_importance &lt;- randomForest(R3haddock ~ ., data = model_data, importance = TRUE)\nvarImpPlot(var_importance, main = \"Важность предикторов\")\n\n\n\n\n\n\n\n# Перед окончательным выбором модели мы проверяем значимость предикторов с помощью линейной регрессии.\n# Функция summary() показывает p-значения коэффициентов - если p &lt; 0.05, переменная считается статистически значимой. \n# Визуализация важности переменных с помощью случайного леса дает дополнительную перспективу,\n# показывая, какие переменные наиболее информативны для предсказания без предположений о линейности.\n\n# ==============================================================================\n#  ПОДГОТОВКА ДАННЫХ\n# Создаём NAOspring, фиксируем финальный набор признаков, сохраняем CSV.\n# ------------------------------------------------------------------------------\n# Цель блока: стандартизировать набор признаков для дальнейшего сравнения\n# моделей и обеспечить воспроизводимость (фиксированный CSV с нужными полями).\n# ==============================================================================\n\n# 1.1 Пакеты и окружение\n# Примечание: блок повторяет базовую инициализацию для автономного запуска.\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(readxl, tidyverse, caret, corrplot)\n\nrm(list = ls())\nset.seed(123)\nsetwd(\"C:/RECRUITMENT/\")\n\n# 1.2 Загрузка исходных данных и приведение типов\nDATA &lt;- readxl::read_excel(\"RECRUITMENT.xlsx\", sheet = \"RECRUITMENT\") %&gt;%\n  filter(YEAR &gt; 1989 & YEAR &lt; 2022) %&gt;%\n  mutate(\n    across(starts_with(\"T\"), as.numeric),\n    across(starts_with(\"I\"), as.numeric),\n    across(starts_with(\"O\"), as.numeric),\n    across(where(is.character), ~na_if(., \"NA\"))\n  )\n\n# 1.3 Создаём NAOspring (если есть NAO3, NAO4, NAO5)\n# Идея: агрегируем весенний индекс NAO как среднее за месяцы 3–5.\nif (all(c(\"NAO3\",\"NAO4\",\"NAO5\") %in% names(DATA))) {\n  DATA &lt;- DATA %&gt;%\n    mutate(NAOspring = rowMeans(pick(NAO3, NAO4, NAO5), na.rm = TRUE)) %&gt;%\n    select(-NAO3, -NAO4, -NAO5)\n}\n\n# NAO (North Atlantic Oscillation) - важный климатический индекс, влияющий описывающий изменения атмосферного давления\n# над Северной Атлантикой. В частности, он отражает разницу в атмосферном давлении между Исландской депрессией и\n# Азорским максимумом. NAO влияет на силу и направление западных ветров, а также на траектории штормов в Северной Атлантике. \n# Мы создаем NAOspring как среднее значение за весенние месяцы (марта, апреля, мая),\n# так как именно в этот период происходят ключевые процессы, влияющие на нерест трески. \n# Создание составных переменных на основе экспертных знаний часто улучшает качество моделей.\n\n# 1.4 Финальный учебный набор предикторов (фиксируем)\n# Важно: проверяем присутствие нужных колонок и формируем компактный датасет.\nneeded &lt;- c(\"codTSB\", \"T12\", \"I5\", \"NAOspring\", \"haddock68\")\nstopifnot(all(needed %in% names(DATA)))\n\n# Сохраняем YEAR в CSV (ниже он будет отброшен при обучении, но нужен для графика)\nmodel_data &lt;- DATA %&gt;%\n  select(YEAR, all_of(needed), R3haddock) %&gt;%\n  drop_na()\n\nwrite.csv(model_data, \"selected_predictors_dataset.csv\", row.names = FALSE)\nglimpse(model_data)\n\nRows: 32\nColumns: 7\n$ YEAR      &lt;dbl&gt; 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, ~\n$ codTSB    &lt;dbl&gt; 913000, 1347064, 1687381, 2197863, 2112773, 1849957, 1697388~\n$ T12       &lt;dbl&gt; 4.72, 4.66, 4.24, 3.90, 3.96, 4.27, 4.16, 4.07, 4.23, 5.08, ~\n$ I5        &lt;dbl&gt; 43, 55, 26, 49, 56, 28, 52, 51, 69, 68, 41, 48, 50, 63, 40, ~\n$ NAOspring &lt;dbl&gt; 0.64333333, 0.05666667, 1.78666667, 0.28666667, 0.61000000, ~\n$ haddock68 &lt;dbl&gt; 74586, 79205, 53195, 36337, 49122, 81514, 172177, 160886, 96~\n$ R3haddock &lt;dbl&gt; 812363, 389416, 99474, 98946, 118812, 63028, 147657, 83270, ~\n\n# (необязательно) Глянуть попарные связи и корреляции\n# ggpairs может быть медленным, оставим по желанию\n ggpairs(model_data, columns = 2:7,\n         lower = list(continuous = wrap(\"smooth\", alpha = 0.3, size = 0.5)),\n         upper = list(cor = wrap(\"cor\", size = 3)))\n\n\n\n\n\n\n\n# ==============================================================================\n# 2) БАЗОВОЕ СРАВНЕНИЕ МОДЕЛЕЙ (5-FOLD CV + HOLDOUT)\n# Единые фолды CV, тренировочно-тестовое разбиение, сводка метрик.\n# ------------------------------------------------------------------------------\n# Идея блока: быстрая «панель» сравнения разных семейств моделей на одинаковых\n# условиях (одинаковые фолды CV) и внешний тест (holdout). Это помогает увидеть\n# уровни ошибок и выбрать несколько лидеров для более строгой проверки далее.\n# ==============================================================================\n\n# 2.1 Пакеты и данные\npacman::p_load(mgcv, randomForest, xgboost, nnet, earth, kernlab, pls, Cubist, ranger, gbm, lattice)\n\nmodel_data &lt;- read.csv(\"selected_predictors_dataset.csv\", header = TRUE, stringsAsFactors = FALSE)\n# Если YEAR отсутствует (на всякий случай), создадим\nif (!\"YEAR\" %in% names(model_data)) {\n  model_data$YEAR &lt;- seq(1990, by = 1, length.out = nrow(model_data))\n}\n\n# Используем только предикторы и отклик (YEAR исключаем)\nmodel_data &lt;- model_data %&gt;%\n  select(codTSB, T12, I5, NAOspring, haddock68, R3haddock) %&gt;%\n  na.omit()\n\n# 2.2 Holdout и CV-контроллер\n# Пропорция 80/20 обеспечивает внешний тест; внутри train — 5-fold CV для\n# корректной настройки моделей и оценки средней ошибки.\ntrain_idx &lt;- caret::createDataPartition(model_data$R3haddock, p = 0.8, list = FALSE)\ntrain &lt;- model_data[train_idx, ]\ntest  &lt;- model_data[-train_idx, ]\n\nctrl &lt;- caret::trainControl(method = \"cv\", number = 5, savePredictions = \"final\")\n\n# Holdout-метод: мы делим данные на обучающую (80%) и тестовую (20%) выборки.\n# Кросс-валидация (5-fold CV): данные разбиваются на 5 частей, модель обучается на 4 частях и тестируется на 5-й, \n# и этот процесс повторяется 5 раз. Это дает более надежную оценку качества модели, чем одно разбиение. \n\n\n# 2.3 Кастомный GAM (mgcv) для caret (bs=\"tp\", REML, select=TRUE)\n# GAM даёт гладкие нелинейности по каждому признаку; REML стабилизирует оценку.\ngam_spec &lt;- list(\n  type = \"Regression\", library = \"mgcv\", loop = NULL,\n  parameters = data.frame(parameter = \"none\", class = \"character\", label = \"none\"),\n  grid = function(x,y,len=NULL,search=\"grid\") data.frame(none = NA),\n  fit = function(x,y,...) {\n    df &lt;- x; df$R3haddock &lt;- y\n    mgcv::gam(\n      R3haddock ~ s(codTSB,bs=\"tp\") + s(T12,bs=\"tp\") + s(I5,bs=\"tp\") +\n                  s(NAOspring,bs=\"tp\") + s(haddock68,bs=\"tp\"),\n      data=df, method=\"REML\", select=TRUE, ...\n    )\n  },\n  predict = function(modelFit, newdata, submodels = NULL) {\n    predict(modelFit, newdata = newdata, type = \"response\")\n  },\n  prob = NULL, sort = function(x) x\n)\n\n# 2.4 Обучение моделей\n# Подсказка: разные методы по-разному чувствительны к масштабу, числу признаков\n# и мультиколлинеарности. Мы применяем одинаковые фолды CV для честного сравнения.\n\n# --- 1. Линейная регрессия (LM)\n# Учебный смысл: базовая линейная модель; ориентир для сравнения.\n# ПОЯСНЕНИЕ: LM предполагает линейную зависимость между предикторами и откликом.\n# Это простая модель, которая служит \"нижней планкой\" - более сложные модели могут быть лучше LM. \nlm_model    &lt;- caret::train(R3haddock ~ ., data = train, method = \"lm\", trControl = ctrl)\n\n# --- 2. Обобщённая линейная модель (GLM: Gamma с лог-ссылкой)\n# Учебный смысл: модель для положительных откликов; допускает нелинейность в шкале log.\n# ПОЯСНЕНИЕ: GLM с Gamma-распределением подходит для положительных непрерывных данных \n# (как размер популяции), где дисперсия зависит от среднего значения.\nglm_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"glm\",\n                            family = Gamma(link = \"log\"), trControl = ctrl)\n\n# --- 3. Обобщённая аддитивная модель (GAM, mgcv: bs=\"tp\", REML, select=TRUE)\n# Учебный смысл: гибкие гладкие нелинейности по каждому предиктору.\n# ПОЯСНЕНИЕ: GAM позволяет моделировать нелинейные зависимости с помощью гладких функций (splines),\n# сохраняя интерпретируемость отдельных эффектов. Это компромисс между простотой LM и сложностью ML.\ngam_model   &lt;- caret::train(x = train[, -which(names(train)==\"R3haddock\")],\n                            y = train$R3haddock, method = gam_spec, trControl = ctrl)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\n# --- 4. Random Forest (rf: ntree=1000, mtry=1)\n# Учебный смысл: ансамбль деревьев; устойчив к шуму; нелинейности/взаимодействия \"из коробки\".\n# ПОЯСНЕНИЕ: Random Forest строит множество деревьев решений и усредняет их результаты.\n# Это мощный метод, который автоматически улавливает нелинейные зависимости и взаимодействия. \nrf_model    &lt;- caret::train(R3haddock ~ ., data = train, method = \"rf\", trControl = ctrl,\n                            ntree = 1000, tuneGrid = data.frame(mtry = 1), importance = TRUE)\n\n# --- 5. XGBoost (xgbTree) \n# Учебный смысл: бустинг деревьев; сильная ML-модель, легко переобучается без валидации.\n# ПОЯСНЕНИЕ: XGBoost - это градиентный бустинг над деревьями решений, который последовательно \n# строит деревья, исправляя ошибки предыдущих. Требует тщательной настройки параметров.\nxgb_grid    &lt;- expand.grid(nrounds=100, max_depth=4, eta=0.1, gamma=0,\n                           colsample_bytree=0.8, min_child_weight=1, subsample=0.8)\nxgb_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"xgbTree\",\n                            trControl = ctrl, tuneGrid = xgb_grid, verbose = 0)\n\n# --- 6. Нейросеть (MLP, nnet: линейный выход, стандартизация)\n# Учебный смысл: универсальный аппроксиматор; чувствителен к масштабу; требует регуляризации.\n# ПОЯСНЕНИЕ: Нейронные сети могут моделировать сложные нелинейные отношения. \n# Используемая архитектура (1 скрытый слой) - компромисс между гибкостью и риском переобучения.\n# Линейный выходной слой подходит для регрессии.\nnnet_model  &lt;- caret::train(R3haddock ~ ., data = train, method = \"nnet\",\n                            trControl = ctrl, preProcess = c(\"center\",\"scale\"),\n                            tuneGrid = expand.grid(size = 5, decay = 0.1),\n                            linout = TRUE, trace = FALSE, MaxNWts = 5000)\n\n# --- 7. Elastic Net (glmnet)\n# Учебный смысл: регуляризация (L1/L2), борьба с мультиколлинеарностью, частичный отбор признаков.\n# ПОЯСНЕНИЕ: Комбинирует L1 (лассо) и L2 (ридж) регуляризации. Автоматически отбирает признаки \n# и уменьшает влияние мультиколлинеарности. Параметр alpha балансирует между лассо и риджем.\nglmnet_model&lt;- caret::train(R3haddock ~ ., data = train, method = \"glmnet\",\n                            trControl = ctrl, preProcess = c(\"center\",\"scale\"), tuneLength = 10)\n\n# --- 8. MARS (earth)\n# Учебный смысл: кусочно-линейные сплайны + простые взаимодействия; гибкая интерпретация.\n# ПОЯСНЕНИЕ: Многомерные адаптивные регрессионные сплайны (MARS) строят кусочно-линейные модели \n# с автоматическим выбором точек излома. Поддерживает взаимодействия ограниченного порядка.\nearth_model &lt;- caret::train(R3haddock ~ ., data = train, method = \"earth\",\n                            trControl = ctrl, tuneLength = 10)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\n# --- 9. SVM с радиальным ядром (svmRadial)\n# Учебный смысл: ядровой метод; улавливает сложные нелинейности; важна стандартизация.\n# ПОЯСНЕНИЕ: Метод опорных векторов с радиальным ядром проецирует данные в пространство \n# высокой размерности, где становится возможным линейное разделение. Параметр gamma управляет \n# гибкостью границы решения.\nsvm_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"svmRadial\",\n                            trControl = ctrl, preProcess = c(\"center\",\"scale\"), tuneLength = 8)\n\n# --- 10. k-ближайших соседей (kNN)\n# Учебный смысл: простая интуитивная нелинейная модель на расстояниях; чувствительна к масштабу.\n# ПОЯСНЕНИЕ: Предсказание основано на усреднении значений k ближайших наблюдений. \n# Требует вычисления попарных расстояний, что может быть ресурсоемким при больших данных.\nknn_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"knn\",\n                            trControl = ctrl, preProcess = c(\"center\",\"scale\"), tuneLength = 15)\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 23 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 25 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 27 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 29 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 31 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.54402860027016,\n-1.04267060653844, : k = 33 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 23 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 25 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 27 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 29 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 31 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.32034464856588,\n-0.795974449614684, : k = 33 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59980040948893,\n-0.134264066935858, : k = 25 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59980040948893,\n-0.134264066935858, : k = 27 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59980040948893,\n-0.134264066935858, : k = 29 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59980040948893,\n-0.134264066935858, : k = 31 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59980040948893,\n-0.134264066935858, : k = 33 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 23 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 25 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 27 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 29 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 31 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.47821802102901,\n-0.982937987281781, : k = 33 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.07995722209223,\n-0.0328010741655768, : k = 25 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.07995722209223,\n-0.0328010741655768, : k = 27 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.07995722209223,\n-0.0328010741655768, : k = 29 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.07995722209223,\n-0.0328010741655768, : k = 31 exceeds number 23 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.07995722209223,\n-0.0328010741655768, : k = 33 exceeds number 23 of patterns\n\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\n# --- 11. Ranger (быстрый Random Forest)\n# Учебный смысл: альтернативная/быстрая реализация леса; сравнить с randomForest.\n# ПОЯСНЕНИЕ: Оптимизированная реализация Random Forest на C++. Поддерживает распараллеливание \n# и эффективную работу с категориальными переменными. Важен параметр mtry (число признаков в узле).\nranger_model&lt;- caret::train(R3haddock ~ ., data = train, method = \"ranger\",\n                            trControl = ctrl, tuneLength = 3, importance = \"impurity\")\n\n# --- 12. GBM (классический градиентный бустинг)\n# Учебный смысл: другой бустинг деревьев; полезно сравнить с XGBoost.\n# ПОЯСНЕНИЕ: Градиентный бустинг строит деревья последовательно, где каждое новое дерево \n# корректирует ошибки предыдущих. Параметр shrinkage (темп обучения) контролирует скорость обучения.\ngbm_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"gbm\",\n                            trControl = ctrl,\n                            tuneGrid = expand.grid(n.trees=100, interaction.depth=1,\n                                                   shrinkage=0.1, n.minobsinnode=2),\n                            distribution = \"gaussian\", bag.fraction = 1, verbose = FALSE)\n\n# --- 13. PLS (Partial Least Squares)\n# Учебный смысл: проекция на скрытые компоненты с учетом отклика; решает мультиколлинеарность.\n# ПОЯСНЕНИЕ: Частные наименьшие квадраты (PLS) проецируют предикторы в латентное пространство, \n# максимизируя ковариацию с откликом. Эффективен при высокой корреляции признаков.\npls_model   &lt;- caret::train(R3haddock ~ ., data = train, method = \"pls\",\n                            trControl = ctrl, preProcess = c(\"center\",\"scale\"), tuneLength = 10)\n\n# --- 14. Cubist (правила + деревья)\n# Учебный смысл: интерпретируемые правила с комитетами; часто силен на табличных данных.\n# ПОЯСНЕНИЕ: Cubist объединяет деревья решений с линейными моделями в листьях. Генерирует \n# набор правил \"если-то\", что улучшает интерпретируемость. Комитеты (комитеты) уменьшают дисперсию.\ncubist_model&lt;- caret::train(R3haddock ~ ., data = train, method = \"cubist\",\n                            trControl = ctrl, tuneLength = 5)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\n# 2.5 Метрики и оценка на тесте\n# Замечание: RMSE/MAE — абсолютные ошибки; R2 — доля объяснённой вариации;\n# MAPE/sMAPE — относительные ошибки (осторожно при малых значениях отклика).\nrmse  &lt;- function(a, p) sqrt(mean((a - p)^2, na.rm = TRUE))\nmae   &lt;- function(a, p) mean(abs(a - p), na.rm = TRUE)\nr2    &lt;- function(a, p) 1 - sum((a - p)^2, na.rm = TRUE) / sum((a - mean(a))^2, na.rm = TRUE)\nmape  &lt;- function(a, p) mean(abs((a - p) / a), na.rm = TRUE) * 100\nsmape &lt;- function(a, p) mean(2 * abs(p - a) / (abs(a) + abs(p)), na.rm = TRUE) * 100\nmetrics_vec &lt;- function(y, pred) c(RMSE=rmse(y,pred), MAE=mae(y,pred), R2=r2(y,pred),\n                                   MAPE=mape(y,pred), sMAPE=smape(y,pred))\n\n# Для оценки качества моделей мы используем несколько метрик:\n#   - RMSE (Root Mean Square Error): среднеквадратичная ошибка (чувствительна к выбросам)\n#   - MAE (Mean Absolute Error): средняя абсолютная ошибка (более интерпретируема)\n#   - R²: коэффициент детерминации (доля объясненной дисперсии)\n#   - MAPE: средняя абсолютная процентная ошибка (в процентах от фактического значения)\n#   - sMAPE: симметричная MAPE (устраняет проблему деления на ноль) \n\ny_test &lt;- test$R3haddock\npreds_test &lt;- list(\n  LM=predict(lm_model,test), GLM=predict(glm_model,test), GAM=predict(gam_model,test),\n  RF=predict(rf_model,test), XGB=predict(xgb_model,test), NNET=predict(nnet_model,test),\n  ENet=predict(glmnet_model,test), MARS=predict(earth_model,test), SVM=predict(svm_model,test),\n  kNN=predict(knn_model,test), RANGER=predict(ranger_model,test), GBM=predict(gbm_model,test),\n  PLS=predict(pls_model,test), CUBIST=predict(cubist_model,test)\n)\nmetrics_table &lt;- do.call(rbind, lapply(names(preds_test), function(nm){\n  data.frame(Model = nm, t(metrics_vec(y_test, preds_test[[nm]])), row.names = NULL)\n})) %&gt;% arrange(RMSE, MAE)\n\n# Создаем копию таблицы для округления\nmetrics_table_rounded &lt;- metrics_table\n\n# Находим индексы числовых столбцов (исключая первый столбец \"Model\")\nnumeric_cols &lt;- sapply(metrics_table_rounded, is.numeric)\n\n# Округляем только числовые столбцы до 2 знаков\nmetrics_table_rounded[numeric_cols] &lt;- round(metrics_table_rounded[numeric_cols], 2)\n\n# Выводим округленную таблицу\nprint(metrics_table_rounded)\n\n    Model      RMSE       MAE    R2   MAPE sMAPE\n1     GBM  65112.49  55805.52  0.76  34.41 27.07\n2    MARS  95150.47  78452.71  0.49  51.69 36.18\n3      RF 125056.40  97862.47  0.12  74.27 44.45\n4     PLS 130411.86 111694.80  0.04  62.51 47.69\n5      LM 131592.23 113063.92  0.02  63.62 48.29\n6     GAM 140393.68 121236.06 -0.11  70.56 48.99\n7  CUBIST 147031.92 123320.33 -0.22  50.52 46.56\n8    ENet 147470.05 124147.91 -0.23  81.27 52.96\n9  RANGER 148623.73 127762.15 -0.25  87.21 54.25\n10    kNN 149082.19 128657.28 -0.26  88.53 53.55\n11    GLM 151391.83 129510.77 -0.30  74.40 55.01\n12    SVM 167864.37 147269.48 -0.59 104.73 57.83\n13    XGB 208719.19 194346.48 -1.46  88.97 65.24\n14   NNET 216172.26 199594.44 -1.64  93.12 87.93\n\nknitr::kable(\n  metrics_table %&gt;% dplyr::mutate(dplyr::across(where(is.numeric), ~round(.x, 2))),\n  caption = \"Holdout-метрики (округлено до 2 знаков)\"\n)\n\n\nHoldout-метрики (округлено до 2 знаков)\n\n\nModel\nRMSE\nMAE\nR2\nMAPE\nsMAPE\n\n\n\n\nGBM\n65112.49\n55805.52\n0.76\n34.41\n27.07\n\n\nMARS\n95150.47\n78452.71\n0.49\n51.69\n36.18\n\n\nRF\n125056.40\n97862.47\n0.12\n74.27\n44.45\n\n\nPLS\n130411.86\n111694.80\n0.04\n62.51\n47.69\n\n\nLM\n131592.23\n113063.92\n0.02\n63.62\n48.29\n\n\nGAM\n140393.68\n121236.06\n-0.11\n70.56\n48.99\n\n\nCUBIST\n147031.92\n123320.33\n-0.22\n50.52\n46.56\n\n\nENet\n147470.05\n124147.91\n-0.23\n81.27\n52.96\n\n\nRANGER\n148623.73\n127762.15\n-0.25\n87.21\n54.25\n\n\nkNN\n149082.19\n128657.28\n-0.26\n88.53\n53.55\n\n\nGLM\n151391.83\n129510.77\n-0.30\n74.40\n55.01\n\n\nSVM\n167864.37\n147269.48\n-0.59\n104.73\n57.83\n\n\nXGB\n208719.19\n194346.48\n-1.46\n88.97\n65.24\n\n\nNNET\n216172.26\n199594.44\n-1.64\n93.12\n87.93\n\n\n\n\n# 2.6 CV-резюме\n# Сводим результаты CV по всем моделям и смотрим распределения ошибок.\nresults &lt;- caret::resamples(list(\n  LM=lm_model, GLM=glm_model, GAM=gam_model, RF=rf_model, XGB=xgb_model, NNET=nnet_model,\n  ENet=glmnet_model, MARS=earth_model, SVM=svm_model, kNN=knn_model, RANGER=ranger_model,\n  GBM=gbm_model, PLS=pls_model, CUBIST=cubist_model\n))\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: LM, GLM, GAM, RF, XGB, NNET, ENet, MARS, SVM, kNN, RANGER, GBM, PLS, CUBIST \nNumber of resamples: 5 \n\nMAE \n            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLM      57204.50 173765.7 178162.9 167545.5 210890.4 217704.1    0\nGLM    121162.89 123189.9 126872.5 161564.3 215648.4 220947.6    0\nGAM    113581.17 186001.4 194396.8 216924.0 243663.3 346977.3    0\nRF     140917.34 173192.3 183106.9 211428.5 199504.4 360421.6    0\nXGB    191739.57 196545.5 204224.0 220586.8 211582.9 298842.3    0\nNNET   210800.50 230696.3 231946.6 255228.9 268869.5 333831.8    0\nENet    95118.26 114161.5 186843.2 175901.3 203787.5 279596.0    0\nMARS   100715.25 160470.7 226674.7 224830.7 281753.1 354539.8    0\nSVM    137655.75 168734.2 186772.6 218295.0 270224.5 328087.8    0\nkNN    153725.61 173448.8 204644.5 201592.1 210974.4 265167.1    0\nRANGER 134039.92 173499.6 173657.3 188631.8 213348.0 248614.3    0\nGBM    142143.64 174377.4 182594.4 195116.5 210961.4 265505.7    0\nPLS    140180.64 169826.6 174374.4 174539.4 177221.3 211093.9    0\nCUBIST  40943.57 166731.9 172415.9 151410.2 183157.2 193802.4    0\n\nRMSE \n            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nLM      83394.05 204460.2 213949.1 198791.4 234397.7 257756.2    0\nGLM    144920.86 174777.3 199683.5 211248.8 268416.8 268445.7    0\nGAM    126481.10 212799.6 231166.1 251450.9 265213.5 421594.3    0\nRF     152505.15 221538.9 237863.1 264290.3 246285.2 463259.3    0\nXGB    224482.10 230814.5 274803.3 282033.4 324393.6 355673.5    0\nNNET   262262.48 292982.1 298735.5 322398.1 312361.7 445649.0    0\nENet    96405.72 194703.2 212260.2 215307.8 227408.1 345761.6    0\nMARS   133708.92 174187.4 275486.7 264992.8 296803.1 444778.0    0\nSVM    175110.30 208575.8 211026.8 261495.0 330657.7 382104.5    0\nkNN    191674.11 194743.2 293783.4 261451.2 307657.4 319397.8    0\nRANGER 179305.47 214964.7 252239.7 245471.8 256030.3 324818.7    0\nGBM    155235.18 236112.1 265637.9 269271.4 342304.1 347067.5    0\nPLS    192456.23 204052.4 206556.2 210244.7 207702.6 240455.9    0\nCUBIST  51000.20 217680.6 220028.3 190972.0 226862.3 239288.7    0\n\nRsquared \n              Min.    1st Qu.     Median      Mean   3rd Qu.      Max. NA's\nLM     0.038191154 0.34151126 0.59570117 0.5146318 0.6446897 0.9530659    0\nGLM    0.036094303 0.25361489 0.52140959 0.5247907 0.8499520 0.9628829    0\nGAM    0.078247285 0.20151523 0.39934478 0.3553497 0.5269644 0.5706765    0\nRF     0.007930602 0.01517051 0.11630358 0.1925039 0.1207826 0.7023323    0\nXGB    0.041587807 0.09841163 0.11696588 0.2060251 0.2118923 0.5612680    0\nNNET   0.018389559 0.02872886 0.07737108 0.1597232 0.1157484 0.5583779    0\nENet   0.170405604 0.55004140 0.62684865 0.5443111 0.6741520 0.7001081    0\nMARS   0.007289893 0.04026192 0.24769274 0.3088361 0.3755802 0.8733557    0\nSVM    0.044855520 0.06565037 0.24061923 0.2637711 0.4593310 0.5083994    0\nkNN    0.001714988 0.10251708 0.41997970 0.3013771 0.4201459 0.5625279    0\nRANGER 0.006761073 0.20688755 0.22338943 0.2919063 0.4553028 0.5671905    0\nGBM    0.056729819 0.15259735 0.27867315 0.2779509 0.3246546 0.5770994    0\nPLS    0.255560646 0.39058052 0.43207618 0.5269205 0.6127945 0.9435907    0\nCUBIST 0.004399571 0.20141307 0.52557866 0.5053060 0.8675585 0.9275803    0\n\nlattice::dotplot(results, metric = \"RMSE\")\n\n\n\n\n\n\n\n# ==============================================================================\n# 3) ВЫБОР ЛУЧШЕЙ ПРОГНОСТИЧЕСКОЙ МОДЕЛИ (TIME-SLICE CV НА 3 ГОДА + ХРОНО-ТЕСТ)\n# Делим последние годы в тест, внутри train — скользящее окно, h=3.\n# ------------------------------------------------------------------------------\n# Почему time-slice: временные данные нельзя случайно перемешивать, иначе мы\n# «подсматриваем в будущее». Создаём серии обучающих/валидационных окон,\n# увеличивая тренировочный период, и тестируем на ближайшем горизонте (3 года).\n# ==============================================================================\n\n# 3.1 Данные для time-slice (с YEAR)\nmodel_data &lt;- read.csv(\"selected_predictors_dataset.csv\", header = TRUE, stringsAsFactors = FALSE)\nif (!\"YEAR\" %in% names(model_data)) {\n  model_data$YEAR &lt;- seq(1990, by = 1, length.out = nrow(model_data))\n}\n# Хронологический порядок\nmodel_data &lt;- model_data %&gt;% arrange(YEAR)\n\n# Для временных рядов обычные методы кросс-валидации (случайное разбиение) неприменимы,\n# так как это приведет к утечке информации из будущего в прошлое. [[1]]\n# Time-slice CV (скользящее окно) имитирует реальную ситуацию прогнозирования:\n#   - Мы обучаемся на данных из прошлого\n#   - Прогнозируем на несколько шагов вперед\n#   - Последовательно сдвигаем окно обучения вперед\n\n# Исходные фичи (исключаем YEAR)\nmd_for_fit &lt;- model_data %&gt;% select(codTSB, T12, I5, NAOspring, haddock68, R3haddock)\n\n# 3.2 Хронологический holdout (последние годы)\n# Идея: отложим ~20% последних лет как полностью внешний тест будущего качества.\nn &lt;- nrow(md_for_fit)\nholdout_frac &lt;- 0.2\nn_test &lt;- max(4, ceiling(n * holdout_frac))\ntrain_ts &lt;- head(md_for_fit, n - n_test)\ntest_ts  &lt;- tail(md_for_fit, n_test)\n\n# 3.3 Time-slice CV (h=3, expanding window рекомендован: fixedWindow=FALSE)\n# initialWindow — размер первого «обучающего» фрагмента; horizon — горизонт\n# валидации (здесь 3 года). Далее окно расширяется.\nn_train &lt;- nrow(train_ts)\ninitial_frac &lt;- 0.6\nhorizon      &lt;- 3\ninitialWindow &lt;- max(10, floor(initial_frac * n_train))\nif (initialWindow + horizon &gt; n_train) initialWindow &lt;- n_train - horizon\n\nslices &lt;- caret::createTimeSlices(1:n_train, initialWindow = initialWindow,\n                                  horizon = horizon, fixedWindow = FALSE)\nctrl_ts &lt;- caret::trainControl(method = \"cv\", index = slices$train, indexOut = slices$test,\n                               savePredictions = \"final\")\n\n# В нашем случае:\n#   - horizon = 3: прогнозируем на 3 года вперед\n#   - expanding window: размер обучающей выборки увеличивается с каждым шагом\n#   - initialWindow: начальный размер обучающей выборки (60% от данных)\n# Этот подход наиболее реалистичен для задач прогнозирования временных рядов в гидробиологии.\n\n\n# 3.4 Обучение (ядро набора, без GBM — он нестабилен на малом n в timeslice)\n# Примечание: используем ту же рецептуру, что и в базовом сравнении, но с\n# хронологическими срезами.\n\nfit_ts &lt;- function(method, form, data, ctrl, ...) {\n  out &lt;- try(caret::train(form, data = data, method = method, trControl = ctrl, ...), TRUE)\n  if (inherits(out,\"try-error\")) NULL else out\n}\nlm_ts   &lt;- fit_ts(\"lm\",        R3haddock ~ ., train_ts, ctrl_ts)\nglm_ts  &lt;- fit_ts(\"glm\",       R3haddock ~ ., train_ts, ctrl_ts, family = Gamma(link=\"log\"))\ngam_ts  &lt;- caret::train(x = train_ts[, -which(names(train_ts)==\"R3haddock\")],\n                        y = train_ts$R3haddock, method = gam_spec, trControl = ctrl_ts)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nrf_ts   &lt;- fit_ts(\"rf\",        R3haddock ~ ., train_ts, ctrl_ts, ntree=1000, tuneGrid=data.frame(mtry=1))\nxgb_ts  &lt;- fit_ts(\"xgbTree\",   R3haddock ~ ., train_ts, ctrl_ts, tuneGrid = xgb_grid, verbose = 0)\nrgr_ts  &lt;- fit_ts(\"ranger\",    R3haddock ~ ., train_ts, ctrl_ts, tuneLength=3)\nnnet_ts &lt;- fit_ts(\"nnet\",      R3haddock ~ ., train_ts, ctrl_ts,\n                  preProcess=c(\"center\",\"scale\"),\n                  tuneGrid=expand.grid(size=5,decay=0.1), linout=TRUE, trace=FALSE, MaxNWts=5000)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nsvm_ts  &lt;- fit_ts(\"svmRadial\", R3haddock ~ ., train_ts, ctrl_ts, preProcess=c(\"center\",\"scale\"), tuneLength=8)\nknn_ts  &lt;- fit_ts(\"knn\",       R3haddock ~ ., train_ts, ctrl_ts, preProcess=c(\"center\",\"scale\"), tuneLength=15)\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 17 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 19 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 21 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 23 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 25 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 27 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 29 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 31 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.91776861098288,\n-0.63635090426016, : k = 33 exceeds number 15 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 17 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 19 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 21 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 23 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 25 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 27 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 29 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 31 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.97509275968546,\n-0.649515010512528, : k = 33 exceeds number 16 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 19 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 21 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 23 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 25 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 27 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 29 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 31 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.03591114922526,\n-0.667021070399982, : k = 33 exceeds number 17 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 19 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 21 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 23 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 25 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 27 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 29 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 31 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-2.09685135650479,\n-0.723233808010845, : k = 33 exceeds number 18 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 21 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 23 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 25 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 27 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 29 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 31 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.87781906605955,\n-0.736313775515053, : k = 33 exceeds number 19 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 21 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 23 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 25 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 27 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 29 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 31 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.59299667707382,\n-0.714710653867683, : k = 33 exceeds number 20 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 23 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 25 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 27 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 29 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 31 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.44471091878981,\n-0.719611760680745, : k = 33 exceeds number 21 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 23 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 25 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 27 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 29 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 31 exceeds number 22 of patterns\n\n\nWarning in knnregTrain(train = structure(c(-1.35845826709587,\n-0.734816317064085, : k = 33 exceeds number 22 of patterns\n\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nenet_ts &lt;- fit_ts(\"glmnet\",    R3haddock ~ ., train_ts, ctrl_ts, preProcess=c(\"center\",\"scale\"), tuneLength=10)\nmars_ts &lt;- fit_ts(\"earth\",     R3haddock ~ ., train_ts, ctrl_ts, tuneLength=10)\npls_ts  &lt;- fit_ts(\"pls\",       R3haddock ~ ., train_ts, ctrl_ts, preProcess=c(\"center\",\"scale\"), tuneLength=10)\ncub_ts  &lt;- fit_ts(\"cubist\",    R3haddock ~ ., train_ts, ctrl_ts, tuneLength=5)\n\nmodels_ts &lt;- list(LM=lm_ts, GLM=glm_ts, GAM=gam_ts, RF=rf_ts, XGB=xgb_ts, RANGER=rgr_ts,\n                  NNET=nnet_ts, SVM=svm_ts, kNN=knn_ts, ENet=enet_ts, MARS=mars_ts, PLS=pls_ts, CUBIST=cub_ts)\nmodels_ts &lt;- models_ts[!vapply(models_ts, is.null, logical(1))]\n\n# 3.5 Ранжирование по time-slice CV и по хронологическому тесту\n# Сначала ранжируем по средним ошибкам на валидационных срезах, затем — по внешнему тесту.\ncv_metrics &lt;- function(m) {\n  if (is.null(m$pred) || !\"Resample\" %in% names(m$pred)) return(c(RMSE=NA, MAE=NA))\n  by_slice &lt;- m$pred %&gt;% group_by(Resample) %&gt;%\n    summarise(RMSE=rmse(obs,pred), MAE=mae(obs,pred), .groups=\"drop\")\n  c(RMSE = mean(by_slice$RMSE, na.rm = TRUE), MAE = mean(by_slice$MAE, na.rm = TRUE))\n}\ncv_rank &lt;- do.call(rbind, lapply(models_ts, cv_metrics)) %&gt;% as.data.frame()\ncv_rank$Model &lt;- rownames(cv_rank)\ncv_rank &lt;- cv_rank[is.finite(cv_rank$RMSE), ] %&gt;% relocate(Model) %&gt;% arrange(RMSE, MAE)\ncat(\"\\nTime-slice CV (h=3), средние RMSE/MAE:\\n\"); print(cv_rank)\n\n\nTime-slice CV (h=3), средние RMSE/MAE:\n\n\n        Model     RMSE      MAE\nSVM       SVM 227929.6 191921.6\nkNN       kNN 234897.8 197135.0\nENet     ENet 250989.0 214248.7\nXGB       XGB 277153.4 248532.6\nRANGER RANGER 280255.1 249992.1\nGLM       GLM 280259.5 237186.9\nNNET     NNET 296856.1 264368.2\nPLS       PLS 302968.3 274707.6\nRF         RF 303710.0 263019.5\nCUBIST CUBIST 314443.4 281437.9\nLM         LM 370298.1 340883.8\nMARS     MARS 427624.1 378476.5\nGAM       GAM 714951.0 625520.3\n\npreds_ts &lt;- lapply(models_ts, function(m) try(predict(m, newdata = test_ts), TRUE))\nkeep &lt;- vapply(preds_ts, function(p) is.numeric(p) && length(p)==nrow(test_ts) && all(is.finite(p)), logical(1))\npreds_ts &lt;- preds_ts[keep]\ntest_rank &lt;- do.call(rbind, lapply(names(preds_ts), function(nm){\n  data.frame(Model=nm, t(metrics_vec(test_ts$R3haddock, preds_ts[[nm]])), row.names = NULL)\n})) %&gt;% arrange(RMSE, MAE)\ncat(\"\\nХронологический тест (последние годы), RMSE/MAE/R2:\\n\"); print(test_rank)\n\n\nХронологический тест (последние годы), RMSE/MAE/R2:\n\n\n    Model     RMSE      MAE         R2      MAPE     sMAPE\n1  CUBIST 148248.4 107629.3  0.5774780  54.93724  38.06342\n2      LM 156940.0 129604.7  0.5264822  97.30313  49.10292\n3     GAM 158131.0 125038.0  0.5192677  51.33030  42.12742\n4     PLS 176786.2 138249.0  0.3991499 123.21995  50.93728\n5     GLM 182047.7 141692.4  0.3628531  73.71724  50.32932\n6      RF 185485.8 148117.2  0.3385600  96.29497  52.15688\n7     kNN 187966.9 143164.4  0.3207460  71.77551  50.80097\n8    ENet 195260.5 161191.7  0.2670102 115.55953  56.27135\n9  RANGER 208161.4 171717.0  0.1669526 115.93679  58.76935\n10    SVM 250994.6 197801.6 -0.2111500  84.52010  67.59732\n11   MARS 273186.2 208579.6 -0.4347849  85.29788  71.46314\n12    XGB 288167.4 233693.4 -0.5964639 102.19238  78.99024\n13   NNET 345227.3 291463.2 -1.2912878 233.62022 102.82204\n\n# ==============================================================================\n# 4) ПРОГНОЗ 2022–2024 (АНСАМБЛЬ CUBIST+LM) И ГРАФИК 1990–2024 С ДИ\n# Прогнозные линии (медиана и ДИ) — пунктир; исторические — сплошные.\n# Можно задать свои сценарии предикторов (user_future); по умолчанию — средние.\n# ------------------------------------------------------------------------------\n# Логика ансамбля: комбинируем сильную нелинейную модель (Cubist) с простой и\n# устойчивой линейной (LM). Веса можно настраивать. Доверительные интервалы\n# получаем эмпирически из распределения остатков (простая и наглядная эвристика).\n# ==============================================================================\n\n# 4.1 Полные модели для прогноза (на всех данных) и вес ансамбля\nmodel_data &lt;- read.csv(\"selected_predictors_dataset.csv\", header = TRUE, stringsAsFactors = FALSE)\nif (!\"YEAR\" %in% names(model_data)) {\n  model_data$YEAR &lt;- seq(1990, by = 1, length.out = nrow(model_data))\n}\nmodel_data &lt;- model_data %&gt;% arrange(YEAR)\n\ncubist_full &lt;- caret::train(R3haddock ~ codTSB + T12 + I5 + NAOspring + haddock68,\n                            data = model_data, method = \"cubist\",\n                            trControl = caret::trainControl(method=\"none\"),\n                            tuneGrid = if (exists(\"cubist_model\")) cubist_model$bestTune else NULL,\n                            tuneLength = if (exists(\"cubist_model\")) 1 else 5)\n\nlm_full &lt;- caret::train(R3haddock ~ codTSB + T12 + I5 + NAOspring + haddock68,\n                        data = model_data, method = \"lm\",\n                        trControl = caret::trainControl(method=\"none\"))\n\nalpha_opt &lt;- if (exists(\"alpha_opt\")) alpha_opt else 0.75\npredict_ensemble &lt;- function(newdata, alpha = alpha_opt) {\n  alpha * predict(cubist_full, newdata) + (1 - alpha) * predict(lm_full, newdata)\n}\n\n# Ансамбль моделей часто дает более точные и устойчивые прогнозы, чем отдельные модели. [[8]]\n# В нашем случае:\n#   - CUBIST: мощная модель, основанная на правилах, хорошо работающая с табличными данными\n#   - LM: простая интерпретируемая модель, устойчивая к шуму\n#   - alpha_opt = 0.75: веса ансамбля (75% CUBIST, 25% LM), оптимизированные ранее (см. скрипт \"ENS_WEIGHT.R\")\n# Комбинирование моделей с разными сильными сторонами снижает риск систематических ошибок.\n\n# 4.2 Остатки для ДИ (из CV, если есть; иначе — по фитам)\n# Эмпирические квантилы остатков дают «практические» интервалы прогноза без\n# предположения нормальности ошибок (хотя строгий PI требует аккуратности).\nget_residuals_for_pi &lt;- function() {\n  if (exists(\"lm_model\") && exists(\"cubist_model\") &&\n      !is.null(lm_model$pred) && !is.null(cubist_model$pred)) {\n    pl &lt;- lm_model$pred %&gt;% select(Resample,rowIndex,obs,p_lm=pred)\n    pc &lt;- cubist_model$pred %&gt;% select(Resample,rowIndex,p_cu=pred)\n    inner_join(pl, pc, by=c(\"Resample\",\"rowIndex\")) %&gt;%\n      mutate(p_ens = alpha_opt * p_cu + (1 - alpha_opt) * p_lm,\n             resid = obs - p_ens) %&gt;%\n      pull(resid) %&gt;% .[is.finite(.)]\n  } else {\n    model_data$R3haddock - predict_ensemble(model_data)\n  }\n}\nresids &lt;- get_residuals_for_pi()\nq025 &lt;- as.numeric(quantile(resids, 0.025, na.rm = TRUE))\nq250 &lt;- as.numeric(quantile(resids, 0.250, na.rm = TRUE))\nq750 &lt;- as.numeric(quantile(resids, 0.750, na.rm = TRUE))\nq975 &lt;- as.numeric(quantile(resids, 0.975, na.rm = TRUE))\n\n# Доверительные интервалы (ДИ) показывают неопределенность прогноза.\n# Мы используем квантили остатков из кросс-валидации для построения ДИ:\n#   - PI50 (50% интервал): между 25-м и 75-м процентилями\n#   - PI95 (95% интервал): между 2.5-м и 97.5-м процентилями\n# Это непараметрический подход, не требующий предположений о нормальности ошибок.\n\n# 4.3 Сценарии будущего (по умолчанию — средние; можно переопределить user_future)\nfc_start &lt;- 2022\npred_cols &lt;- c(\"codTSB\",\"T12\",\"I5\",\"NAOspring\",\"haddock68\")\ntrain_period &lt;- model_data %&gt;% filter(YEAR &gt; 1989 & YEAR &lt; fc_start)\nmu &lt;- train_period %&gt;% summarise(across(all_of(pred_cols), ~mean(.x, na.rm = TRUE))) %&gt;% as.list()\n\n# Пример пользовательского сценария:\n# user_future &lt;- tibble::tribble(\n#   ~YEAR, ~codTSB, ~T12, ~I5, ~NAOspring, ~haddock68,\n#   2022, 2100000, 5.1, 48,  0.3, 120000,\n#   2023, 2050000, 4.8, 50, -0.1, 115000,\n#   2024, 2150000, 5.0, 47,  0.2, 118000\n# )\nif (!exists(\"user_future\")) user_future &lt;- NULL\n\nbuild_future &lt;- function(years, mu, user_df=NULL) {\n  df &lt;- tibble::tibble(YEAR = years)\n  for (v in pred_cols) df[[v]] &lt;- mu[[v]]\n  if (!is.null(user_df)) {\n    for (i in seq_len(nrow(user_df))) {\n      yr &lt;- user_df$YEAR[i]\n      if (yr %in% years) {\n        idx &lt;- which(df$YEAR == yr)\n        for (v in intersect(pred_cols, names(user_df))) {\n          val &lt;- user_df[[v]][i]\n          if (!is.na(val)) df[[v]][idx] &lt;- val\n        }\n      }\n    }\n  }\n  df\n}\nfuture_years &lt;- fc_start:2024\nscenario_future &lt;- build_future(future_years, mu, user_future)\n\n# 4.4 Прогноз и таблица ДИ\npred_future &lt;- predict_ensemble(scenario_future)\nforecast_tbl &lt;- tibble::tibble(\n  YEAR      = scenario_future$YEAR,\n  pred_mean = as.numeric(pred_future),\n  PI50_low  = pred_future + q250, PI50_high = pred_future + q750,\n  PI95_low  = pred_future + q025, PI95_high = pred_future + q975\n)\n\n#### Таблица прогноза 2022–2024\nprint(forecast_tbl)\n\n# A tibble: 3 x 6\n   YEAR pred_mean PI50_low PI50_high PI95_low PI95_high\n  &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  2022   253815.   63865.   536150.  -46219.   668189.\n2  2023   253815.   63865.   536150.  -46219.   668189.\n3  2024   253815.   63865.   536150.  -46219.   668189.\n\n# По умолчанию мы используем средние значения предикторов для прогноза.\n# Однако вы можете определить собственный сценарий (user_future), указав конкретные значения\n# для каждого года и каждого предиктора. Это позволяет моделировать различные экологические сценарии. \n\n# 4.5 Непрерывный ряд 1990–2024 и график: ленты сплошные; линии медианы/ДИ — сплошные до 2021, пунктир с 2022\npred_df &lt;- bind_rows(\n  model_data %&gt;% select(YEAR, all_of(pred_cols)),\n  scenario_future\n) %&gt;% distinct(YEAR, .keep_all = TRUE) %&gt;% arrange(YEAR)\n\npred_df$Pred      &lt;- as.numeric(predict_ensemble(pred_df))\npred_df$PI50_low  &lt;- pred_df$Pred + q250\npred_df$PI50_high &lt;- pred_df$Pred + q750\npred_df$PI95_low  &lt;- pred_df$Pred + q025\npred_df$PI95_high &lt;- pred_df$Pred + q975\n\nhist_df &lt;- model_data %&gt;% select(YEAR, R3haddock)\n\nggplot() +\n  geom_ribbon(data = pred_df, aes(x = YEAR, ymin = PI95_low, ymax = PI95_high),\n              fill = \"grey80\", alpha = 0.25) +\n  geom_ribbon(data = pred_df, aes(x = YEAR, ymin = PI50_low, ymax = PI50_high),\n              fill = \"grey60\", alpha = 0.35) +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = PI95_low),\n            color = \"grey45\", linewidth = 0.6) +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = PI95_high),\n            color = \"grey45\", linewidth = 0.6) +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = PI50_low),\n            color = \"grey35\", linewidth = 0.6) +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = PI50_high),\n            color = \"grey35\", linewidth = 0.6) +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = PI95_low),\n            color = \"grey45\", linewidth = 0.6, linetype = \"dashed\") +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = PI95_high),\n            color = \"grey45\", linewidth = 0.6, linetype = \"dashed\") +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = PI50_low),\n            color = \"grey35\", linewidth = 0.6, linetype = \"dashed\") +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = PI50_high),\n            color = \"grey35\", linewidth = 0.6, linetype = \"dashed\") +\n  geom_line(data = subset(pred_df, YEAR &lt; fc_start), aes(x = YEAR, y = Pred),\n            color = \"steelblue4\", linewidth = 1) +\n  geom_line(data = subset(pred_df, YEAR &gt;= fc_start-1), aes(x = YEAR, y = Pred),\n            color = \"steelblue4\", linewidth = 1, linetype = \"dashed\") +\n  geom_point(data = hist_df, aes(x = YEAR, y = R3haddock),\n             color = \"black\", size = 2, alpha = 0.9) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0))) +\n  labs(\n    title = \"Пополнение R3haddock: факт (1990–2021) и прогноз (2022–2024)\\nАнсамбль CUBIST+LM; непрерывные ДИ, прогноз — пунктир\",\n    x = \"Год\", y = \"R3haddock\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# На графике:\n#   - Черные точки: исторические данные (1990-2021)\n#   - Сплошная синяя линия: прогнозные значения (1990-2021)\n#   - Пунктирная синяя линия: прогноз на 2022-2024\n#   - Серые ленты: 50% и 95% доверительные интервалы\n# Такая визуализация позволяет легко интерпретировать как исторические данные, \n# так и будущие прогнозы с учетом неопределенности.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Прогноз пополнения: от факторов до ансамбля</span>"
    ]
  },
  {
    "objectID": "chapter 8.html",
    "href": "chapter 8.html",
    "title": "9  Модель Catch-Survey Analysis (CSA)",
    "section": "",
    "text": "9.1 Введение\nПродолжим с честного предупреждения. Как только мы делим запас на осмысленные группы — пререкруты, рекруты, пострекруты — и прописываем переходы между ними, очень легко почувствовать, что мы «внутри механизма», а значит, управляем им. Это та самая иллюзия контроля: формула кажется ближе к биологии, чем агрегатный продукционный баланс, и мозг дорисовывает уверенность, которой в данных может не хватать. Дисциплина «медленного» режима в том, чтобы отделить структуру от знания: CSA и правда даёт больше биологического смысла, но достоверность этого смысла определяется не изяществом уравнений, а качеством наблюдений, идентифицируемостью параметров и тем, как аккуратно мы проверяем альтернативы. У системы есть история, пороги, сдвиги и запаздывания; толстые хвосты и редкие годы «сюрпризов». Наша задача — встроить всё это в анализ, не потеряв прозрачности и воспроизводимости.\nПрактическая ценность CSA в том, что она распаковывает общую динамику на «каналы»: пополнение, рост (переходы между категориями), естественная смертность и изъятие промыслом, а затем связывает скрытые состояния с наблюдаемыми индексами через улавливаемость. Это полезно для объектов с выборочным промыслом и сильно неоднородной размерной структурой: давление смещается по группам, и агрегаты легко маскируют реальные сдвиги. Но вместе с детализацией приходит классическая ловушка идентифицируемости: M путается с q, переходные вероятности (Gp, Gr, Mp) — между собой, процессная дисперсия — с наблюдательной. Если не заякорить параметры априорами и не опереться на внешнюю информацию (калибровки тралов, биологию линьки, разумные диапазоны смертности), модель будет «объяснять» всё и сразу, но в каждом запуске по‑разному. Байесовская постановка — именно способ честно ввести эти якоря и сразу показать, где данные «передвинули» прайеры, а где — нет.\nГлавные источники смещения здесь не банальны. Улавливаемость редко постоянна: меняются суда, орудия, глубины, сезонность; индексы могут отражать доступность, а не истинную численность. Выборочная природа съёмок порождает нули и неоднородную дисперсию; логнормальная модель наблюдений удобна, но не всегда робастна к выбросам. Переходы между группами зависят от роста и линьки, а значит — от среды; если эти зависимости «впитаны» в постоянные Gp/Gr/Mp, то часть динамики будет ложиться в ошибки. Поэтому мы заранее признаём: фиксированные прайеры — осознанный компромисс, а чувствительность к прайерам — обязательная проверка, а не факультатив.\nДиагностика — не приложение, а часть модели. Мы смотрим траектории цепей, эффективный размер выборки, MC‑ошибку, проверяем сходимость независимых цепей и, что не менее важно, сопоставляем прайеры и постериоры по ключевым параметрам: если плотности почти не разошлись, значит, данные нас мало чему научили; если разошлись «до хвостов» — возможно, мы зашли за границы биологически правдоподобного. Постериорные проверочные прогонки (posterior predictive) — простой и мощный тест: генерируем псевдо‑индексы из модели и убеждаемся, что реальная серия не выглядит «инородным телом». Бабл‑графики остатков по группам и годам помогают увидеть систематику: дрейф знака — сигнал к времени‑зависимому q или пропущенным ковариатам. А вероятность нарушить управленческий предел (PR &lt; PRlim) должна выводиться прямо из постериора, а не прикидываться на глаз по одной траектории.\nСтабильность выводов проверяем во времени — ретроспективой и хайндкастом. Отрезая по одному–несколько последних лет и переоценивая модель, мы видим, «дышит» ли оценка прошлых лет от добавления новой информации. Это сохраняет нас от соблазна «подогнать настоящее» и выдать его за прогнозную силу. Там же хорошо выявляются скрытые конфликты идентифицируемости: если при каждом «срезе» меняется баланс M–q или расползаются переходы, значит, данных не хватает или приоры слишком расплывчаты. Такой «проверенный на бордюре» консерватизм — это не скепсис, а инструмент против самоуверенности.\nЧасть неопределённости мы принимаем как данность и переводим в язык решений. Менеджменту нужны не «числа», а вероятности: какова P(PR&lt;PRlim) в текущем году, как меняется она при сценарии улова, каков шанс сохранить PR над порогом при консервативном и при агрессивном изъятии. Картина «веера» — медиана и 50/95% интервалы — честнее единственной жирной линии. И здесь полезна мысль: аккуратно собранные данные и прозрачные процедуры, повторённые из года в год, делают систему разумнее — даже если в каждом конкретном году интервал широк. Прогресс не в том, чтобы угадать до тонны, а в том, чтобы системно сокращать неопределённость и принимать решения, устойчивые к её остаткам.\nПрактическая реализация в этом занятии выдержана в том же ключе. Мы задаём прайеры, учим модель в JAGS, сохраняем полные постериоры параметров и состояний, сравниваем прайеры с постериорами, строим диагностические графики остатков и динамики по группам, считаем вероятность пересечения порога PRlim и даём интерпретацию в управленческих терминах. Там, где это уместно, тестируем чувствительность к диапазонам прайеров на M и q, а также к альтернативам в наблюдательной части (логнормальная дисперсия). И да, Excel‑симулятор на четыре группы — не игрушка, а хороший способ «почувствовать руками» идентифицируемость: как меняется постерирог при фиксации M, при расширении дисперсий наблюдений, при «дрейфе» q. Интуиция, подкреплённая такими играми, экономит много времени в полноценной байесовской оценке.\nНаконец, важная оговорка: CSA — не конечная станция. В данных с явными климатическими сдвигами стоит рассмотреть время‑зависимую улавливаемость, ковариаты для переходов и смертности, иерархическую связку нескольких съёмок. Если в этом блоке мы делаем базовую, учебную версию, то следующий шаг — включать «регуляторы» сложности только после того, как базовая модель пройдёт диагностику. Это тот самый «мост» между ясностью и гибкостью: сперва минимально достаточная структура, затем — аккуратные расширения с прицельной проверкой альтернатив. Так вводится порядок в систему, где соблазнов «знать больше, чем знаем» всегда больше, чем данных.\nИ так, модель “анализа уловов и съемок” - Catch-Survey Analysis (CSA) представляет собой инструмент для оценки состояния запасов, особенно тех видов, данные по индивидуальному возрасту которых труднодоступны или отсутствуют, что типично для многих беспозвоночных, таких как крабы, креветки, а также для некоторых рыб. В отличие от классических продукционных моделей, которые оперируют агрегированными показателями всей популяции и требуют строгих допущений о ее равновесном состоянии и постоянной емкости среды, когортные модели, подобные CSA, позволяют отслеживать судьбу отдельных функциональных категорий (например, пререкруты, рекруты, пострекруты). Они явным образом учитывают такие процессы, как рост, пополнение и естественная смертность, разделяя запас на дискретные размерные или возрастные группы. Это дает несомненное преимущество при анализе динамики популяций с выраженной цикличностью или тех, которые подвергаются интенсивному промысловому прессу, избирательно воздействующему на определенные размерные или возрастные категории (например, пререкруты не подвержены прямой прмысловой смертности в отличие от рекрутов и посрекрутов). Подробнее о модели и ее реализации можно почитать в статье “Результаты применения стохастической когортной модели CSA для оценки запаса камчатского краба Paralithodes camtschaticus в Баренцевом море”. В статье описывается реализация модели в программе OpenBUGS, которая в упрощенном виде (без прогноза, риск-анализа и диагностики) и в учебных целях была переведена в среду R и представлена ниже, а полный срипт здесь.Также доступна иммитационная CSA модель для 4 размерных групп, реализованная в MS Excel по ссылке.\nДанная реализация модели представляет собой байесовский подход к оценке запасов, который позволяет учитывать неопределенности как в процессе динамики популяции, так и в процессе наблюдений, что особенно важно при работе с данными, характеризующимися высокой вариабельностью и неполнотой. В основе модели лежит разделение популяции на три размерно-возрастные группы: пререкруты (P1), рекруты (R) и пострекруты (P), что соответствует биологическим особенностям многих видов крабов, включая камчатского краба. Модель включает два основных компонента: динамику процесса, описывающую естественные изменения численности популяции, и модель наблюдений, связывающую ненаблюдаемые “истинную” численность запаса с доступными данными съемок (индексами численности пререкрутов, рекрутов и пострекрутов). Уравнения процессной динамики для пострекрутов имеют вид:\nP[i] = [(P1[i-1]×Gp×Mp) + R[i-1] + P[i-1] - catch[i-1]] × exp(-M) + εP, где\nGp обозначает вероятность перехода пререкрутов в пострекруты,\nMp - вероятность линьки пререкрутов,\nM - коэффициент естественной смертности, а εP представляет собой процессную ошибку.\nДля рекрутов уравнение динамики выглядит как\nR[i] = (P1[i-1]×Gr×Mp) × exp(-M) + εR, где\nGr - вероятность перехода пререкрутов в рекруты. Динамика пререкрутов моделируется как лог-случайное блуждание P1[i] = P1[i-1] + εP1. Модель наблюдений предполагает, что данные траловых съемок соответствуют логнормальному распределению относительно истинной численности, умноженной на коэффициент улавливаемости:\nbioindexP1[i] ~ lognormal(log(q1×P1[i]), precbioindexP1),\nаналогично для рекрутов и пострекрутов, где q1, q2, q3 - коэффициенты улавливаемости для каждой группы, а precbioindex - параметры точности. В байесовском подходе ключевую роль играют априорные распределения параметров, которые в данной реализации задаются как равномерные для коэффициентов улавливаемости (q1, q2, q3 ~ dunif(0.1,1)), нормальные для вероятностей перехода (Gr ~ dnorm(0.9,500), Gp ~ dnorm(0.075,500), Mp ~ dnorm(0.95,500)) и для коэффициента естественной смертности (M ~ dnorm(0.2,100)). Использование байесовского подхода позволяет не только получить точечные оценки параметров, но и оценить полные апостериорные распределения, что дает возможность проводить риск-анализ различных сценариев управления запасом. В данном занятии мы реализуем модель CSA в среде R с использованием пакетов rjags и coda, что позволяет эффективно работать с байесовскими иерархическими моделями через интерфейс с программой JAGS, которую также необходимо установить.\nМы рассмотрим полный цикл работы с моделью: от подготовки данных и задания априорных распределений до обучения модели и анализа результатов, включая визуализацию априорных и апостериорных распределений параметров, анализ остатков и сравнение моделируемой и фактической динамики запаса. Особое внимание будет уделено интерпретации результатов в контексте управления водными биоресурсами, что является ключевой целью применения подобных моделей в практической деятельности гидробиологов и ихтиологов. ## Загрузка данных и первичный осмотр",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Модель Catch-Survey Analysis (CSA)</span>"
    ]
  },
  {
    "objectID": "chapter 8.html#реализация-модели",
    "href": "chapter 8.html#реализация-модели",
    "title": "9  Модель Catch-Survey Analysis (CSA)",
    "section": "9.2 Реализация модели",
    "text": "9.2 Реализация модели\n\n# ========================================================================================================================\n# ПРАКТИЧЕСКОЕ ЗАНЯТИЕ: МОДЕЛЬ Catch-Survey Analysis (CSA) - три категории (пререкруты (P1), рекруты (R), пострекруты (P)\n# Курс: \"Оценка водных биоресурсов в среде R (для начинающих)\"\n# Автор: Баканев С. В. Дата: 20.08.2025\n# Структура:\n# 1) Входные данные\n# 2) Модель\n# 3) Прайеры\n# 4) Обучение модели\n# 5) Подготовка выходных данных \n# 6) Анализ результатов (визуализация априорных и апостериорных параметров;бабл-плоты остатков;  динамика индексов) \n# ========================================================================================================================\n# Установка рабочей директории\nsetwd(\"C:/CSA\")\n\n# Подключение необходимых библиотек\n# install.packages(c(\"rjags\", \"coda\"))  # Раскомментировать для установки\nlibrary(rjags)  # Для работы с JAGS\n\nЗагрузка требуемого пакета: coda\n\n\nLinked to JAGS 4.3.1\n\n\nLoaded modules: basemod,bugs\n\nlibrary(coda)   # Для анализа MCMC-выхода\nlibrary(ggplot2)# Рисунки\n\n# ========================================================================================================================\n# --- Входные данные ---\n# ========================================================================================================================\ndata_list &lt;- list(\n  N = 16,# Количество временных точек\n # Наблюдаемые данные (индексы запаса)\n  bioindexP1 = c(1500,1028,554,887,1345,1817,2291,1958,1500,1028,554,887,1345,1817,2291,1958),\n  bioindexR  = c(2531,1927,1305,764,1216,   1820,2442,2983,2531,1927,1305,764,1216,1820,2442,2983),\n  bioindexP  = c(13741,13770,13060,11653,9782,8634,8321,8793,9809,10177,9776,9566,8789,8640,9240,10547),\n  catch      = c(6,2,6,15,21,37,37,315,945,890,991,1060,1000,1000,1600,1673,1250)\n)\n\n# Создание вектора лет для подписей\nYEAR &lt;- 2000 + 0:(data_list$N - 1)\n\n# ========================================================================================================================\n# --- Генерация модели CSA --\n# ========================================================================================================================\nmodel_string &lt;- \"\nmodel {\n   for (i in 1:N) {\n    bioindexP1med[i] &lt;- log(1.0E-6 + q1 * P1[i])\n    bioindexP1[i] ~ dlnorm(bioindexP1med[i], precbioindexP1)\n    bioindexRmed[i]  &lt;- log(1.0E-6 + q2 * R[i])\n    bioindexR[i] ~ dlnorm(bioindexRmed[i],  precbioindexR)\n    bioindexPmed[i]  &lt;- log(1.0E-6 + q3 * P[i])\n    bioindexP[i] ~ dlnorm(bioindexPmed[i],  precbioindexP)\n  }\n\n  inv_surv &lt;- exp(-M)# Коэффициент естественной смертности\n  for (i in 2:N) {\n       tmpPraw[i] &lt;- (P1[i-1]*Gp*Mp + R[i-1] + P[i-1] - catch[i-1]) * inv_surv\n    tmpPpos[i] &lt;- tmpPraw[i] * step(tmpPraw[i]) \n    Pmed[i] &lt;- log(1.0E-6 + tmpPpos[i])\n    P[i] ~ dlnorm(Pmed[i], precP)\n\n    tmpRraw[i] &lt;- (P1[i-1]*Gr*Mp) * inv_surv\n    tmpRpos[i] &lt;- tmpRraw[i] * step(tmpRraw[i])\n    Rmed[i] &lt;- log(1.0E-6 + tmpRpos[i])\n    R[i] ~ dlnorm(Rmed[i], precR)\n\n    P1med[i] &lt;- log(1.0E-6 + P1[i-1])\n    P1[i] ~ dlnorm(P1med[i], precP1)\n  }\n\n  for (i in 1:N) {\n    PR[i] &lt;- P[i] + R[i]\n    p.PRlim[i] &lt;- step(PRlim - PR[i])\n  }\n  PRlim &lt;- 4000\n\n\n  P1[1] ~ dunif(200,4000)\n  P[1]  ~ dunif(200,6000)\n  R[1]  ~ dunif(200,25000)\n\n  Gr ~ dnorm(0.9,  500)\n  Gp ~ dnorm(0.075,500)\n  Mp ~ dnorm(0.95, 500)\n\n   precbioindexP1 ~ dgamma(12.22, 1.1)\n  precbioindexR  ~ dgamma(12.22, 1.1)\n  precbioindexP  ~ dgamma(12.22, 1.1)\n\n  q1 ~ dunif(0.1,1)\n  q2 ~ dunif(0.1,1)\n  q3 ~ dunif(0.1,1) \n\n  precP1 ~ dgamma(12.22, 1.1)\n  precR  ~ dgamma(12.22, 1.1)\n  precP  ~ dgamma(12.22, 1.1)\n\n  M ~ dnorm(0.2, 100)\n}\n\"\n\n\n# ========================================================================================================================\n# --- Обучение модели ---\n# ========================================================================================================================\nset.seed(1)  # Для воспроизводимости\n# Инициализация модели JAGS\njm &lt;- jags.model(\n  textConnection(model_string),  # Модель из строки\n  data = data_list,             # Данные\n  n.chains = 3,                 # Количество цепей\n  n.adapt = 1500                # Длина адаптационной фазы\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 48\n   Unobserved stochastic nodes: 61\n   Total graph size: 576\n\nInitializing model\n\n# Обновление модели (burn-in)\nupdate(jm, 4000)\n\n# Переменные для мониторинга\nvars_to_monitor &lt;- c(\n  \"M\",\"Gp\",\"Gr\",\"Mp\",\"q1\",\"q2\",\"q3\",                    # Параметры\n  \"precP\",\"precP1\",\"precR\",\"precbioindexP\",\"precbioindexP1\",\"precbioindexR\",  # Точности\n  \"P\",\"P1\",\"R\",\"PR\",\"p.PRlim\"                           # Состояния и производные\n)\n\n\n# Генерация MCMC-выборок\nsamps &lt;- coda.samples(\n  jm, \n  variable.names = vars_to_monitor,  # Мониторируемые переменные\n  n.iter = 6000,                     # Длина выборки\n  thin = 3                           # Прореживание\n)\n# ========================================================================================================================\n# --- Анализ результатов ---\n# ========================================================================================================================\n# Стандартная статистика по выборкам\nsm &lt;- summary(samps)\nstats &lt;- sm$statistics   # Средние, SD, стандартные ошибки\nquants &lt;- sm$quantiles   # Квантили (2.5%, 25%, 50%, 75%, 97.5%)\n\n# Матрица всех сэмплов для ручных вычислений\ndraws_mat &lt;- as.matrix(samps)\n\n# Функция для расчета MC ошибки через эффективный размер выборки\nmcse_from_ess &lt;- function(vec) {\n  ess &lt;- effectiveSize(as.mcmc(vec))  # Эффективный размер выборки\n  sd(vec) / sqrt(as.numeric(ess))     # MC ошибка\n}\n\n# Функция для создания строки результата\nmake_row &lt;- function(year, mapping, node, mean, sd, mcse, q2.5, q25, q50, q75, q97.5) {\n  data.frame(\n    YEAR = year,\n    `#Vectors to monitor` = mapping,\n    node = node,\n    mean = mean,\n    sd = sd,\n    `MC error` = mcse,\n    `2.50%` = q2.5,\n    `25.00%` = q25,\n    median = q50,\n    `75.00%` = q75,\n    `97.50%` = q97.5,\n    check.names = FALSE\n  )\n}\n\n# Список для накопления результатов\nrows &lt;- list()\n\n# Функция добавления скалярных параметров\nadd_scalar &lt;- function(x_idx, vname) {\n  if (vname %in% rownames(stats)) {\n    # Если параметр есть в готовой статистике\n    m &lt;- stats[vname, \"Mean\"]\n    s &lt;- stats[vname, \"SD\"]\n    mcse &lt;- mcse_from_ess(draws_mat[, vname])\n    q &lt;- quants[vname, c(\"2.5%\", \"25%\", \"50%\", \"75%\", \"97.5%\")]\n    rows[[length(rows) + 1]] &lt;&lt;- make_row(NA, paste0(\"x[\", x_idx, \"]&lt;-\", vname), paste0(\"x[\", x_idx, \"]\"),\n                                          m, s, mcse, q[1], q[2], q[3], q[4], q[5])\n  } else if (vname %in% c(\"sigmaP1\",\"sigmaR\",\"sigmaP\")) {\n    # Для стандартных отклонений (преобразуем из точности)\n    src &lt;- switch(vname,\n                  sigmaP1 = \"precP1\",\n                  sigmaR  = \"precR\",\n                  sigmaP  = \"precP\")\n    if (src %in% colnames(draws_mat)) {\n      vec &lt;- sqrt(1 / draws_mat[, src])  # Преобразование precision -&gt; sigma\n      m &lt;- mean(vec); s &lt;- sd(vec); mcse &lt;- mcse_from_ess(vec)\n      q &lt;- quantile(vec, c(0.025,0.25,0.5,0.75,0.975))\n      rows[[length(rows) + 1]] &lt;&lt;- make_row(NA, paste0(\"x[\", x_idx, \"]&lt;-\", vname), paste0(\"x[\", x_idx, \"]\"),\n                                            m, s, mcse, q[1], q[2], q[3], q[4], q[5])\n    }\n  }\n}\n\n# Добавление основных параметров\nadd_scalar(1,  \"M\")\nadd_scalar(2,  \"q1\")\nadd_scalar(3,  \"q2\")\nadd_scalar(4,  \"q3\")\nadd_scalar(5,  \"sigmaP1\")\nadd_scalar(6,  \"sigmaR\")\nadd_scalar(7,  \"sigmaP\")\nadd_scalar(8,  \"precbioindexP1\")\nadd_scalar(9,  \"precbioindexR\")\nadd_scalar(10, \"precbioindexP\")\nadd_scalar(11, \"Gr\")\nadd_scalar(12, \"Gp\")\nadd_scalar(13, \"Mp\")\n\n# Функция добавления временных рядов\nadd_series &lt;- function(base_idx, varname, years) {\n  for (i in seq_along(years)) {\n    rn &lt;- paste0(varname, \"[\", i, \"]\")  # Имя переменной с индексом\n    if (!rn %in% rownames(stats)) next  # Пропуск если нет данных\n    m &lt;- stats[rn, \"Mean\"]\n    s &lt;- stats[rn, \"SD\"]\n    mcse &lt;- mcse_from_ess(draws_mat[, rn])\n    q &lt;- quants[rn, c(\"2.5%\", \"25%\", \"50%\", \"75%\", \"97.5%\")]\n    xi &lt;- base_idx + (i - 1)  # Вычисление индекса в выходной таблице\n    rows[[length(rows) + 1]] &lt;&lt;- make_row(years[i], paste0(\"x[\", xi, \"]&lt;-\", rn), paste0(\"x[\", xi, \"]\"),\n                                          m, s, mcse, q[1], q[2], q[3], q[4], q[5])\n  }\n}\n\n# Добавление временных рядов\nadd_series(100, \"P1\", YEAR)\nadd_series(200, \"R\",  YEAR)\nadd_series(300, \"P\",  YEAR)\n\n# Создание итоговой таблицы\nout_df &lt;- do.call(rbind, rows)\n\n# Создание групп для сортировки\nout_df$group &lt;- ifelse(is.na(out_df$YEAR), \"param\",\n                ifelse(grepl(\"&lt;-P1\\\\[\", out_df$`#Vectors to monitor`), \"P1\",\n                ifelse(grepl(\"&lt;-R\\\\[\",  out_df$`#Vectors to monitor`), \"R\", \"P\")))\n\n# Сортировка параметров по индексу\nparam_rows &lt;- out_df[out_df$group == \"param\", ]\nparam_idx  &lt;- as.numeric(sub(\".*\\\\[(\\\\d+)\\\\].*\", \"\\\\1\", param_rows$node))\nparam_rows &lt;- param_rows[order(param_idx), ]\n\n# Сортировка временных рядов по году\np1_rows &lt;- out_df[out_df$group == \"P1\", ]\np1_rows &lt;- p1_rows[order(p1_rows$YEAR), ]\n\nr_rows  &lt;- out_df[out_df$group == \"R\", ]\nr_rows  &lt;- r_rows[order(r_rows$YEAR), ]\n\np_rows  &lt;- out_df[out_df$group == \"P\", ]\np_rows  &lt;- p_rows[order(p_rows$YEAR), ]\n\n# Компоновка финальной таблицы\nout_df &lt;- rbind(param_rows, p1_rows, r_rows, p_rows)\nout_df$group &lt;- NULL  # Удаление вспомогательной колонки\n\n# Сохранение результатов\nwrite.csv(out_df, \"monitor_summary.csv\", row.names = FALSE)\ncat(\"Saved: monitor_summary.csv\\n\")\n\nSaved: monitor_summary.csv\n\n# Вывод структуры результатов\nstr(out_df)\n\n'data.frame':   61 obs. of  11 variables:\n $ YEAR               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ #Vectors to monitor: chr  \"x[1]&lt;-M\" \"x[2]&lt;-q1\" \"x[3]&lt;-q2\" \"x[4]&lt;-q3\" ...\n $ node               : chr  \"x[1]\" \"x[2]\" \"x[3]\" \"x[4]\" ...\n $ mean               : num  0.173 0.417 0.74 0.933 0.316 ...\n $ sd                 : num  0.0634 0.0913 0.1339 0.0611 0.042 ...\n $ MC error           : num  0.001876 0.006145 0.008863 0.001879 0.000578 ...\n $ 2.50%              : num  0.0481 0.2635 0.4813 0.7821 0.2452 ...\n $ 25.00%             : num  0.132 0.353 0.643 0.905 0.287 ...\n $ median             : num  0.174 0.408 0.737 0.951 0.312 ...\n $ 75.00%             : num  0.216 0.475 0.842 0.979 0.342 ...\n $ 97.50%             : num  0.296 0.618 0.979 0.998 0.408 ...\n\n# ========================================================================================================================\n# Визуализация априорных и апостериорных параметров\n# Параметры: M, Gp, Gr, Mp, q1, q2, q3, precP1, precR, precP, precbioindexP1, precbioindexR, precbioindexP\n# И производные: sigmaP1, sigmaR, sigmaP\n# ========================================================================================================================\n\n# Сэмплируем приоры прямо из той же JAGS-модели (без данных)\nsample_priors_from_model &lt;- function(model_string, n_iter = 20000, n_adapt = 500) {\n  jm_prior &lt;- jags.model(textConnection(model_string), data = list(N = 0), n.chains = 1, n.adapt = n_adapt)\n  vars &lt;- c(\"M\",\"Gp\",\"Gr\",\"Mp\",\"q1\",\"q2\",\"q3\",\n            \"precP1\",\"precR\",\"precP\",\"precbioindexP1\",\"precbioindexR\",\"precbioindexP\")\n  priors &lt;- coda.samples(jm_prior, variable.names = vars, n.iter = n_iter)\n  as.matrix(priors)\n}\n\n# Получаем матрицы приоров и постериоров\nprior_mat &lt;- sample_priors_from_model(model_string, n_iter = 20000, n_adapt = 500)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 0\n   Unobserved stochastic nodes: 16\n   Total graph size: 33\n\nInitializing model\n\npost_mat  &lt;- as.matrix(samps)\n\n# Добавляем производные сигмы из прецизионов\nadd_sigmas &lt;- function(mat) {\n  add &lt;- function(dst, src) {\n    if (all(src %in% colnames(mat))) dst &lt;- cbind(dst, setNames(as.data.frame(sqrt(1/mat[, src, drop=FALSE])), gsub(\"^prec\",\"sigma\", src)))\n    dst\n  }\n  out &lt;- mat\n  out &lt;- add(out, c(\"precP1\"))\n  out &lt;- add(out, c(\"precR\"))\n  out &lt;- add(out, c(\"precP\"))\n  out\n}\nprior_mat &lt;- add_sigmas(prior_mat)\npost_mat  &lt;- add_sigmas(post_mat)\n\n# Список параметров для визуализации\nparams &lt;- intersect(\n  c(\"M\",\"Gp\",\"Gr\",\"Mp\",\"q1\",\"q2\",\"q3\",\n    \"sigmaP1\",\"sigmaR\",\"sigmaP\",\n    \"precbioindexP1\",\"precbioindexR\",\"precbioindexP\"),\n  union(colnames(prior_mat), colnames(post_mat))\n)\n\n# В long-формат\nmk_df &lt;- function(mat, label) {\n  if (is.null(mat) || nrow(mat) == 0) return(data.frame())\n  mat &lt;- mat[, intersect(colnames(mat), params), drop = FALSE]\n  reshape(\n    data.frame(iter = seq_len(nrow(mat)), mat, check.names = FALSE),\n    direction = \"long\", varying = params, v.names = \"value\", timevar = \"param\", times = params\n  )[, c(\"param\",\"value\")]\n}\nprior_df &lt;- mk_df(prior_mat, \"Prior\"); prior_df$dist &lt;- \"Prior\"\npost_df  &lt;- mk_df(post_mat,  \"Posterior\"); post_df$dist &lt;- \"Posterior\"\nplot_df  &lt;- rbind(prior_df, post_df)\n\n# Подписи\nparam_labels &lt;- c(\n  M=\"M (mortality)\", Gp=\"Gp\", Gr=\"Gr\", Mp=\"Mp\",\n  q1=\"q1\", q2=\"q2\", q3=\"q3\",\n  sigmaP1=\"sigmaP1\", sigmaR=\"sigmaR\", sigmaP=\"sigmaP\",\n  precbioindexP1=\"precbioindexP1\", precbioindexR=\"precbioindexR\", precbioindexP=\"precbioindexP\"\n)\nplot_df$param_f &lt;- factor(plot_df$param, levels = params, labels = unname(param_labels[params]))\n\n# График prior vs posterior (берёт priors из модели!)\nlibrary(ggplot2)\nggplot(plot_df, aes(x = value, color = dist, fill = dist)) +\n  geom_density(alpha = 0.25, linewidth = 0.7) +\n  facet_wrap(~ param_f, scales = \"free\", ncol = 4) +\n  scale_color_manual(values = c(\"Prior\" = \"#999999\", \"Posterior\" = \"#1b9e77\")) +\n  scale_fill_manual(values  = c(\"Prior\" = \"#bbbbbb\", \"Posterior\" = \"#1b9e77\")) +\n  labs(title = \"Априорные (из модели) vs апостериорные распределения\",\n       x = \"Значение\", y = \"Плотность\", color = \"\", fill = \"\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n# ========================================================================================================================\n# Бабл-плоты остатков P1, R, P по годам (2000–2015)\n# Требуется: объекты samps, data_list. Если YEAR не создан, создадим.\n# ========================================================================================================================\n\nif (!exists(\"YEAR\")) YEAR &lt;- 2000 + 0:(data_list$N - 1)\ndraws_mat &lt;- as.matrix(samps)\neps &lt;- 1.0e-6\n\nresid_bubble_summary &lt;- function(series, obs_vec, q_name, state_name_prefix) {\n  rows &lt;- list()\n  for (i in seq_along(obs_vec)) {\n    if (is.na(obs_vec[i])) next\n    q_draws     &lt;- draws_mat[, q_name]\n    state_draws &lt;- draws_mat[, paste0(state_name_prefix, \"[\", i, \"]\")]\n    # residual per draw: log(observed) - log(expected)\n    res_draws &lt;- log(obs_vec[i]) - log(eps + q_draws * state_draws)\n    r_mean &lt;- mean(res_draws, na.rm = TRUE)\n    rows[[length(rows) + 1]] &lt;- data.frame(\n      YEAR = YEAR[i],\n      series = series,\n      resid = r_mean,\n      abs_resid = abs(r_mean),\n      sign = ifelse(r_mean &gt;= 0, \"pos\", \"neg\")\n    )\n  }\n  do.call(rbind, rows)\n}\n\nb1 &lt;- resid_bubble_summary(\"P1\", data_list$bioindexP1, \"q1\", \"P1\")\nb2 &lt;- resid_bubble_summary(\"R\",  data_list$bioindexR,  \"q2\", \"R\")\nb3 &lt;- resid_bubble_summary(\"P\",  data_list$bioindexP,  \"q3\", \"P\")\nbubbles &lt;- rbind(b1, b2, b3)\n\n# Порядок рядов сверху вниз: P1, R, P\nbubbles$series &lt;- factor(bubbles$series, levels = c(\"P1\", \"R\", \"P\"))\n\n# Убираем пустое расстояние - используем минимальные интервалы\nlvl &lt;- c(\"P1\",\"R\",\"P\")\ny_map &lt;- setNames(c(1, 2, 3), lvl)  # Числовые позиции без больших промежутков\n\nbubbles$y_pos &lt;- unname(y_map[as.character(bubbles$series)])\n\n# Создаем вытянутый прямоугольный график\nggplot(bubbles, aes(x = YEAR, y = y_pos)) +\n  geom_point(aes(size = abs_resid, fill = sign), shape = 21, color = \"black\", alpha = 0.9) +\n  scale_fill_manual(values = c(neg = \"black\", pos = \"white\"),\n                    breaks = c(\"pos\",\"neg\"),\n                    labels = c(\"положительные\",\"отрицательные\"),\n                    name = \"\") +\n  scale_size_area(max_size = 12, name = \"Остатки\") +\n  scale_x_continuous(breaks = seq(2000, 2015, by = 2), limits = c(2000, 2015)) +\n  scale_y_continuous(breaks = unname(y_map), \n                     labels = names(y_map),\n                     limits = c(0.5, 3.5),  # Убираем пустое пространство сверху и снизу\n                     expand = c(0, 0)) +     # Убираем расширение осей\n  labs(title = \"Пузырьковая диаграмма остатков (лог-шкала): P1, R, P\", \n       x = \"Год\", \n       y = \"\") +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = \"top\",\n    panel.grid.major.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    aspect.ratio = 0.3,  # Делаем график вытянутым прямоугольником (ширина &gt; высоты)\n    plot.margin = margin(5, 10, 5, 5, \"pt\")  # Убираем лишние отступы вокруг графика\n  )\n\n\n\n\n\n\n\n# ========================================================================================================================\n# ДИНАМИКА ИНДЕКСОВ (ПРЕРЕКРУТЫ, РЕКРУТЫ, ПОСТРЕКРУТЫ) МОДЕЛЬНЫХ И ФАКТИЧЕСКИХ (ТОЧКИ)\n# ========================================================================================================================\n# Три графика динамики P1, R, P: медиана (линия), 95% ДИ (лента), точки — наблюдения,\n# приведённые к единому масштабу  делением на медиану q (Posterior median).\n# ========================================================================================================================\nif (!exists(\"YEAR\")) YEAR &lt;- 2000 + 0:(data_list$N - 1)\ndraws_mat &lt;- as.matrix(samps)\n\nseries_summary &lt;- function(varname, obs_vec, qname, series_label) {\n  med_q &lt;- median(draws_mat[, qname], na.rm = TRUE)\n  rows &lt;- vector(\"list\", length(obs_vec))\n  for (i in seq_along(obs_vec)) {\n    rn &lt;- paste0(varname, \"[\", i, \"]\")\n    if (!rn %in% colnames(draws_mat)) next\n    v &lt;- draws_mat[, rn]\n    qs &lt;- quantile(v, c(0.025, 0.5, 0.975), na.rm = TRUE)\n    obs_state &lt;- if (!is.na(obs_vec[i])) obs_vec[i] / med_q else NA_real_\n    rows[[i]] &lt;- data.frame(\n      YEAR = YEAR[i],\n      series = series_label,\n      median = qs[2],\n      lo = qs[1],\n      hi = qs[3],\n      obs = obs_state\n    )\n  }\n  do.call(rbind, rows)\n}\n\ndf_p1 &lt;- series_summary(\"P1\", data_list$bioindexP1, \"q1\", \"P1\")\ndf_r  &lt;- series_summary(\"R\",  data_list$bioindexR,  \"q2\", \"R\")\ndf_p  &lt;- series_summary(\"P\",  data_list$bioindexP,  \"q3\", \"P\")\n\n\np_P1 &lt;- ggplot(df_p1, aes(x = YEAR)) +\n  geom_ribbon(aes(ymin = lo, ymax = hi), fill = \"#1f77b4\", alpha = 0.2) +\n  geom_line(aes(y = median), color = \"#1f77b4\", linewidth = 1) +\n  geom_point(aes(y = obs), shape = 21, size = 2, color = \"black\", fill = \"white\", na.rm = TRUE) +\n  scale_x_continuous(breaks = seq(2000, 2015, by = 2), limits = c(2000, 2015)) +\n  labs(title = \"Моделируемая и фактическая (точки) динамика пререкрутов\", x = \"Годы\", y = \"Пререкруты (экз.)\") +\n  theme_minimal(base_size = 12)\n\nprint(p_P1)\n\n\n\n\n\n\n\np_R &lt;- ggplot(df_r, aes(x = YEAR)) +\n  geom_ribbon(aes(ymin = lo, ymax = hi), fill = \"#2ca02c\", alpha = 0.2) +\n  geom_line(aes(y = median), color = \"#2ca02c\", linewidth = 1) +\n  geom_point(aes(y = obs), shape = 21, size = 2, color = \"black\", fill = \"white\", na.rm = TRUE) +\n  scale_x_continuous(breaks = seq(2000, 2015, by = 2), limits = c(2000, 2015)) +\n  labs(title = \"Моделируемая и фактическая (точки) динамика рекрутов\", x = \"Годы\", y = \"Рекруты (экз.)\") +\n  theme_minimal(base_size = 12)\n\nprint(p_R)\n\n\n\n\n\n\n\np_P &lt;- ggplot(df_p, aes(x = YEAR)) +\n  geom_ribbon(aes(ymin = lo, ymax = hi), fill = \"#ff7f0e\", alpha = 0.2) +\n  geom_line(aes(y = median), color = \"#ff7f0e\", linewidth = 1) +\n  geom_point(aes(y = obs), shape = 21, size = 2, color = \"black\", fill = \"white\", na.rm = TRUE) +\n  scale_x_continuous(breaks = seq(2000, 2015, by = 2), limits = c(2000, 2015)) +\n  labs(title = \"Моделируемая и фактическая (точки) динамика пострекрутов\", x = \"Годы\", y = \"Пострекруты (экз.)\") +\n  theme_minimal(base_size = 12)\n\nprint(p_P)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Модель Catch-Survey Analysis (CSA)</span>"
    ]
  },
  {
    "objectID": "chapter 10.html",
    "href": "chapter 10.html",
    "title": "11  Съёмка: оптимизация маршрута",
    "section": "",
    "text": "11.1 Введение\nЕсли смотреть на планирование съёмки из высоты «Космоса» Карла Сагана и «Краткой истории времени» Стивена Хокинга, всё сводится к простой геометрии и неумолимой физике: расстояния складываются, время утекает, а топология района диктует, сколько топлива и суток мы действительно можем себе позволить. Но как только мы возвращаемся к карте, «быстрый» ум спешит дорисовать удобную историю: «расширим полигон — маршрут вырастет вдвое». На практике рост длины маршрута ведёт себя нелинейно, а форма «важнее площади» чаще, чем нам кажется. Это занятие — про то, как дисциплинировать интуицию, превратить картинку в числа и принять решения, которые выдерживают встречу с реальностью.\nМы используем минималистичный, но честный инструментариум: переводим исходный полигон в метрическую проекцию UTM, генерируем равномерную сетку станций и решаем задачу коммивояжёра для маршрута судна. Дальше — серия контролируемых экспериментов: асимметрично «растягиваем» полигон к северу (имитируя расширение арктического фронта работ) в 1.5, 2 и 3 раза, каждый раз оставляя одно и то же число станций. Сравниваем площади и оптимальные маршруты. Наша цель проста: понять, насколько дороже становится логистика от изменения площади и формы, когда часы и топливо конечны.\nМы заранее признаём «чёрных лебедей»: погодные окна, запреты и ледовую обстановку скрипт не видит. Но и это хорошая новость — систематический, воспроизводимый подход всё равно делает нас лучше: регулярная оценка маршрутов при разных сценариях снижает долю импровизации и помогает не переплачивать за «красивые» идеи. Здесь важнее не одна точная цифра, а устойчивая картина: где логистика растёт медленнее, чем площадь, а где внезапно «взлетает» из‑за длинной узкой шейки полигона или неудобно расположенных станций.\nЭта логистическая «эволюция маршрутов» чем‑то напоминает «Эгоистичный ген» Ричарда Докинза: выживают те траектории, которые экономят ресурс — и не важно, как они появились, важно, что они меньше «едят» времени и топлива. А из «SPQR» Мэри Бирд можно позаимствовать трезвость римских инженеров: дороги строили не по вдохновению, а по задачам снабжения; в море у нас те же дороги, только из линий на карте. Управление — это всегда история, которую мы рассказываем про будущее рейса; хорошая история — та, которая прошла проверку расчётом. И, да, «воля и самоконтроль» здесь не про героизм, а про дисциплину: держать число станций, окна и границы сценариев в рамках, чтобы сравнение было честным.\nЧуть техники, но по делу. Мы:\nЧто мы хотим увидеть. Во‑первых, нелинейность: умеренное расширение площади иногда почти «бесплатно», а узкая северная протяжка может внезапно удорожить рейс. Во‑вторых, роль формы: два полигона одинаковой площади могут иметь разные «ценники» из‑за геометрии. В‑третьих, смысл «предельной» станции: где последняя добавленная точка дороже всех предыдущих вместе. Это те наблюдения, которые превращают космическую картинку Нила Деграсса Тайсона о «геометрии вселенной» в бытовую навигацию: за изгибами карты стоят очень приземлённые деньги и часы.\nЕсть и границы. Наш TSP‑эвристик быстрый, но не гарантирует глобальный оптимум; станции расставлены равномерно и не знают про глубины; старт/финиш жёстко заданы; мы не моделируем штормовые задержки и ледовые коридоры. Нам нужны «хорошие объяснения», а не идеально подогнанные: скрипт даёт прозрачную, проверяемую основу для решений и для постановки более сложных задач — со слоями глубин, реальными портами, окнами и альтернативными алгоритмами маршрутизации. А следующий шаг: добавить предиктивные слои (погода/лед) и дать планировщику инструмент «думать наперёд».\nИ последнее — о масштабе. История учит смотреть на мир одновременно широко и конкретно. Наши полигоны — маленькие кусочки океана, но в них решается вечная задача: как превратить энергию и время в полезные данные с минимальными потерями. Этот скрипт — простой, «большой» по духу метод: он берёт карту, делает из неё информацию и возвращает управленческое решение. Именно такие мосты между картинкой и числом уменьшают долю случайности и, шаг за шагом, ведут к более разумной экспедиции.\nИ так, этот R-скрипт выполняет геопространственный анализ и моделирование станций исследований в ходе научно-исследовательских съемок.\nОсновная цель\nСкрипт моделирует оптимальные маршруты для станций в разных вариантах исследовательского полигона, чтобы определить, как изменение площади и формы полигона влияет на длину необходимого маршрута. Полный скрипт можно скачать по ссылке.\nДля работы скрипта:\nПошаговая работа скрипта\n1. Подготовка данных\n2. Моделирование расширенных полигонов\nСоздан алгоритм, который асимметрично расширяет полигон на север (имитируя расширение зоны исследования в арктическом направлении):\n3. Генерация траловых станций\n4. Оптимизация маршрутов\nКлючевая часть скрипта — решение задачи коммивояжера (TSP) для оптимизации маршрута судна:\n5. Анализ результатов\nСкрипт рассчитывает и сравнивает:\nИнтерпретация результатов\nИз сводной таблицы видно, как увеличение площади полигона влияет на длину маршрута:\nВажные наблюдения:\nПрактическое применение\nЭтот анализ полезен для:\nСкрипт демонстрирует, как геопространственный анализ и алгоритмы оптимизации могут помочь в принятии решений при планировании полевых исследований водных биоресурсов, особенно в условиях арктических морей, где логистика особенно сложна и дорогостояща.\n# ========================================================================================================================\n# ПРАКТИЧЕСКОЕ ЗАНЯТИЕ: ПЛАНИРОВАНИЕ МАРШРУТА СЪЕМКИ ПРИ ОГРАНИЧЕННОМ ВРЕМЕННОМ РЕСУРСЕ\n# Курс: \"Современные методы анализа данных в оценке водных биоресурсов\"\n# Автор: Баканев С.В.\n# Дата: 24.04.2025\n# \n# Цель: Освоить методы геопространственного анализа и оптимизации для планирования научно-исследовательских съемок\n# \n# Структура:\n# 1. Загрузка пакетов и настройка среды\n# 2. Загрузка и подготовка исходных данных\n# 3. Создание расширенных полигонов\n# 4. Генерация схемы станций\n# 5. Построение оптимальных маршрутов\n# 6. Визуализация результатов\n# 7. Сравнительный анализ и экспорт результатов\n# \n# Описание: Скрипт демонстрирует подход к планированию морских исследований с использованием\n#           методов пространственного анализа и решения задачи коммивояжера (TSP).\n#           Моделируются различные сценарии расширения района работ с оценкой\n#           их влияния на протяженность маршрута.\n# ========================================================================================================================\n\n# ЗАГРУЗКА НЕОБХОДИМЫХ ПАКЕТОВ -----------------------------------------------\n# Все пакеты должны быть предварительно установлены (install.packages(...))\n\nlibrary(sf)            # Базовые операции с пространственными данными (вектор)\n\nLinking to GEOS 3.13.1, GDAL 3.10.2, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(tidyverse)     # Метасборка пакетов для обработки и визуализации данных\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv ggplot2   3.5.2     v tibble    3.2.1\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.0.4     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rnaturalearth) # Загрузка готовых полигональных карт мира (для фона)\nlibrary(ggplot2)       # Создание продвинутых графиков (входит в tidyverse, но подключаем для ясности)\nlibrary(geosphere)     # Геодезические расчеты на сфере (расчет расстояний между точками)\nlibrary(TSP)           # Решение \"Задачи коммивояжера\" (Traveling Salesman Problem)\n\n\n# 1. НАСТРОЙКА СРЕДЫ ----------------------------------------------------------\n# Установка рабочей директории (замените на путь к своей папке)\nsetwd(\"C:/SURVEY/\")\n\n# 2. ЗАГРУЗКА ИСХОДНЫХ ДАННЫХ -------------------------------------------------\n# Загрузка предварительно созданного файла с полигонами\nload(\"polygons.RData\")\n\n# Фильтрация полигона за 2020 год и извлечение его геометрии\npolygon_2020 &lt;- polygons %&gt;%\n  filter(YEAR == 2020) %&gt;%\n  st_geometry()\n\n# 3. ПРЕОБРАЗОВАНИЕ СИСТЕМЫ КООРДИНАТ -----------------------------------------\n# Преобразование из географических координат (WGS84) в проекцию UTM (зона 40N).\n# Это необходимо для корректного вычисления площадей и расстояний в метрах.\npolygon_2020_utm &lt;- st_transform(polygon_2020, 32640)\n\n# 4. РАСЧЕТ ПЛОЩАДИ ИСХОДНОГО ПОЛИГОНА ----------------------------------------\n# Вычисление площади в кв. метрах (st_area) и конвертация в кв. километры (/ 1e6)\narea_km2_original &lt;- (st_area(polygon_2020_utm) / 1e6) %&gt;% as.numeric()\n# Вывод результата в консоль\ncat(\"Площадь полигона 2020 года: \", round(area_km2_original, 2), \" км²\\n\")\n\nПлощадь полигона 2020 года:  63101.31  км&lt;U+00B2&gt;\n\n# 5. ФУНКЦИЯ ДЛЯ ЭКСПАНСИИ ПОЛИГОНА НА СЕВЕР -----------------------------------\n# Создание функции, которая \"растягивает\" северную часть полигона на заданный множитель.\n# Аргументы:\n#   poly - исходный полигон (в UTM)\n#   factor - коэффициент расширения (например, 1.5 - увеличить на 50%)\nexpand_polygon_north &lt;- function(poly, factor) {\n  # Извлечение координат вершин полигона и преобразование в DataFrame\n  coords &lt;- st_coordinates(poly)[, 1:2] %&gt;%\n    as.data.frame() %&gt;%\n    rename(x = X, y = Y)\n\n  # ЛОГИКА РАСШИРЕНИЯ:\n  # 1. Определяем \"северную\" часть полигона (верхние 30% точек по оси Y)\n  north_threshold &lt;- quantile(coords$y, 0.7)\n  # 2. Находим общий разброс полигона по оси Y (высоту)\n  y_range &lt;- diff(range(coords$y))\n  # 3. Сдвигаем все северные точки на север на величину (factor - 1) * высоту_полигона\n  coords[coords$y &gt;= north_threshold, \"y\"] &lt;-\n    coords[coords$y &gt;= north_threshold, \"y\"] + y_range * (factor - 1)\n\n  # Создание нового полигона из модифицированных точек:\n  # 1. Преобразование точек в spatial object\n  # 2. Объединение точек в один объект\n  # 3. Построение выпуклой оболочки для получения гладкого полигона\n  expanded_poly &lt;- st_as_sf(coords, coords = c(\"x\", \"y\"), crs = st_crs(poly)) %&gt;%\n    st_combine() %&gt;%\n    st_convex_hull()\n\n  return(expanded_poly)\n}\n\n# 6. СОЗДАНИЕ НАБОРА ЭКСПАНДИРОВАННЫХ ПОЛИГОНОВ -------------------------------\n# Применение функции к коэффициентам 1.5, 2 и 3\nfactors &lt;- c(1.5, 2, 3)\nexpanded_polygons &lt;- map(factors, ~ expand_polygon_north(polygon_2020_utm, .x))\n\n# 7. РАСЧЕТ ПЛОЩАДЕЙ НОВЫХ ПОЛИГОНОВ ------------------------------------------\nareas_expanded &lt;- map_dbl(expanded_polygons, ~ {\n  (st_area(.x) / 1e6) %&gt;% as.numeric() # Площадь в кв. км.\n})\n\n# 8. ФОРМИРОВАНИЕ ЕДИНОЙ ТАБЛИЦЫ С ВСЕМИ ПОЛИГОНАМИ ---------------------------\n# Создание именованного списка всех полигонов\nall_polygons &lt;- list(\n  Original = polygon_2020_utm,\n  Expanded_x1_5 = expanded_polygons[[1]],\n  Expanded_x2 = expanded_polygons[[2]],\n  Expanded_x3 = expanded_polygons[[3]]\n)\n\n# Вектор меток и площадей\nlabels &lt;- c(\"Original\", \"Expanded_x1_5\", \"Expanded_x2\", \"Expanded_x3\")\nareas &lt;- c(area_km2_original, areas_expanded)\n\n# Комбинирование геометрий и создание итогового SF-объекта\ngeometry &lt;- do.call(c, all_polygons) # Объединение геометрий в один вектор\n\npolygon_df &lt;- tibble(\n  label = factor(labels, levels = labels), # Метка как фактор для сохранения порядка\n  area_km2 = areas,                        # Площадь\n  geometry = st_sfc(geometry)              # Геометрия\n) %&gt;%\n  st_as_sf() # Преобразование в пространственный объект\n\n# 9. ГЕНЕРАЦИЯ СЕТКИ ТОЧЕК (СТАНЦИЙ) ВНУТРИ КАЖДОГО ПОЛИГОНА -----------------\n# Генерация 137 точек по регулярной сетке внутри каждого полигона.\n# Используется rowwise для применения функции к каждой строке-полигону.\nsample_points &lt;- polygon_df %&gt;%\n  rowwise() %&gt;%\n  mutate(points = list(st_sample(geometry, size = 137, type = \"regular\"))) %&gt;%\n  ungroup()\n\n# 10. СОЗДАНИЕ ЕДИНОЙ ТАБЛИЦЫ ВСЕХ ТОЧЕК --------------------------------------\n# Преобразование вложенного списка точек в плоскую таблицу\npoints_list &lt;- map2(sample_points$geometry, sample_points$label, ~ {\n  st_sample(.x, size = 137, type = \"regular\") %&gt;% # Извлечение точек\n    st_as_sf() %&gt;%                                # Конвертация в SF\n    mutate(label = .y)                            # Добавление метки полигона\n})\n\n# Объединение всех точек в один DataFrame\npoints_df &lt;- do.call(rbind, points_list) %&gt;%\n  rename(geometry = x) %&gt;%\n  st_set_geometry(\"geometry\") %&gt;%\n  st_set_crs(st_crs(polygon_2020_utm)) # Важно: явно задаем систему координат\n\n# 11. ПРЕОБРАЗОВАНИЕ В WGS84 ДЛЯ ВИЗУАЛИЗАЦИИ И РАСЧЕТОВ ----------------------\n# Большинство картографических пакетов и функций расчета расстояний работают с WGS84\npolygon_wgs84 &lt;- st_transform(polygon_df, 4326) # WGS84 (широта/долгота)\npoints_wgs84 &lt;- st_transform(points_df, 4326)\n\n# 12. ЗАГРУЗКА ФОНОВОЙ КАРТЫ (ГРАНИЦЫ РОССИИ) ---------------------------------\nrussia &lt;- ne_countries(scale = \"medium\", country = \"Russia\", returnclass = \"sf\") %&gt;%\n  st_transform(4326) # Преобразование в WGS84\n\n# 13. ВИЗУАЛИЗАЦИЯ: ПОЛИГОНЫ И ТОЧКИ ------------------------------------------\n# Построение карты с фацетами (subplots) для каждого варианта полигона\nggplot() +\n  geom_sf(data = russia, fill = \"lightgray\", color = \"black\", linewidth = 0.3) + # Фон\n  geom_sf(data = polygon_wgs84, aes(fill = label), alpha = 0.6, color = \"darkred\", linewidth = 0.5) + # Полигоны\n  geom_sf(data = points_wgs84, aes(color = label), shape = 16, size = 1) + # Точки\n  facet_wrap(~ label, scales = \"fixed\") + # Фацеты по варианту полигона\n  coord_sf(xlim = c(35, 50), ylim = c(68, 75)) + # Обрезка карты до нужного региона\n  labs(\n    title = \"Полигоны с разной площадью и траловыми станциями\",\n    subtitle = \"Регулярное распределение 137 точек внутри каждого полигона\",\n    fill = \"Вариант\", color = \"Вариант\"\n  ) +\n  theme_minimal() +\n  theme(strip.background = element_rect(fill = \"lightblue\"))\n\n\n\n\n\n\n\n# 14. ФУНКЦИЯ ДЛЯ ПОСТРОЕНИЯ ОПТИМАЛЬНОГО МАРШРУТА (TSP) ----------------------\n# Создает маршрут, проходящий через все точки, с фиксацией начала и конца.\ncreate_route &lt;- function(points) {\n  # Извлечение координат (долгота, широта) из SF-объекта\n  coords &lt;- st_coordinates(points)\n\n  # СТРАТЕГИЯ: Начало и конец маршрута - две самые западные точки.\n  # Это имитирует выход судна из порта и возврат в него.\n  west_points &lt;- order(coords[,1])[1:2] # Индексы двух точек с min долготой\n\n  # Расчет матрицы расстояний между всеми точками (в метрах)\n  dist_matrix &lt;- distm(coords, fun = distHaversine)\n\n  # СОЗДАНИЕ И НАСТРОЙКА ЗАДАЧИ KОММИВОЯЖЕРА (TSP):\n  tsp &lt;- TSP(dist_matrix / 1000) # Создание объекта TSP (расстояния в км)\n  atsp &lt;- as.ATSP(tsp)           # Преобразование в Asymmetric TSP\n\n  # ФИКСАЦИЯ НАЧАЛА И КОНЦА:\n  # Обнуляем расстояния ДО стартовой точки -&gt; она станет первой.\n  atsp[, west_points[1]] &lt;- 0\n  # Обнуляем расстояния ОТ конечной точки -&gt; она станет последней.\n  atsp[west_points[2], ] &lt;- 0\n\n  # РЕШЕНИЕ TSP: метод \"Ближайшая вставка\" (быстрый, но не всегда оптимальный)\n  tour &lt;- solve_TSP(atsp, method = \"nearest_insertion\")\n\n  # ФОРМИРОВАНИЕ ПОСЛЕДОВАТЕЛЬНОСТИ ТОЧЕК МАРШРУТА:\n  # Начало -&gt; Маршрут -&gt; Конец. unique() убирает возможные дубликаты.\n  ordered_indices &lt;- c(west_points[1], as.integer(tour), west_points[2])\n  ordered_indices &lt;- unique(ordered_indices)\n\n  return(ordered_indices)\n}\n\n# 15. ПОСТРОЕНИЕ МАРШРУТОВ ДЛЯ КАЖДОГО ПОЛИГОНА -------------------------------\n# Применение функции create_route к каждой группе точек\nroutes &lt;- points_wgs84 %&gt;%\n  group_by(label) %&gt;%        # Группировка по варианту полигона\n  group_modify(~ {\n    ids &lt;- create_route(.x)  # Получение упорядоченного списка индексов точек\n    # Соединение точек в линию (маршрут)\n    route_line &lt;- st_combine(.x$geometry[ids]) %&gt;%\n      st_cast(\"LINESTRING\")  # Явное указание типа геометрии\n\n    tibble(geometry = st_sfc(route_line, crs = 4326)) # Возврат маршрута\n  }) %&gt;%\n  st_as_sf() # Преобразование результата в SF-объект\n\n# 16. ВИЗУАЛИЗАЦИЯ: ПОЛИГОНЫ, ТОЧКИ И МАРШРУТЫ --------------------------------\nggplot() +\n  geom_sf(data = russia, fill = \"lightgray\", color = \"black\", linewidth = 0.3) +\n  geom_sf(data = polygon_wgs84, aes(fill = label), alpha = 0.6, linewidth = 0.5) +\n  geom_sf(data = points_wgs84, aes(color = label), shape = 16, size = 1) +\n  geom_sf(data = routes, color = \"darkblue\", linewidth = 0.8) + # Маршруты\n  facet_wrap(~ label, scales = \"fixed\") +\n  coord_sf(xlim = c(35, 50), ylim = c(68, 74.5)) +\n  labs(\n    title = \"Полигоны с оптимальными маршрутами\",\n    subtitle = \"Начало и конец маршрута - две самые западные точки\",\n    fill = \"Вариант\", color = \"Вариант\"\n  ) +\n  theme_minimal() +\n  theme(strip.background = element_rect(fill = \"lightblue\"))\n\n\n\n\n\n\n\n# 17. СОХРАНЕНИЕ КАРТЫ В ФАЙЛ -------------------------------------------------\nggsave(\"polygon_with_optimal_routes.png\", width = 12, height = 10, dpi = 300)\n\n# 18. СОХРАНЕНИЕ ДАННЫХ МАРШРУТОВ (Ошибка: файл уже существует) ---------------\n# st_write(routes, \"optimal_routes.gpkg\") # Раскомментируйте и используйте append=FALSE для перезаписи\n# st_write(routes, \"optimal_routes.gpkg\", append=FALSE)\n\n# 19. ФИНАЛЬНЫЙ СРАВНИТЕЛЬНЫЙ АНАЛИЗ ------------------------------------------\n# Создание сводной таблицы с ключевыми метриками для каждого сценария\nsurvey_summary &lt;- routes %&gt;%\n  # Добавление данных о площади из таблицы полигонов\n  left_join(st_drop_geometry(polygon_wgs84), by = \"label\") %&gt;%\n  mutate(\n    `Количество тралений` = 137, # Константа по условию задачи\n    # Расчет длины маршрута: st_length(geometry) возвращает длину в метрах, делим на 1000 для перевода в км.\n    `Длина маршрута (км)` = round(as.numeric(st_length(geometry)) / 1000, 1),\n    `Площадь полигона (км2)` = round(area_km2, 1)\n  ) %&gt;%\n  select( # Выбор и переименование колонок для итоговой таблицы\n    Вариант = label,\n    `Количество тралений`,\n    `Длина маршрута (км)`,\n    `Площадь полигона (км2)`\n  )\n\n# 20. ВЫВОД ИТОГОВОЙ ТАБЛИЦЫ В КОНСОЛЬ ----------------------------------------\ncat(\"\\nСводная статистика по вариантам:\\n\")\n\n\nСводная статистика по вариантам:\n\nprint(survey_summary)\n\nSimple feature collection with 4 features and 4 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 36.79041 ymin: 68.47354 xmax: 47.76963 ymax: 74.49279\nGeodetic CRS:  WGS 84\n# A tibble: 4 x 5\n# Groups:   Вариант [4]\n  Вариант     `Количество тралений` `Длина маршрута (км)` Площадь полигона (км~1\n  &lt;fct&gt;                       &lt;dbl&gt;                 &lt;dbl&gt;                  &lt;dbl&gt;\n1 Original                      137                 3104.                 63101.\n2 Expanded_x~                   137                 4082.                 99791.\n3 Expanded_x2                   137                 5032.                135352.\n4 Expanded_x3                   137                 6000.                207066.\n# i abbreviated name: 1: `Площадь полигона (км2)`\n# i 1 more variable: geometry &lt;LINESTRING [arc_degree]&gt;",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Съёмка: оптимизация маршрута</span>"
    ]
  },
  {
    "objectID": "chapter 10.html#введение",
    "href": "chapter 10.html#введение",
    "title": "11  Съёмка: оптимизация маршрута",
    "section": "",
    "text": "загружаем полигон 2020 года и считаем его площадь в километрах,\nстроим три северные экспансии (×1.5, ×2, ×3), не симметрично раздувая фигуру, а добавляя «арктический хвост»,\nгенерируем внутри каждого полигона по 137 регулярных станций — одинаковая «нагрузка» для справедливости сравнения,\nрешаем TSP с геодезическими расстояниями (Haversine) и фиксируем начало/конец маршрута в самых западных точках — реалистичная постановка для судна, идущего «с запада»,\nвизуализируем и сводим метрики: площадь и длина маршрута по каждому сценарию.\n\n\n\n\n\n\n\n\n\nСкачайте файл данных (polygons.RData)\nУстановите рабочую директорию в setwd()\nУстановите необходимые пакеты.`\n\n\n\n\nЗагружается полигон за 2020 год из файла polygons.RData\nПолигон преобразуется в систему координат UTM зоны 40N (EPSG:32640) для точных метрических вычислений\nРассчитывается площадь полигона: 63,101.31 км²\n\n\n\n\nexpand_polygon_north() определяет северную часть (верхние 30% точек)\nРасширяет только эту часть на заданный коэффициент\nСоздаются 3 варианта: увеличенные на 1.5x, 2x и 3x\n\n\n\nДля каждого полигона генерируются 137 равномерно распределенных точек (метод “regular”)\nЭто имитирует расположение траловых станций в исследовательском полигоне\nКоличество станций одинаково для всех вариантов полигона\n\n\n\n\nНачало и конец маршрута фиксируются как две самые западные точки (логично для судна, приходящего с запада)\nИспользуется алгоритм ближайшего включения (nearest_insertion)\nМатрица расстояний рассчитывается с помощью distHaversine (точное геодезическое расстояние)\n\n\n\n\nПлощадь каждого полигона\nДлину оптимального маршрута для 137 станций\nВизуализирует все варианты на карте\n\n\n\n\n\n\n\n\n\n\n\n\nВариант\nКоличество тралений\nДлина маршрута (км)\nПлощадь полигона (км²)\n\n\n\n\nOriginal\n137\n3,745\n63,101.3\n\n\nExpanded_x1.5\n137\n4,076\n99,791.0\n\n\nExpanded_x2\n137\n4,701\n135,352.0\n\n\nExpanded_x3\n137\n6,038\n207,066.0\n\n\n\n\n\nПри увеличении площади на 57% (до 99,791 км²) длина маршрута возрастает только на 9% (до 4,076 км)\nПри троекратном увеличении площади (до 207,066 км²) длина маршрута возрастает на 61% (до 6,038 км)\nЭто показывает нелинейную зависимость между площадью полигона и длиной маршрута\n\n\n\n\nПланирования гидробиологических исследований в условиях ограниченного бюджета\nОценки дополнительных затрат при расширении зоны исследования\nОптимизации маршрутов научно-исследовательских судов\nМоделирования последствий изменения границ охраняемых территорий",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Съёмка: оптимизация маршрута</span>"
    ]
  },
  {
    "objectID": "chapter 11.html",
    "href": "chapter 11.html",
    "title": "12  Картография: повышение разрешения",
    "section": "",
    "text": "Если смотреть на карты океана так же широко, как Карл Саган смотрел на космос, быстро понимаешь: «зерно» наблюдений определяет, что мы вообще способны увидеть. Масштаб меняет саму формулировку вопросов, — и редкая сетка EN4 в Баренцевом море порой даёт не «карту», а грубую мозаику. Ресэмплинг — это наш способ подружить масштаб и видимость: мы увеличиваем пространственное разрешение растра и интерполируем значения, чтобы получить непрерывную поверхность, пригодную для изолиний и анализа.\nВажно не перепутать ясность с иллюзией знания. Ум любит гладкие картинки и готов принять их за истину; по мир полон неожиданностей, и интерполяция не создаёт новых данных, а лишь аккуратно «зашивает» пустоты. Всегда держим критерий хорошего объяснения: метод должен быть воспроизводимым, проверяемым и не обещать лишнего. В этом скрипте выбран билинейный ресэмплинг как честный компромисс между сглаживанием и сохранением крупных структур: визуализация становится читабельной, а исходная информативность не «приписывается задним числом».\nПодход эволюционен и прагматичен — выживает не «самая умная» интерполяция, а та, что лучше служит цели карты и не вносит артефактов. Мы задаём область (Баренцево море), собираем растровую поверхность из точек в WGS84, формируем целевую сетку с более тонким шагом, интерполируем, категоризируем значения по осмысленным бинам и строим итоговую карту: растровая подложка, изолинии, координатная сетка, береговая линия. Геометрия и проекции названы своими именами, а не «растворены» в красивой картинке.\nМетодологический оптимизм уместен, но дисциплинирован: систематическая, воспроизводимая обработка действительно делает нас лучше, если помнить границы. Для строгих расчётов держим нативное разрешение; для картографирования — используем сглаженную сетку. Если исходная сетка слишком редка, тестируем альтернативы (бикубика, IDW, сплайны, кригинг) и выбираем то, что минимизирует артефакты. Это ровно тот случай, где «будущее разума» — не метафора: добавление предиктивных слоёв (погода, лед) и смена интерполяторов — путь к более умным картам.\nХорошая история опирается на честные визуализации. Этот скрипт даёт именно такую «витрину»: карты до/после ресэмплинга и TIFF высокого разрешения для публикаций. Он не заменяет данные — он помогает их видеть. А видеть лучше — значит принимать решения, которые выдерживают столкновение с реальностью.\nПолный скрипт можно скачать по ссылке.\nДля работы скрипта:\n\nСкачайте файл данных (diffTemp.xlsx)\nУстановите рабочую директорию в setwd()\nУстановите необходимые пакеты.`\n\nСкрипт начинается с очистки рабочей среды и загрузки необходимых пакетов. Задаются параметры области исследования: минимальная и максимальная долгота и широта, ограничивающие регион Баренцева моря. Загружаются данные из Excel-файла, содержащего разности температур, и преобразуются в data.frame с колонками долготы, широты и значения температуры. Создается исходный растровый объект из этих данных с указанием системы координат. Определяется целевой растр с более высоким разрешением (0.01 градуса), после чего выполняется ресэмплинг исходного растра с использованием билинейной интерполяции для сглаживания значений. Данные ресэмплинга преобразуются в data.frame с фильтрацией по заданной области и категоризацией температурных значений для построения цветовых градаций. Загружаются географические данные мировых береговых линий и создается координатная сетка. Определяется цветовая палитра для отображения температурных аномалий. Строится финальный график с использованием сглаженных данных: растровая поверхность, контуры температурных изолиний, координатная сетка и подложка мировых границ. График сохраняется в формате TIFF с высоким разрешением.\n\n# ОЧИСТКА РАБОЧЕЙ СРЕДЫ И ЗАГРУЗКА БИБЛИОТЕК --------------------------------\nrm(list = ls())  # Очистка среды от предыдущих объектов\n\n# Загрузка необходимых библиотек:\nlibrary(dplyr)      # Для манипуляций с данными\n\n\nПрисоединяю пакет: 'dplyr'\n\n\nСледующие объекты скрыты от 'package:stats':\n\n    filter, lag\n\n\nСледующие объекты скрыты от 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(sf)         # Для работы с пространственными данными\n\nLinking to GEOS 3.13.1, GDAL 3.10.2, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(ggplot2)    # Для построения графиков\nlibrary(rnaturalearth)  # Для получения географических данных\nlibrary(terra)      # Для работы с растровыми данными\n\nterra 1.8.42\n\nlibrary(metR)       # Дополнительные функции для визуализации\n\n# УСТАНОВКА РАБОЧЕЙ ДИРЕКТОРИИ -----------------------------------------------\nsetwd(\"C:/SUPERPIC/\")  # Замените на актуальный путь к вашим данным\n\n# ПАРАМЕТРЫ ОБЛАСТИ ИССЛЕДОВАНИЯ ---------------------------------------------\nxmin &lt;- 10      # Минимальная долгота (границы Баренцева моря)\nxmax &lt;- 65      # Максимальная долгота\nymin &lt;- 68      # Минимальная широта\nymax &lt;- 82      # Максимальная широта\n\n# ПАРАМЕТРЫ РЕСЭМПЛИНГА ------------------------------------------------------\nnew_res &lt;- 0.1  # Новое разрешение в градусах после передискретизации\n\n# 1. ЗАГРУЗКА ИСХОДНЫХ ДАННЫХ -------------------------------------------------\n# Чтение данных из Excel-файла\nNEMO &lt;- readxl::read_excel(\"diffTemp.xlsx\", sheet = \"diffTemp\")\n\n# Создание датафрейма с координатами и температурными данными\ndf &lt;- data.frame(\n  longitude = NEMO$Lon,\n  latitude = NEMO$Lat,\n  TEMP = NEMO$dif\n)\n\n# 2. ПОДГОТОВКА ИСХОДНЫХ ДАННЫХ ДЛЯ ВИЗУАЛИЗАЦИИ ------------------------------\n# Фильтрация данных по области исследования и категоризация температур\nDF &lt;- as.data.frame(df, xy = TRUE, na.rm = TRUE) %&gt;% \n  filter(\n    longitude &gt;= xmin, longitude &lt;= xmax,\n    latitude &gt;= ymin, latitude &lt;= ymax\n  ) %&gt;%\n  mutate(\n    TEMP_cat = cut(\n      TEMP,\n      breaks = c(-Inf, 0, 0.25, 0.5, 0.75, 1, Inf),\n      labels = c(\"&lt;0\", \"0~0.25\", \"0.25~0.5\", \"0.5~0.75\", \"0.75~1\", \"&gt;1\"),\n      include.lowest = TRUE\n    )\n  )\n\n# 3. ЗАГРУЗКА ГЕОГРАФИЧЕСКИХ ДАННЫХ -------------------------------------------\n# Получение данных о береговых линиях мира\nworld &lt;- ne_countries(scale = 50, returnclass = \"sf\") %&gt;% \n  st_transform(4326) %&gt;%  # Преобразование в WGS84\n  st_wrap_dateline()      # Коррекция линии перемены дат\n\n# Создание координатной сетки\ngraticule &lt;- st_graticule(\n  lat = seq(ymin, ymax, 2),  # Шаг по широте: 2 градуса\n  lon = seq(xmin, xmax, 5),  # Шаг по долготе: 5 градусов\n  datum = st_crs(4326)       # Система координат\n)\n\n# 4. НАСТРОЙКА ЦВЕТОВОЙ СХЕМЫ -------------------------------------------------\n# Цвета для отрицательных температур (холодные тона)\ncool_colors &lt;- c(\"#2171b5\", \"#6baed6\", \"#9ecae1\")\n# Цвета для положительных температур (теплые тона)\nwarm_colors &lt;- c(\"#fee391\", \"#fe9929\", \"#d95f0e\")\n# Объединенная палитра\npalette &lt;- c(cool_colors, warm_colors)\n\n# 5. ПОСТРОЕНИЕ ГРАФИКА С ИСХОДНЫМ РАЗРЕШЕНИЕМ --------------------------------\n# Создание карты с исходными данными (низкое разрешение)\nplot_lowres &lt;- ggplot() +\n  # Отображение данных в виде растровых плиток\n  geom_tile(data = DF, aes(x = longitude, y = latitude, fill = TEMP_cat), alpha = 0.7) +\n  # Добавление изолиний температур\n  geom_contour(\n    data = DF,\n    aes(x = longitude, y = latitude, z = TEMP),\n    breaks = c(0, 0.25, 0.5, 0.75, 1),\n    color = \"black\",\n    linewidth = 0.2\n  ) +\n  # Добавление координатной сетки\n  geom_sf(data = graticule, color = \"gray70\", linewidth = 0.3) +\n  # Добавление береговых линий\n  geom_sf(\n    data = world, \n    color = \"gray30\",\n    fill = \"#E8E5D6\",\n    lwd = 0.3\n  ) +\n  # Настройка области отображения\n  coord_sf(\n    xlim = c(xmin, xmax),\n    ylim = c(ymin, ymax),\n    expand = FALSE,\n    crs = st_crs(4326)\n  ) +\n  # Настройка цветовой шкалы\n  scale_fill_manual(\n    name = \"T (°C)\",  # Знак дельта вместо \"?\"\n    values = palette,\n    drop = FALSE,\n    na.value = \"grey90\"\n  ) +\n  # Настройка внешнего вида графика\n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    panel.border = element_rect(color = \"black\", fill = NA, linewidth = 1.5),\n    legend.position = \"bottom\",\n    axis.title = element_blank(),\n    text = element_text(size = 12),\n    legend.text = element_text(size = 10)\n  )\n\n# Отображение графика\nplot_lowres\n\n\n\n\n\n\n\n# 6. ПРОЦЕДУРА РЕСЭМПЛИНГА ----------------------------------------------------\n# Создание исходного растра из данных\nr &lt;- rast(df, type = \"xyz\", crs = \"EPSG:4326\")\n\n# Создание целевого растра с новым разрешением\ntarget &lt;- rast(\n  extent = ext(c(xmin, xmax, ymin, ymax)),\n  resolution = new_res,\n  crs = \"EPSG:4326\"\n)\n\n# Выполнение ресэмплинга с билинейной интерполяцией\nr_resampled &lt;- resample(r, target, method = \"bilinear\")\n\n# 7. ПОДГОТОВКА РЕСЭМПЛИРОВАННЫХ ДАННЫХ ---------------------------------------\n# Преобразование растра в датафрейм для ggplot2\nTEMPERATURE &lt;- as.data.frame(r_resampled, xy = TRUE, na.rm = TRUE) %&gt;% \n  filter(\n    x &gt;= xmin, x &lt;= xmax,\n    y &gt;= ymin, y &lt;= ymax\n  ) %&gt;%\n  mutate(\n    TEMP_cat = cut(\n      TEMP,\n      breaks = c(-Inf, 0, 0.25, 0.5, 0.75, 1, Inf),\n      labels = c(\"&lt;0\", \"0~0.25\", \"0.25~0.5\", \"0.5~0.75\", \"0.75~1\", \"&gt;1\"),\n      include.lowest = TRUE\n    )\n  )\n\n# 8. ПОСТРОЕНИЕ ГРАФИКА С ВЫСОКИМ РАЗРЕШЕНИЕМ ---------------------------------\n# Создание карты с ресэмплированными данными\nfinal_plot &lt;- ggplot() +\n  geom_tile(data = TEMPERATURE, aes(x = x, y = y, fill = TEMP_cat), alpha = 0.7) +\n  geom_contour(\n    data = TEMPERATURE,\n    aes(x = x, y = y, z = TEMP),\n    breaks = c(0, 0.25, 0.5, 0.75, 1),\n    color = \"black\",\n    linewidth = 0.2\n  ) +\n  geom_sf(data = graticule, color = \"gray70\", linewidth = 0.3) +\n  geom_sf(\n    data = world, \n    color = \"gray30\",\n    fill = \"#E8E5D6\",\n    lwd = 0.3\n  ) +\n  coord_sf(\n    xlim = c(xmin, xmax),\n    ylim = c(ymin, ymax),\n    expand = FALSE,\n    crs = st_crs(4326)\n  ) +\n  scale_fill_manual(\n    name = \"T (°C)\",  # Знак дельта вместо \"?\"\n    values = palette,\n    drop = FALSE,\n    na.value = \"grey90\"\n  ) +\n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    panel.border = element_rect(color = \"black\", fill = NA, linewidth = 1.5),\n    legend.position = \"bottom\",\n    axis.title = element_blank(),\n    text = element_text(size = 12),\n    legend.text = element_text(size = 10)\n  )\n\n# Отображение финального графика\nfinal_plot\n\n\n\n\n\n\n\n# 9. СОХРАНЕНИЕ РЕЗУЛЬТАТОВ ---------------------------------------------------\n# Сохранение карты с высоким разрешением в файл\nggsave(\n  filename = \"Temperature_Map.tiff\",\n  plot = final_plot,\n  device = \"tiff\",\n  width = 17,\n  height = 15,\n  units = \"cm\",\n  dpi = 600,\n  compression = \"lzw\",\n  bg = \"white\"\n)\n\n# Дополнительно: сохранение карты с исходным разрешением\nggsave(\n  filename = \"Temperature_Map_LowRes.tiff\",\n  plot = plot_lowres,\n  device = \"tiff\",\n  width = 17,\n  height = 15,\n  units = \"cm\",\n  dpi = 600,\n  compression = \"lzw\",\n  bg = \"white\"\n)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Картография: повышение разрешения</span>"
    ]
  }
]