# ------------------------------------------------------------
# Скрипт: "Деревья решений и Random Forest для биологов"
# Цель: научиться предсказывать длину креветки и определять её пол
# по простым измерениям (вес, возраст, пол/длина)
# ------------------------------------------------------------

# 1. ЗАГРУЗКА НЕОБХОДИМЫХ БИБЛИОТЕК
# Библиотеки — это "инструментальные ящики", которые добавляют новые функции в R

library(tidyverse)        # Для удобной работы с данными и графиков
library(rpart)            # Для построения одного дерева решений
library(rpart.plot)       # Для красивой визуализации дерева
library(randomForest)     # Для построения "леса" из множества деревьев
library(caret)            # Для разделения данных и оценки качества моделей
library(vip)              # Для отображения важности признаков (какие измерения важнее)

# 2. ЗАГРУЗКА ДАННЫХ
# Данные: морфометрия креветок (длина, вес, возраст, пол)
# Ссылка ведёт к открытому CSV-файлу на GitHub

data <- read.csv("https://mombus.github.io/cRab/data/shrimp_catch.csv") %>%
  # Убираем два "подозрительных" образца с id = 10 и 50 (возможно, ошибки измерений)
  filter(!id %in% c(10, 50)) %>%
  # Преобразуем переменные в нужные типы:
  mutate(
    sex = as.factor(sex),   # Пол — категориальная переменная (F/M)
    age = as.factor(age)    # Возраст — тоже категория (например, "молодая", "взрослая")
  )

# Посмотрим, как выглядят данные
head(data)

# 3. ДЕРЕВО РЕШЕНИЙ: "ПОЛЕВОЙ ОПРЕДЕЛИТЕЛЬ"
# Представьте: вы в поле, не можете заглянуть под панцирь, но хотите определить пол.
# Дерево задаёт последовательные вопросы: "Длина > 25 мм?", "Вес < 1.5 г?" и т.д.

tree_model <- rpart(
  sex ~ length + weight + age,   # Формула: предсказываем пол по длине, весу и возрасту
  data = data,                   # Используем весь набор данных
  method = "class",              # Это задача классификации (пол — категория)
  control = rpart.control(minsplit = 20, cp = 0.01)  # Настройки: не дробить мелкие группы
)

# Визуализируем дерево — как "определитель видов" в учебнике
rpart.plot(
  tree_model,
  type = 3,               # Красивый стиль: вероятности в узлах
  extra = 104,            # Показывать % по классам и число наблюдений
  box.palette = "Blues",  # Цветовая палитра
  main = "Дерево для определения пола креветки",
  shadow.col = "gray"
)

# Какие признаки важны в этом дереве?
# Чем больше значение — тем сильнее признак помогал разделять самцов и самок
print("Важность признаков в дереве:")
print(tree_model$variable.importance)

# 4. ПОЧЕМУ ОДНО ДЕРЕВО — ЭТО НЕНАДЕЖНО?
# Одно дерево может "переобучиться" — запомнить шум в данных.
# Random Forest — это "совет экспертов": сотни деревьев голосуют вместе!

# Разделим данные на обучающую (70%) и тестовую (30%) части
# Это как "учить на одной выборке, проверять на другой"
set.seed(123)  # Зафиксируем случайность, чтобы результаты были воспроизводимы
train_index <- createDataPartition(data$length, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data  <- data[-train_index, ]

# 5. RANDOM FOREST ДЛЯ ПРЕДСКАЗАНИЯ ДЛИНЫ (регрессия)
# Теперь мы предсказываем **число** (длину в мм), а не категорию → это регрессия

rf_model <- randomForest(
  length ~ age + weight + sex,   # Что предсказываем и по каким признакам
  data = train_data,             # Обучающие данные
  ntree = 500,                   # Сколько деревьев строить (чем больше — тем стабильнее)
  mtry = 2,                      # Сколько признаков пробовать при каждом разбиении (обычно ~1/3 от общего числа)
  importance = TRUE,             # Сохранить информацию о важности признаков
  na.action = na.omit            # Игнорировать строки с пропущенными данными (NA)
)

# Выведем краткую сводку модели
print(rf_model)
# % Var explained — насколько хорошо модель объясняет вариацию длины (чем ближе к 100% — тем лучше)

# 6. КАК МЕНЯЕТСЯ ОШИБКА С РОСТОМ ЧИСЛА ДЕРЕВЬЕВ?
# OOB (out-of-bag) — оценка ошибки "внутри" модели, без отдельной тестовой выборки
plot(rf_model, main = "Как ошибка уменьшается с ростом числа деревьев")
legend("topright", legend = "OOB ошибка", col = "black", lty = 1)

# 7. КАКИЕ ПРИЗНАКИ ВАЖНЫ В ЛЕСУ?
# vip() строит красивый график важности
vip(rf_model, num_features = 3) +
  ggtitle("Важность признаков для предсказания длины") +
  theme_minimal()

# 8. СРАВНЕНИЕ С ПРОСТОЙ ЛИНЕЙНОЙ МОДЕЛЬЮ
# Линейная регрессия — классический метод: "длина = a*вес + b*возраст + ..."
lm_model <- lm(length ~ age + weight + sex, data = train_data)

# Делаем предсказания на тестовых данных
rf_pred <- predict(rf_model, test_data)   # Предсказания Random Forest
lm_pred <- predict(lm_model, test_data)   # Предсказания линейной модели

# Оценим точность:
# - RMSE (Root Mean Squared Error): средняя ошибка в мм (меньше — лучше)
# - R² (коэффициент детерминации): доля объяснённой дисперсии (ближе к 1 — лучше)

results <- data.frame(
  Модель = c("Random Forest", "Линейная регрессия"),
  RMSE = c(
    sqrt(mean((rf_pred - test_data$length)^2)),
    sqrt(mean((lm_pred - test_data$length)^2))
  ),
  R2 = c(
    cor(rf_pred, test_data$length)^2,
    cor(lm_pred, test_data$length)^2
  )
)

print("Сравнение моделей:")
print(results)

# Заметьте: иногда линейная модель работает почти так же хорошо!
# Это нормально — если связь действительно линейная, сложные модели не дают выигрыша.

# 9. RANDOM FOREST ДЛЯ КЛАССИФИКАЦИИ: ОПРЕДЕЛЯЕМ ПОЛ
# Теперь снова предсказываем **категорию** (пол), а не число

rf_class <- randomForest(
  sex ~ length + weight + age,
  data = train_data,
  ntree = 500,
  importance = TRUE
)

# Предсказываем пол на тестовых данных
predictions <- predict(rf_class, test_data)

# Строим матрицу ошибок (confusion matrix):
# - Сколько самок правильно определено?
# - Сколько самцов ошибочно названо самками?
conf_matrix <- confusionMatrix(predictions, test_data$sex)
print("Матрица ошибок для определения пола:")
print(conf_matrix)

# Важность признаков для классификации
vip(rf_class, num_features = 3) +
  ggtitle("Важность признаков для определения пола") +
  theme_minimal()

# 10. ПОДБОР ГИПЕРПАРАМЕТРОВ: НАСКОЛЬКО СЛОЖНЫМ ДОЛЖЕН БЫТЬ ЛЕС?
# mtry — сколько признаков использовать при построении каждого дерева
# tuneRF() автоматически подбирает лучшее значение

# ⚠️ Внимание: у нас всего 3 признака, поэтому вариантов мало (1, 2 или 3)
# График может выглядеть "бедно" — это нормально!

tune_result <- tuneRF(
  x = train_data[, c("age", "weight", "sex")],  # Признаки
  y = train_data$length,                        # Целевая переменная (длина)
  ntreeTry = 300,                               # Сколько деревьев использовать при тестировании
  stepFactor = 1.5,                             # Шаг изменения mtry
  improve = 0.01,                               # Минимальное улучшение ошибки, чтобы продолжать поиск
  trace = TRUE,                                 # Печатать промежуточные результаты
  plot = TRUE                                   # Построить график зависимости ошибки от mtry
)

# Совет: при большом числе признаков этот график будет гораздо информативнее!
# При 3 признаках — просто помните, что mtry = 1, 2 или 3 — и всё.

# ------------------------------------------------------------
# ЗАКЛЮЧЕНИЕ
# Вы научились:
# - Строить дерево решений как "определитель"
# - Использовать Random Forest для регрессии и классификации
# - Оценивать качество моделей
# - Интерпретировать важность признаков
# - Сравнивать сложные и простые модели
# ------------------------------------------------------------


